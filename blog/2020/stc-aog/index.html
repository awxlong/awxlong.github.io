<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Spatial, temporal, causal and-or graph, the next step for Cognitive Science? | Xuelong An Wang</title> <meta name="author" content="Xuelong An Wang"> <meta name="description" content="thoughts on an unifying framework for cognitive science"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/brain-fractal-3.jpg"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://awxlong.github.io/blog/2020/stc-aog/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Spatial, temporal, causal and-or graph, the next step for Cognitive Science?",
      "description": "thoughts on an unifying framework for cognitive science",
      "published": "August 18, 2020",
      "authors": [
        {
          "author": "An Xuelong",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Xuelong¬†</span>An Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">research</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Spatial, temporal, causal and-or graph, the next step for Cognitive Science?</h1> <p>thoughts on an unifying framework for cognitive science</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#searching-for-a-unifying-framework-for-addressing-cognitive-science">Searching for a unifying framework for addressing cognitive science</a></div> <div><a href="#on-the-origin-of-such-mental-representation-and-how-it-affects-the-design-of-ai">On the origin of such mental representation and how it affects the design of AI</a></div> <div><a href="#alternative-to-natural-language-processing">Alternative to natural language processing</a></div> <div><a href="#errata">Errata</a></div> </nav> </d-contents> <p><strong>Please see Errata at the end of the article concerning the limitations of a chatbot</strong></p> <h1 id="searching-for-a-unifying-framework-for-addressing-cognitive-science">Searching for a unifying framework for addressing cognitive science</h1> <p>Cognitive science has shared parallels with artificial intelligence in the sense that it has also gone through ‚Äúbooming‚Äù periods, meaning times when a proposed theoretical framework seemed to be able to explain aspects of the human mind, and subsequent ‚Äúbusting‚Äù periods, times when that framework was rebuked by new findings<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. Such booming and busting fluctuation revolved around the dichotomy between nativism and empiricism, with their respective ramifications<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, but regardless of this rivalry, the 21st century lifted its curtains being able to find reconciliation between these two and propose yet another theoretical framework seeking to ultimately explain how our minds work, called Predictive processing (PP) (see <d-cite key="clark_2013_whatever"></d-cite>). The following text seek to defend how this framework can indeed unify several theories in the field of cognitive science and help gear future actions regarding implementing it in the field of AI.</p> <p>It‚Äôs important the seeking of a unifying basis to direct the study of cognitive science because it helps make its study coherent. Furthermore, in other fields such as that of Physics, as pointed out by Prof. Song, it has been noticed that current studies of this discipline revolves around the fundamentals of forces, fields and particles. In that regard, predictive processing, along with spatial, temporal, causal and-or graphs (STC-AoG)<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> might serve as the forces, fields and particles of cognitive science.</p> <p>Both predictive processing and STC-AoG converge in that they propose the existence of a mental structure that constantly processes incoming inputs, in other words, top-down deduction addressing bottom-up incoming precepts from the environment. One implication of such structure is that our minds gears our body to be able to perform complex actions and articulate complex language even with small input data, given that as little as there is, it undergoes constant processing from which predictions (stemmed from logical causal relationships established between data) are drawn. Another implication is that the mind discriminates the ubiquitous data in our surroundings, meaning, that we don‚Äôt perceive thoroughly the environment, but rather hierarchically choose data that‚Äôs beneficial for us to accomplish an immediate task.</p> <p>If the above similarities hold true, then predictive processing can indeed act as a framework addressing STC-AoG, therefore help explain from the simplest of our daily life activities, while also serve as a framework for studying computational cognitive science.</p> <p>For example, assuming our mind does have a representation of our immediate surrounding that follows a hierarchy and establishes causal relationships in order to make predictions and gear actions, we can then explain the action of drinking water when thirsty. For instance, see <a href="#figure-1">Figure 1</a>:</p> <figure> <img src="/assets/img/stc-aog-song.png" alt="Figure couldn't load due to an unknown error. Sorry."> <figcaption id="figure-1">Figure 1: Example of STC-PG (a slight variation of the STC-AoG, see <d-cite key="xiong_2016_robot"></d-cite>) in which a hierarchical-causal structure is applied to the immediate environment, with the top-right, gray pentagon indicating the purpose of being in the room known as kitchen. This helps define a series of current actions that can be taken to solve the problem of thirst, such as fill a vase with water, while also predicting later interactions with the environment depending on what is perceived, which influences future intents, such as resting, and gears future actions. Note: the camera in the lower left corner indicates that if these were a robot‚Äôs eyes, then it should be able to reconstruct this human mental structure. Image extracted from <d-cite key="songchun_2017_qiantan"></d-cite>.</figcaption> </figure> <h1 id="on-the-origin-of-such-mental-representation-and-how-it-affects-the-design-of-ai">On the origin of such mental representation and how it affects the design of AI</h1> <p>Until now, PP and STC-AoG have been discussed, but why is it that humans were born with such mental structures? A possible answer is given by Prof. Zhu, and is also mentioned by cognitive neuroscientist Anil Seth<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">4</a></sup>, which regards survivability. Perhaps it‚Äôs an innate property that humans were born with, but the pursuit of survival can provide an explanation to a lot of fundamental aspects of the mind, such as the origin of PP or the STC-AoG, that is, we have that internal predictive structure so that we can act accordingly to survive in an environment by reducing harm to our body, minimize fatigue, feed ourselves properly, maximize rewards, among others. Survivability also accounts for the origin of communication, given that cooperating with other beings and environment through language was meant to enhance survival.</p> <p>Another example, now taking survivability into account, would be regarding an individual perceiving a sandstorm incoming; in order to reduce harm to his eyes, his mental structure would parse the elements in his surrounding (clothing, eyeglasses, accessories), predict their functionality and respond to the changing environment by acting (using scarf to cover eyes). Notice that he doesn‚Äôt necessarily parse everything in his surrounding, meaning that he might ignore the exact temperature of the environment, sunlight intensity, air speed, whether there‚Äôs a tree five miles away from him, among others, hence it‚Äôs a discrimination of inputs, but despite hierarchically choosing certain inputs he can still design a series of steps to protect himself. Conversely, we can ask why isn‚Äôt his mental structure guiding him to receive and withstand pain. Also notice that in this case, PP is not a registry of data as it would mean that if the individual hasn‚Äôt been exposed to the environment, then he wouldn‚Äôt know how to act as there is no previous information regarding how to behave during a sandstorm.</p> <h1 id="on-vision-and-natural-language-processing">On Vision and Natural Language processing</h1> <p>Also, PP and STC-AoG can take into account how human beings can perform complex tasks despite having a relative poor amount of input (this is partly due to the natural biologically limits of the eye<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">5</a></sup>, given that what acutely detects colour and details is in the fovea centralis, which accounts for about 15¬∞ out of 200¬∞ of range of vision (see <d-cite key="rodnave_2017_the"></d-cite> Nave, 2017)). This is by implying that humans, despite not seeing every aspect of nature in detail, instead hierarchically chooses some of them, he can predict the rest of useful features and how they can be used for their benefits. This reduces time of processing, as there is fewer data, and also makes actions faster.</p> <p>If a human were to grab a can sitting on a table, he wouldn‚Äôt place much attention in the walls of the room, as they can be internally predicted, nor even the entire table, but part of it. Instead, he would focus his attention on the can, hence can quickly perform the task. This contrasts with, for instance, how Shakey, the robot<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">6</a></sup>, behaved, given that it had to analyse each and every single piece of input independently (walls, floor, objects), without discriminating data, which then made the task of pushing cubes very slow. In that regard, both PP and STC-AoG focus more on drawing logical connections among data input, so that predictions are made, instead of quantity of data input<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">7</a></sup>.</p> <p>PP and STC-AoG can permeate into the fields of linguistics, mainly because incoming data wouldn‚Äôt be treated as unrelated individual chunks of meaningless data, but rather part of a structure that‚Äôs constantly discriminating them and drawing connections between them. For instance, regarding the discrimination of data, one can find that sentences can be set into hierarchies as show in <a href="#figure-2">Figure 2</a>:</p> <figure> <img src="/assets/img/stc-aog-grammar.png" alt="Figure couldn't load due to an unknown error. Sorry."> <figcaption id="figure-2">Figure 2:The way how syntax is structured perhaps displays innate principles of human language. </figcaption> </figure> <p>From <a href="#figure-2">Figure 2</a>, it‚Äôs worth noting that sentences can be broken down into parts that are more important, called the heads of a phrase, and others that are less important, the heads‚Äô complements, hence occupy the bottom of the hierarchy, such as:</p> <figure> <img src="/assets/img/eg_grammar.jpg" alt="Figure couldn't load due to an unknown error. Sorry."> </figure> <p>In other words, in a verb phrase, we value the verb over everything else, in a noun phrase, we value the noun, so on and so forth. Therefore, in a complete sentence, we would value nouns, verbs and adjectives, but ignore what‚Äôs considered complements to those heads. Perhaps that‚Äôs the fundamental principle of language, to describe reality by identifying an object (to which we subjectively called a ‚Äúnoun‚Äù), that object‚Äôs properties (to which we subjectively called an ‚Äúadjective‚Äù), and any object‚Äôs function (to which we subjectively called a ‚Äúverb‚Äù). This kind of discriminative parsing of utterances means less amount of data needed to be processed while also maintaining coherent speech:</p> <p>George: Hi! My name is George</p> <p>(What Anna‚Äôs mind processes: ‚Ä¶name ‚Ä¶ George)</p> <p>Anna: Hello, greetings from the library located next to Amazon, the best shop ever where you can buy anything you want, at the lowest of the lowest price.</p> <p>(What George‚Äôs mind processes: ‚Ä¶library‚Ä¶next Amazon‚Ä¶buy‚Ä¶low price)</p> <p>George: what was my name again?</p> <p>Anna: George, by the way, where do I work?</p> <p>George: At the library, next to Amazon.</p> <p>This contrasts with some old language processors given that they analyze data independently, without discriminating nor putting it into hierarchies, which disables coherent speech, and as such understanding, as shown in <a href="#figure-3">Figure 3</a>:</p> <figure> <img src="/assets/img/cleverbot.jpg" alt="Figure couldn't load due to an unknown error. Sorry."> <figcaption id="figure-3">Figure 3: Screenshot of a dialogue with [Cleverbot](https://www.cleverbot.com/) displaying how the lack of a hierarchical, interconnected data input leads to incoherence in output. </figcaption> </figure> <p>Each line of data is treated as a ‚Äúseparate‚Äù entity, instead of finding logical connections. Each word of each sentence is also given equal amount of importance.</p> <p>PP also allow us to draw causal relations in sentences and build predictions out of data input. Hence, we can predict what other people might say, and even what they think based on what we have heard about them before or on data previously categorized, see <d-cite key="saxe_2003_people"></d-cite> <sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">8</a></sup>. Consider one of the ways to test this as shown in <a href="#figure-4">Figure 4</a>:</p> <figure> <img src="/assets/img/saxon_screenshot.jpg" alt="Figure couldn't load due to an unknown error. Sorry."> <figcaption id="figure-4">Figure 4: Participants are able to predict what Emily would think it‚Äôs a Porsche based on previous data input (Figure extracted from <d-cite key="saxe_2003_people"></d-cite>). </figcaption> </figure> <p>Nonetheless, given the above example of CleverBot, it would also probably fall short when trying to predict what Emily is thinking.</p> <p>Therefore, for robots to take the next step of being more human (or the next step for cognitive science), this unifying framework in the form of theory (PP) and practice (STC-AoG) can be implemented.</p> <h1 id="alternative-to-natural-language-processing">Alternative to natural language processing</h1> <p>Human language is perhaps designed arbitrarily, therefore it might be too much to program AI so that it discriminates language.</p> <p>An alternative to that, provided by Prof. Zhu, is to use pictographs, as in <a href="#figure-5">Figure 5</a>:</p> <figure> <img src="/assets/img/pictograph-song.png" alt="Figure couldn't load due to an unknown error. Sorry."> <figcaption id="figure-5">Figure 5: Pictographs can be used for communication between AI, that is, for AI to learn between one another they can use this system, while when communicating with human they use our language. </figcaption> </figure> <h1 id="errata">Errata</h1> <p>I wrote this article in 2020, before ChatGPT or any GPT-powered chatbot was released. As such, I have to take back some statements concerning chatbots such as ‚Äúthey‚Äôre only able to treat each sentence independently‚Äù. Thanks to Transformers, now natural language processing chatbots are able to account for long-range dependencies between the words of a sentence, and thus produce realistic sentences that has long term coherence.</p> <p>Understanding of the sentences is still debatable. One such test I mentioned <a href="/blog/2020/stargaze-future">previously</a> where we check whether these chatbots are beyond stochastic parrots that predict the next word goes as follows:</p> <p>You: Do you understand what I say, or are you a stochastic parrot?</p> <p>Robot: Yes.</p> <p>You: Please don‚Äôt say anything afterwards.</p> <p>Robot: ‚Ä¶ (If the chatbot ‚Äúunderstood‚Äù the sentence, it won‚Äôt output words afterwards)</p> <p>I‚Äôm not confident that GPT-powered chatbots are able to carry out instructions, such as stop predicting words (you can try this!).</p> <p>To see another example where authors exploit ChatGPT being a next-token predictor to gauge its limitations can be seen as follows, with the associated <a href="https://arxiv.org/abs/2309.13638" rel="external nofollow noopener" target="_blank">paper</a>:</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">ü§ñüß†NEW PAPERüß†ü§ñ<br><br>Language models are so broadly useful that it's easy to forget what they are: next-word prediction systems<br><br>Remembering this fact reveals surprising behavioral patterns: üî•Embers of Autoregressionüî• (counterpart to "Sparks of AGI")<a href="https://t.co/w0wl4eW1M0" rel="external nofollow noopener" target="_blank">https://t.co/w0wl4eW1M0</a><br>1/8 <a href="https://t.co/gTezJKGYA1" rel="external nofollow noopener" target="_blank">pic.twitter.com/gTezJKGYA1</a></p>‚Äî Tom McCoy (@RTomMcCoy) <a href="https://twitter.com/RTomMcCoy/status/1706664506913399198?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">September 26, 2023</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>This is a parallel drawn from UCLA Prof. Song Zhu-Chun‚Äôs description of the evolution of Artificial Intelligence research.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>For empiricism, behaviourism was an example of a discipline stemmed from it. As for nativism, cognitive science emerged.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>To see further work of Prof. Zhu, please visit VCLA at UCLA¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:6" role="doc-endnote"> <p>He is also a proponent of predictive coding, see <d-cite key="seth_2013_extending"></d-cite>¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:7" role="doc-endnote"> <p>This can be evidenced by asking individuals to draw and test whether they can exactly reproduce a landscape. Robots, through cameras, can reproduce exactly the environment, nonetheless, it would ignore the efficiency of inputting what‚Äôs necessary while predicting the rest.¬†<a href="#fnref:7" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:8" role="doc-endnote"> <p>Robot developed at the SRI International. Performance can be watched at: https://www.youtube.com/watch?v=GmU7SimFkpU¬†<a href="#fnref:8" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:9" role="doc-endnote"> <p>Prof. Song refers to ‚Äúsmall data for big tasks‚Äù as the basis for the future of AI.¬†<a href="#fnref:9" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> <li id="fn:11" role="doc-endnote"> <p>Briefly, a region in the human temporo-parietal junction (TPJ-M) is involved specifically in reasoning about the contents of another person‚Äôs mind¬†<a href="#fnref:11" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> </ol> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-principia-refs.bib"></d-bibliography><div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="al-folio",disqus_identifier="/blog/2020/stc-aog",disqus_title="Spatial, temporal, causal and-or graph, the next step for Cognitive Science?";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 Xuelong An Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>