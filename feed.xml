<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://awxlong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://awxlong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-05T16:58:51+00:00</updated><id>https://awxlong.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Conversatoria sobre la inteligencia artificial en la medicina</title><link href="https://awxlong.github.io/blog/2025/tertulia-kennedy/" rel="alternate" type="text/html" title="Conversatoria sobre la inteligencia artificial en la medicina"/><published>2025-02-04T13:42:00+00:00</published><updated>2025-02-04T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2025/tertulia-kennedy</id><content type="html" xml:base="https://awxlong.github.io/blog/2025/tertulia-kennedy/"><![CDATA[<h1 id="el-rol-de-la-inteligencia-artificial-en-la-medicina">El Rol de la Inteligencia Artificial en la Medicina</h1> <p>Tuve la maravillosa oportunidad de dar una presentaci√≥n junto con el ex-Ministro de Salud y fundador del Grupo Hospitalario Kennedy Dr. Te√≥filo Lama. En esta, compart√≠ mi experiencia entorno a los usos de la inteligencia artificial (IA) en la medicina.</p> <p>El tema principal de la conversatoria trata sobre el ‚ÄúPasado, Presente y Futuro de la Medicina‚Äù, en donde el Dr. Lama comparti√≥ su carrera en la medicina, c√≥mo esta ha avanzado a trav√©s del tiempo y nos contagi√≥ con su entusiasmo e inter√©s sobre c√≥mo la IA poco a poco sinergiza con la medicina.</p> <figure> <img src="/assets/img/kennedy-2.jpg" alt="Sorry. Image couldn't load." width="auto" height="auto"/> <figcaption id="cell-embedder">El pasado, presente y futuro de la medicina con Dr. Lama </figcaption> </figure> <p>Durante mi turno, estructur√© mi presentaci√≥n en base a una conversaci√≥n con un amigo quien estudia medicina:</p> <ul> <li>¬øA qui√©n asiste la IA: al paciente o al doctor?</li> <li>Costos de implementaci√≥n de la IA</li> <li>Casos ejemplares</li> </ul> <h2 id="la-ia-como-un-asistente-de-diagn√≥stico">La IA como un asistente de diagn√≥stico</h2> <p>Compart√≠ mi tesis de mi masterado en UCL, ilustrando un caso en donde la IA act√∫a como asistente de pat√≥logos para el an√°lisis a gran escala de tejido colorectal para identificar poblaciones de c√©lulas en fase G0, lo cual es relevante para estudiar resistencia a quimioterapia y relapso. Compart√≠ mi demo en: https://awxlong.github.io/assets/img/pathoinsightmil_demo.mp4</p> <p>El costo de implementar este sistema en un local m√©dico es bastante alto. En un lado esto es debido a la alta complejidad del dato, pues las im√°genes que representan la digitalizaci√≥n de una biopsia contiene m√°s de 1 mill√≥n de p√≠xeles. Por ende para entrenar estos modelos de aprendizaje profundo, as√≠ como almacenar estas im√°genes, requerir√≠an cl√∫steres y servidores, lo cual no necesariamente estar√≠a disponible.</p> <figure> <img src="/assets/img/kennedy-3.png" alt="Sorry. Image couldn't load." width="auto" height="auto"/> <figcaption id="cell-embedder">El presente y futuro de la medicina con la IA</figcaption> </figure> <h2 id="la-ia-como-un-asistente-para-lidiar-con-obligaciones-burocr√°ticas">La IA como un asistente para lidiar con obligaciones burocr√°ticas</h2> <p>Otro uso de la IA concierne implementarlo como un asistente para automatizar el llenado de formularios varios, que constituyen trabajo tedioso que todo doctor enfrenta d√≠a a d√≠a. Compart√≠ mi proyecto sobre YachayMed https://github.com/awxlong/medical_asr. Este sistema transcribe conversaciones entre doctor y paciente con el fin de facilitar el llenado de un reporte m√©dico de un paciente.</p> <p>En este caso, el costo de implementaci√≥n ser√≠a menor debido a que el avance y accesibilidad de modelos de lenguaje es mayor que los de im√°genes. En parte esto es debido a que las transcripciones tienen menor complejidad de datos (son cortas, y m√°s o menos consisten de alrededor de 700-1000 palabras para una conversaci√≥n de casi 10 minutos). Adem√°s, paralelo al movimiento de c√≥digo abierto, la integraci√≥n de modelos de lenguaje abre bastantes oportunidades en el √°mbito m√©dico.</p> <p>Claro, lo m√°s accesible y f√°cil de implementar ser√≠a modelos estad√≠sticos/aprendizaje mec√°nico como regresi√≥n lineal o log√≠stica para el an√°lisis masivo de bases de datos como formularios de Excel. Hay problemas bastantes interesantes que pueden ser resueltos como predicci√≥n de sobrevivencia o readmisi√≥n.</p> <figure> <img src="/assets/img/kennedy-1.jpg" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="afold"> Conversaci√≥n y discusi√≥n sobre la IA en la medicina </figcaption> </figure> <p>Las diapositivas que emple√© se encuentran anexado. Hay diapositivas extras explicando qu√© son redes neuronales y c√≥mo han sido entrenadas.</p> <style>.pdf-embed-wrap-f428e4f8-2b9c-4d13-a906-7e8216b5fc88{display:flex;flex-direction:column;width:100%;height:650px}.pdf-embed-container-f428e4f8-2b9c-4d13-a906-7e8216b5fc88{height:100%}.pdf-link-f428e4f8-2b9c-4d13-a906-7e8216b5fc88{background-color:white;text-align:center;border-style:solid}.pdf-embed-container-f428e4f8-2b9c-4d13-a906-7e8216b5fc88 iframe{width:100%;height:100%}</style> <div class="pdf-embed-wrap-f428e4f8-2b9c-4d13-a906-7e8216b5fc88"> <div class="pdf-link-f428e4f8-2b9c-4d13-a906-7e8216b5fc88"> <a href="/assets/pdf/tertulia_ia.pdf" target="_blank">View PDF</a> </div> <div class="pdf-embed-container-f428e4f8-2b9c-4d13-a906-7e8216b5fc88"> <iframe src="/assets/pdf/tertulia_ia.pdf" frameborder="0" allowfullscreen=""></iframe> </div> </div>]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><summary type="html"><![CDATA[compart√≠ mi experiencia sobre las aplicaciones de la IA en la medicine en el Grupo Hospitalario Kennedy, Guayaquil]]></summary></entry><entry><title type="html">The interesting analogy between language and biology</title><link href="https://awxlong.github.io/blog/2025/lang-biology/" rel="alternate" type="text/html" title="The interesting analogy between language and biology"/><published>2025-01-19T12:42:00+00:00</published><updated>2025-01-19T12:42:00+00:00</updated><id>https://awxlong.github.io/blog/2025/lang-biology</id><content type="html" xml:base="https://awxlong.github.io/blog/2025/lang-biology/"><![CDATA[<h1 id="on-the-interesting-parallels-of-language-and-biology-a-taster">On the interesting parallels of language and biology: a taster</h1> <p>There are interesting parallels between language and biology. As a result, problems surfacing in natural language processing (NLP) tasks, may well inform us about possible problems and solutions in computational biology.</p> <p>The simplest common denominator to observe the parallels concerns the analysis of sequences: algorithms and modelling paradigms like recurrent neural networks and self-attention in transformer that help process a sequence of words to solve a task like text classification can surprisingly work with sequences of genes to classify what disease does an RNA sequence correspond to. This is because at a computational level, a string of nucleotides (adenine (A), thymine (T) or uracil (U) in RNA, cytosine (C), and guanine (G)) can be <a href="https://www.sciencedirect.com/science/article/pii/S2001037021000945">represented as a sequence of letters</a>.</p> <p>Another common point concerns the representation of words and biological entities. It would not suffice to represent a word like ‚Äòairplane‚Äô based solely on this string (i.e. this symbol) so it is common to use a fixed-size embedding vector to denote it. This be extracted from open-source libraries like GLoVE and word2Vec. In parallel, molecules also have expert-coded or learnt <a href="https://pubs.acs.org/doi/epdf/10.1021/acs.jcim.9b00237">molecular fingerprints</a> as vector embeddings which are readily available in RDKit. Another case is that of obtaining embeddings for biological entities. A gene can be treated as a vocabulary token, while a cell can be treated <a href="https://www.biorxiv.org/content/10.1101/2023.04.30.538439v1.full.pdf">as a sentence</a>. Another work on a similar vein is <a href="https://www.biorxiv.org/content/10.1101/2023.09.11.557287v2">Cell2Sentence</a>, where gene expression (single-cell transcriptomics) is treated as text.</p> <h1 id="language-agnostic-grammar-structure-for-language-and-biology">Language-agnostic grammar: structure for language and biology</h1> <p>Inspired by Chomsky‚Äôs linguistics, there is work on learning a language-agnostic tree-like structure to learn embeddings of text, called <a href="https://arxiv.org/abs/2305.05588">Self-Structured AutoEncoder</a>. This tree-like structure combines bottom-up composition of local contextual embeddings with top-down decomposition of fully contextual embeddings to learn in an unsupervised routine. Self-StrAE achieves competitive autoencoding performance with respect to a LLM, BERT-Based variant, denoting its low parameter efficiency that reflects the parsimonious nature of grammar.</p> <p>Given the parallels with language, it is interesting to explore how would it work if Self-StrAE is applied to a DNA, or RNA sequence. What will the learnt structure tell us about the DNA sequence. Learnt embeddings would shed light into sequences of DNA which may have fucntional similarities. The compositions found by Self-StrAE would also find sequence motifs, which are short recurring patterns of DNA that map to specific biological functions like protein docking, because of frequent coocurrence leading to similar embeddings.</p> <h1 id="language-respects-constraints-so-does-biology">Language respects constraints, so does biology</h1> <p>Another interesting parallel are the constraints underlying both generation of sentences and biological entities/processes.</p> <p>Imagine that you want to generate sentences. You have a collection of sentences to train a probabilistic model on. Because we can‚Äôt control what the probabilistic model learns from the given data, how do you make sure you don‚Äôt generate an ungrammatical sentence like ‚Äújogging Octopie went‚Äù, where we have a ‚Äúverb-subject-verb‚Äù structure. The simplest way would be to set up a hidden template like ‚Äúsubject-verb-object‚Äù<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> in order to offset portion of the probabilistic model‚Äôs support<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> to not put any probability mass to structures other than ‚Äúsubject-verb-object‚Äù. Given this, this will first guarantee that sentences follow the correct grammar rules, and second guide <a href="https://awxlong.github.io/blog/2023/sgd/">learning</a> since the model is not exploring over the whole parameter space, but only that which conform to the supplied template.</p> <p>Consider the following analogous task, given some atoms like hydrogen, carbon, oxygen, among others, you want to train a probabilistic model to predict the angle of rotation when such input atoms are assembled together into a compound. This could be relevant for a 3D reconstruction of an assembled molecule. You have a dataset of different atom-atom compounds with their associated rotation angles. If you only rely on learning, then the resulting probabilistic model may put probability mass on biologically implausible rotation angles, even if that implausible probability mass is negligible. In the end, statistics is just about counting and averaging, meaning that even if you only see that some compound like water has a bond angle of around <a href="http://witcombe.sbc.edu/water/chemistrystructure.html">105 degrees</a>, the model‚Äôs support would still put probability mass in-between 0 or 200 degrees <sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. Enumerating all possible rules concerning rotation angles would be impractical, so one can resort to biomedical ontologies.</p> <p>Arguably, this is how DeepMind‚Äôs AlphaFold ensures that proteins generated from DNA input sequences are biologically plausible, as can be observed in the model‚Äôs pipeline. In it, the input sequence first goes through a database search that is subsequently converted into embeddings used to generate the protein structure. Without this first phase, this probabilistic model may output proteins that resemble the ones observed during training, at the risk of following impossible configurations.</p> <figure> <img src="/assets/img/alphafold-pipeline.png" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="afold"> Structural constraints in the first phase concerning genetic database search and structure database search ensures that rotation angles of bonds in the compounds of the protein are biologically plausible, setting structure in learning. Figure extracted from https://www.nature.com/articles/s41586-021-03819-2 </figcaption> </figure> <p>Generating implausible proteins, or invalid angle rotations between bonds is akin to generating sentences that violate grammar rules.</p> <p>It is debatable whether the rules can be learnt from data. Rather, the role of rules and constraints might be to shape learning.</p> <figure> <img src="/assets/img/nesy-cell.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="cell-embedder">A depiction of a neurosymbolic programming approach to virtual cell. Consider a single nucleotide sequenceüß¨ changing over time. A neural-network driven search of programs takes the sequence as input, and outputs a sequence of program or steps that transforms the input sequence to a final state. The sequence of steps serve as an explanation to the final state, which could correspond to cancer. </figcaption> </figure> <p>For further thought experiments, imagine whether a model can learn what are the rules of programming languages by only observing code, or infer the rules of grammar from just observing written text.</p> <h1 id="nlp-inspiring-future-biomedical-research-directions">NLP inspiring future biomedical research directions</h1> <p>A dictionary with mostly indisputable grammar rules are akin to biological ontologies like the Gene Ontology An interesting thought experiment hence, is can this grammar be expanded given the text available, analogous to asking whether</p> <figure> <img src="/assets/img/expand-ontology.png" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="ontology"> . </figcaption> </figure> <h1 id="final-thoughts">Final thoughts</h1> <p>I believe that because we can natively read Human language, research on NLP is more commoditized and well-received by the public than computational genomics, despite the parallels outlined above. I think it is worth for me and anyone interested in computational biology to closely follow the research literature on language modelling to draw inspiration ofr DNA/RNA sequence modelling.</p> <p>Finally, I leave you the following thought experiment: we humans can only read human language. While we can not natively understand the language of biology, our AI-based tools can understand them.</p> <p>‚ÄòAI may just turn out to be the language to describe biology‚Äô - Demis Hassabis‚Äôs <a href="https://www.youtube.com/watch?v=Gfr50f6ZBvo">statement</a></p> <p>If you have answers, share thoughts, you can leave a comment or please email me!</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Sentences don‚Äôt follow this simple template, but it helps get the idea across that placing structure into the support of a probabilistic model helps guide learning.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>A probabilistic model‚Äôs support refers to the domain of values for which the output of the model is non-zero.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>While it is true that some bonds between atoms like a carbon-carbon bond have no restricted rotation angle, others like a <a href="https://www.sciencedirect.com/science/article/pii/S0022285217302990">hydrogen peroxide</a> is constrained to a setting of rotation degrees (with some uncertainty).¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><category term="food-for-thought"/><summary type="html"><![CDATA[comments on the interesting parallels that surface in NLP and computational biology, and how each informs about problems and solutions about each other]]></summary></entry><entry><title type="html">InQuery: Text2SQL App coded with LLMs</title><link href="https://awxlong.github.io/blog/2025/inquery/" rel="alternate" type="text/html" title="InQuery: Text2SQL App coded with LLMs"/><published>2025-01-15T15:42:00+00:00</published><updated>2025-01-15T15:42:00+00:00</updated><id>https://awxlong.github.io/blog/2025/inquery</id><content type="html" xml:base="https://awxlong.github.io/blog/2025/inquery/"><![CDATA[<h1 id="why-dart">Why Dart?</h1> <p>Dart is a programming language which enables the same code to be compiled to multiple platforms, be it IOS, Android or a web platform. This means that you can build sort of ‚Äúuniversal‚Äù applications accessible through any platform.</p> <h1 id="why-text-to-sql">Why Text to SQL?</h1> <p>Not everyone is an expert in structured language. There is a general field of research on text to SQL dedicated to users unfamiliar with this syntax navigate a database. In our project, we grounded it in the context of querying a medical database. As a sample use case, we query a <a href="https://physionet.org/content/mimic-iv-demo/1.0/">free, open-source demo of the MIMIC-IV Clinical Database</a>.</p> <h1 id="are-llms-enough-to-build-this-app">Are LLMs enough to build this app?</h1> <p>While I acknowledge the use of LLMs in building this app, their role is better described as an assistant rather than a substitute. An app is way more complicated, in the process of building it I faced a lot of problems which requires a human‚Äôs debugging skills, and continuous iterative improvements (tons of breakpoints and trying to pinpoint the source of a problem).</p> <p>For example, many of the problems which the LLM couldn‚Äôt solve:</p> <ol> <li> <p>I faced an API query error which required me to try different API connection points to identify which is the one which works and could bridge the flask backend with the front end interface.</p> </li> <li> <p>There was a problem regarding querying the database whereby I didn‚Äôt know the database should be copied in the memory of the device, otherwise I would face unknown tables identified in the database. I needed a lot of breakpoints to debug to figure this out. Originally I wanted to query an external database, but as a proof-of-concept I started easy and queried a local database. The idea should generalize quiet smoothly.</p> </li> <li> <p>There was a lot of trial and error before deciding to design the app as it is (something no LLM could anticipate at the beginning of the project).</p> </li> </ol> <p>3.1 Initially I wanted to use this BERT-based model, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8701710/">MedTS</a>, for my app. However, it‚Äôs difficult to use and requires intricate input preprocessing before I can use it. In my app, I‚Äôm just assumming the user will provide a natural language input, not information such as a database schema intricately parsed into a json file, conveniently preprocessed in tree structures to be fed to MedTS. MedTS is lightweight and definitely would have been good, but it‚Äôd require extensive automatization regarding preprocessing of input.</p> <p>3.2 Another approach to use a powerful text-to-SQL model is to use an open-source big model like one based on Llama, and compress it using tflite to deploy it on-device. This was not possible for me, because this powerful model is just too big to be compressed in &lt;2 gb. After many trial and error attempts, I ended up (thanks to brainstorming with Perplexity.AI) to host the LLM in a Flask-based backend and call it through an API in the app. I‚Äôll still explore quantization to reduce the memory footprint of the sqlcoder-7b-2 used.</p> <p>The app‚Äôs name, InQuery, is meant to be a play of the words Inquiry and Query as part of SQL.</p> <p>The code along with instructions on how to run it is found at https://github.com/awxlong/ai_sql_coder. Hope you find it helpful!</p>]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><summary type="html"><![CDATA[a brief description of an app written in Dart with the help of Perplexity.AI, ChatGPT and DeepSeek-V3]]></summary></entry><entry><title type="html">Ideas on transitioning from monolith to manifold of models</title><link href="https://awxlong.github.io/blog/2024/manifold-vqa/" rel="alternate" type="text/html" title="Ideas on transitioning from monolith to manifold of models"/><published>2024-09-28T15:42:00+00:00</published><updated>2024-09-28T15:42:00+00:00</updated><id>https://awxlong.github.io/blog/2024/manifold-vqa</id><content type="html" xml:base="https://awxlong.github.io/blog/2024/manifold-vqa/"><![CDATA[<h1 id="from-monolith-to-manifold">From monolith to manifold</h1> <p>The common trend in deep learning is to propose a single model which can achieve SoTA performance on some well-established benchmark. However, models proposed which achieve high performance in different benchmarks carry their advantages and limitations. For example, a VQA model with a vision encoder pretrained on rare skin lesions may answer queries more accurately from patients with these edge cases, but perform badly on common lesions. Another VQA model (which could be an ablation of the former) may perform better at spotting normal skin lesions and perform badly on rare diseases. A question which arises is how can we combine their strengths and address each other‚Äôs limitations, akin to members of a team compensating for each other‚Äôs level of expertise. One approach is to combine multiple VQA models to leverage their interoperability so that they can discuss with each other as proposed in https://nips.cc/virtual/2023/76544, transitioning from a monolith model to a manifold of models. The proposed training and inference regime is very similar to the work of Garc√≠a and Lithgow-Serrano, (2024) found in https://aclanthology.org/2024.clinicalnlp-1.45.pdf, where the difference lies in that we propose multiple VQA model responses, instead of responses from a single VQA model, to be summarized by a powerful LLM.</p> <figure> <img src="/assets/img/training_m3g.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="train">At a) we extract features from the question Q which is constructed differently per each language model, and from the stack of images I . At b) we fuse the features to be passed to the language model for free-form answer prediction. At c), each VQA model outputs an answer via greedy search (or beam search with a width of 1 ), and is trained by minimizing the cross-entropy loss function between ground-truth tokens and the predicted tokens. In the illustration we depict prompts in Spanish.</figcaption> </figure> <p>We take into account memory efficiency, and as such we implement model compression techniques for memory efficiency during fine-tuning, which includes 1) mixed-precision and 2) gradient accumulation to simulate mini-batch gradient descent. Answers are generated via greedy-search.</p> <figure> <img src="/assets/img/publication_preview/inference_m3g.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="inference">Similar to training's stage a), we extract features from the question and images, and fuse them at b) so that at c) each of the language models output a list of answers. At d), the answers of each VQA model is put as context for a prompt to a LLM, where we query for a concise answer that accounts for the different diagnoses offered by each model. In the illustration we depict prompts in Spanish.</figcaption> </figure> <p>With memory efficiency concerns, the LLM is 1) quantized, 2) input tensors are loaded with 16-bit precision, 3) and a beam width of 1 (greedy search) is used for answer generation. The code is found at https://github.com/awxlong/manifold-medvqa/tree/main</p>]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><summary type="html"><![CDATA[my thoughts on a visual-question answering pipeline inspired by the Society of Minds]]></summary></entry><entry><title type="html">Thoughts on How to Build the Virtual Cell with Artificial Intelligence</title><link href="https://awxlong.github.io/blog/2024/thoughts-deepmind-cell/" rel="alternate" type="text/html" title="Thoughts on How to Build the Virtual Cell with Artificial Intelligence"/><published>2024-09-21T11:42:00+00:00</published><updated>2024-09-21T11:42:00+00:00</updated><id>https://awxlong.github.io/blog/2024/thoughts-deepmind-cell</id><content type="html" xml:base="https://awxlong.github.io/blog/2024/thoughts-deepmind-cell/"><![CDATA[<h1 id="thoughts-on-how-to-build-the-virtual-cell-with-artificial-intelligence-priorities-and-opportunities">Thoughts on <em>How to Build the Virtual Cell with Artificial Intelligence: Priorities and Opportunities</em></h1> <blockquote> <blockquote> <p>‚Ä¶ [J]ust as mathematics turned out to be the right description language for physics, AI may turn out to play a similar role for biology - Demis Hassabis, CEO of DeepMind</p> </blockquote> </blockquote> <p>A very interesting position preprint on how to build a virtual cell has been published on arxiv: https://arxiv.org/abs/2409.11654. I‚Äôve previously came across this idea of virtual cell from DeepMind CEO Demis Hassabis: https://www.youtube.com/watch?v=AuO0Y8Iwq0E. Since DeepMind built <a href="https://alphafold.com/">AlphaFold</a>, a DNA sequence üß¨ to 3D protein protein structure model, I guessed that its research team would be trying out to build the virtual cell by synthethizing a population of proteins interacting with each other. In the end, a cell is technically <em>a bag of proteins interacting with each other</em> ü¶†.</p> <h1 id="first-impressions-on-the-preprint">First impressions on the preprint</h1> <p>There are many novel ideas proposed throughout the preprint. One of the main approaches proposed in this position paper is to build a foundation model (let‚Äôs call it AI Virtual Cell) which can produce embeddings at multiple scales: molecular, cellular and tissue-level universal representations. Such foundation model would be trained with multimodal data spanning genomic information (sequence data), fluorence microscopy (imaging), single-cell RNA sequencing data (sequencing data), multiplex imaging (imaging), spatial transcriptomics (spatially-resolved sequencing data, or imaging + sequencing data), among others. See the figure below</p> <figure> <img src="/assets/img/foundation-model-cell-embedder.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="cell-embedder">On the left, multiple modalities of data spanning genetics, fluorescence microscopy to spatially-resolved omics, are used to train a foundatio embedder. On the right, it is depicted how an input can be transformed into a universal representation across 3 scales: molecular, cellular and tissue. Figure is cropped from the original article at https://arxiv.org/abs/2409.11654 (without the original authors' permission by the way üòÖ)</figcaption> </figure> <p>I understand it that such universal representations, which are essentially feature vectors $f \in \mathcal{R}$, can be used for some downstream tasks. The authors call the (neural network-based) models which use these feature vectors as ‚Äúvirtual instruments‚Äù, akin to digital toolsüî¨ to run in-silico experiments üß´. If you have some temporal dataset that shows how genes change through mutations during metastasis, the idea is that you can use the AI Virtual Cell to obtain feature vectors of the genetic sequences, and train a network to predict the changes of genes across time.</p> <p>While I understand that the need of feature vectors is ubiquituous for any machine/deep learning task, I disagree this is enough to build a virtual cell. On one hand, there are standard caveats to representation learning and building foundation models that we must be wary of. First, we have no control on what artifacts of the dataset a deep learning model is learning, and in biology it‚Äôs extremely difficult to distinguish noise versus actual biologically meaningful perturbations. The training of such foundation model would have far-reaching, biomedical implications, and as such extra-care must be taken for training this AI Virtual Cell embedder.</p> <p>My other problem is that this position paper seems to mainly rely on statistical approaches to build this virtual cell and it‚Äôs constrained to building a model to produce representations, a form of bottom-up learning from biological raw data. It‚Äôs true that mechanistic modelling has limitations such as needing prior expertise/assumptions constrained to a simplistic setting such as modelling the growth of a population of cells assumming they want to replicate. However, I think mechanistic modelling, also known as a top-down modelling, will play a bigger role in building a virtual cell. Prof. Xavier Trepat offers a very good <a href="https://www.nature.com/articles/d41586-018-07246-8">account</a> on the complementarity of top-down and bottom-up processes to build a virtual cell. Consider the following thought experiment to note the importance of both approaches: top-down modelling allows us to explain high-level processes like traffic jams. While bottom-up processes are powerful since it allows us to thoroughly simulate and understand how a single car work, it at most help us know how a single car work, not necessarily how a population of cars lead to high-level problems. In biomedicine, if we want to further scientific discovery, mechanistic insight is indispensable for its interpretability and explanations to high-level phenomena. In that regard, deep learning and mechanistic modelling can complement each other to build a virtual cell.</p> <h1 id="neuro-symbolic-ai-for-building-next-generation-virtual-cells">Neuro-symbolic AI for building next-generation virtual cells</h1> <p>In one of the most interesting articles I‚Äôve read recently on <a href="https://www.sciencedirect.com/science/article/pii/S0006349523002369">Building the next generation of virtual cells to understand cellular biology</a>, I think a paradigm that can merge deep learning with mechanistic modelling‚öôÔ∏è is neurosymbolic AI, in particular neurosymbolic programming.</p> <p>Instead of focusing on training foundation model embedders, we can think of training a model that can output programs, where programs take as input raw biological data like a genetic sequence, and outputs a sequence of programs that can transforms the raw biological data. In the metastasis example above, the programs could explain to us how metastasis happens. See the depiction below:</p> <figure> <img src="/assets/img/nesy-cell.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="cell-embedder">A depiction of a neurosymbolic programming approach to virtual cell. Consider a single nucleotide sequenceüß¨ changing over time. A neural-network driven search of programs takes the sequence as input, and outputs a sequence of program or steps that transforms the input sequence to a final state. The sequence of steps serve as an explanation to the final state, which could correspond to cancer. </figcaption> </figure> <h1 id="inverse-reinforcement-to-model-cell-behavior">Inverse reinforcement to model cell behavior</h1> <p>Another approach to building a virtual cell which I find very interesting and promising is by integrating inverse reinforcement learning (RL) as argued in this article: https://www.frontiersin.org/journals/systems-biology/articles/10.3389/fsysb.2024.1333760/full. One caveat in mechanistic modelling is its strong assumptions, for example, ‚Äúcancer cells metastasize because they want to replicate‚Äù. This is <em>disputable</em>, what if the goal of cancer cells was different, and more nuanced than intially assummed. Inverse RL can alleviate this modelling assumption by not specifying some reward for a particular goal, but derive the reward function from observe behavior from cancer cells. The above paper proposes the following diagram, in which the first step on transforming the microscopy image to a cell state can make use of the above AI Virtual Cell embedder.</p> <figure> <img src="/assets/img/irl.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="cell-embedder">A diagram of how to use inverse reinforcement learning to model cell behavior in a dynamic feedback loop with wet-lab experiments to validate model predictions. </figcaption> </figure> <p>There are many approaches to building a virtual cell, and I think a combination of the above methods: deep learning, neurosymbolic AI and reinforcement learning, among others, will be needed together to build a virtual cell ü¶†. How exciting times we‚Äôre living‚öõ. <strong>What do you think of the virtual cell?</strong></p> <h3 id="a-brief-thought-on-serendipity-">A brief thought on serendipity üß†ü™ê</h3> <p>Many methods in AI like neurosymbolic AI or reinforcement learning are not originally designed to solve biomedical tasks like single-cell data analysis, in a way such as a sequence alignment algorithm is tailored for identifying similarities in gene sequences or a deep network architecture is assembled to solve a medical imaging reconstruction task. At its core, research in AI is to understand how the brain works. Despite pursuing how human intelligence functions, this can nonetheless can yield unexpected insights into tackling problems in completely unrelated fields like network biology or cell science.</p> <p>The above statement is supported by anecdotal evidence where scientific research translated into overarching technological applications for social good on completely unrelated fields. My confidence on such an idea can come from Prof. Geoffrey Hinton‚Äôs <a href="https://www.youtube.com/watch?v=qpoRO378qRY">interview with CBS</a>, among the endeavors of other researchers pursuing science whose work was converted into technologies benefiting humankind. Efforts include:</p> <ul> <li><a href="https://arstechnica.com/health/2023/10/after-being-demoted-and-forced-to-retire-mrna-researcher-wins-nobel/">Prof. Katalin Karik√≥</a>, whose pursuit in understanding the central dogma of biology, namely how messenger-RNA translates into proteins, laid the foundation for the design of mRNA vaccines against the coronavirus during the COVID-19 pandemic.</li> <li>Prof. Yann LeCun, Prof. Yoshua Bengio, Prof. J√ºrgen Schmidhuber and Prof. Geoffrey Hinton himself, whose research into how the human brain works, such as how to simulate the visual cortex, how to reproduce human language, how can a machine achieve self-reference or how machines can learn like human beings, kickstarted the Third Revolution of Artificial Intelligence that can translate into a plethora of applications spanning over biomedicine</li> <li>Prof. Noam Chomsky‚Äôs work on a universal grammar underlying Human thought for understanding human language, which helped inspire work like the design of the <a href="https://en.wikipedia.org/wiki/Noam_Chomsky#Reception_and_influence">FORTRAN programming language</a></li> <li>And many more examples, where who knows how research on intelligence can help with simulating a biological cell.</li> </ul> <p>If the above anecdotes remind you of the book <a href="https://link.springer.com/book/10.1007/978-3-319-15524-1">Why Greatness Cannot Be Planned</a> by Prof. Kenneth O. Stanley and Joel Lehman, then there is another perspective from which to support the idea that basic science can fuel technological progress. We don‚Äôt know what are the stepping stones for achieving technological progress. Therefore, to achieve progress, we may have to be creative in the questions we ask and formulate novel ideas hoping and rely to some extent serendipity, that some of them will translate into groundbreaking innovations.</p> ]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><summary type="html"><![CDATA[my comments various papers on how to build a virtual cell]]></summary></entry><entry><title type="html">phenotype = (genotype + environment) * consciousness. Reflections on The consilience of knowledge by Edward O. Wilson</title><link href="https://awxlong.github.io/blog/2024/consilience/" rel="alternate" type="text/html" title="phenotype = (genotype + environment) * consciousness. Reflections on The consilience of knowledge by Edward O. Wilson"/><published>2024-06-22T08:42:00+00:00</published><updated>2024-06-22T08:42:00+00:00</updated><id>https://awxlong.github.io/blog/2024/consilience</id><content type="html" xml:base="https://awxlong.github.io/blog/2024/consilience/"><![CDATA[<h1 id="why-consilience"><strong>Why Consilience?</strong></h1> <p>The <strong>consilience</strong> of knowledge essentially means finding the principles which underlying two or more distinct branches of knowledge. People who subscribe to the idea that knowledge, in its many shapes and forms, is intimately connected, can be referred to as ‚Äúconsilientists‚Äù. We are often in awe by questions such as ‚Äúwhat‚Äôs the fundamental question?‚Äù to ask in particularly field of knowledge such as math, or artificial intelligence. The unknowns behind such fundamental questions, along with their answers, would shed insight into how seemingly disparate fields like AI and, say, cuisine, are deep down interconnected. The intrisic value of asking fundamental questions, and gauging principles of knowledge rather than superficial forms of knowledge, is that while consilientists can‚Äôt know everything, they can <em>know the things that help them understand most things</em>.</p> <p>Consilientists are lateral thinkers, meaning that they like to focus on the ‚Äúbig picture‚Äù, as in how a piece of knowledge fits with other knowledge, instead of compromising oneself to understanding thoroughly this piece of knowledge. To achieve consilience, it is required to know something about everything, and find a common principle that could explain most things. It is not being a polymath, i.e., knowing everything. It is being open to several ideas, and that ideas that seemingly contradict each other, may actually complement once we think deeply enough about them. For example, the empiricist vs. nativist debate in cognitive science (https://plato.stanford.edu/entries/innateness-cognition/) . To me, while empiricism and nativism are advertised as opposing ideas, in the eyes of a consilient researcher, they‚Äôre complementary. This is because the bottom-up epistemological approach of empiricists is intricately tied to the top-down perception of the world advocated by nativists, as depicted below.</p> <figure> <img src="/assets/img/induction_deduction.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="bottleneck">A depiction of inter-relation between induction and deduction. One can't process data without an a-priori model (theory-landeness). The model is data-dependent, and is part of an active loop of information exchange. </figcaption> </figure> <p>Another seemingly dichotomy is that of questions and answers. Problems are, in essence, solutions. This is because we can only ask questions for which we can have the answer, or we ask the questions based on the answer we can obtain. Consilientists questions can be of the form:</p> <ul> <li> <p>Are two seemingly different ideas just two faces of the same coin?</p> </li> <li> <p>Are two ideas that are often pitted against each other complementary to each other, rather than contradictory?</p> </li> </ul> <p>There are different ways to achieve consilience, i.e., to search for an universal principle, or a theory of everything.</p> <p>Prof. Edward O. Wilson mainly advocates for common principles through an empiricist/reductionist, excluding approaches such as rationalism or deduction. In the literature, empiricists and rationalists are commonly pitted against each other, on the assumption that acceptance of empiricism negates rationalism. But as argued in point 2 above, to my opinion, they not only are not against each other, they complement each other. Empiricist can explain how our mind can learn from external data, but can‚Äôt explain how we reason about math, simply because math doesn‚Äôt exist in the real world. Nativists can answer how we reason about math by arguing it is endowed by our genes after some random mutation in that marked the beginning of the Homo sapiens species, but this doesn‚Äôt mean that they can reject the relevance of learning from environmental cues. I‚Äôm personally critical of reductionism or empiricism (for a better take please read David Deutsch‚Äôs The Beginning of Infinity). The problem with reductionism is its lack of explanatory power for meaningful phenomena like universal cultural values, history of the emergence of nation-states, or love. At its core, reductionism aims at proposing as a theory of everything the interplay of molecules and fundamental forces, which equifinal outcome make them meaningless to explain any phenomena. The problem with empiricism is that one often shapes hypothesis and instruments of measurement based on the evidence one wants to collect.</p> <p>In the book by Edward Wilson, he discussed the fundaments which tie the biological and the social sciences, of which can be succintly summarized as:</p> \[phenotype = genotype + environment\] <ul> <li>phenotype encapsulates the physical, behavioral and emotional trait of an individual organism.</li> <li>gene refers to the selfish gene, or mutations, that are inherited from progenitors. Genetics is also what predisposes us to fight against each other, and our consciousness can either amplify this predisposition for war or tone it down.</li> <li>environment refers to the geographical constraints in which an organism lives and influences how a gene expresses, and thus how an organism develops. Epigenetic rules or epigenesis is heredity and environmental factors</li> </ul> <p>The beauty of such equation rests in the infinite amount of knowledge which hides behind such abstract simplicity. Phenotype is regarded as an abstract term with which we describe the human or humans‚Äô conditions, the subject of study of the social sciences.</p> <p>I have some criticism of this equation, in that it doesn‚Äôt account for the uniqueness of human beings. The equation is ‚Äòuniversal‚Äô to all living beings, such as animals. One phenotypic trait is intelligence, with which from the equation we can argue that it is intimately endowed by the genes and environment. However, I don‚Äôt believe that animals have the same competence as us humans, and instead propose the following equation, which just slightly modifies the equation above:</p> <h1 id="phenotype--genotype--environment--consciousness"><strong>\(phenotype = (genotype + environment) * consciousness\)</strong></h1> <p>Marvelous! Consciousness, which itself is something which we don‚Äôt understand nor define, can explain the uniqueness of us human beings: we have more ‚Äúconsciousness‚Äù than non-human animals. The consciousness in this equation is an umbrella term to encapsulate many other terms: creativity, hypothesis making, mathematical wisdom, emotions, qualia, among others.</p> <p>I personally add consciousness as an additional component to epigenetic rules which influences our phenotype. I placed it in the right hand side of the equation, instead of the left hand side, on the assumption that it is not a physical property. This is debatable nonetheless. Consciousness refers to our mental capacity (call it intelligence if you want) which can shake off the shackles of our genes and environment. Consciousness, or the capacity to stay conscious, is the central drive of humans to domesticate other species, nature itself and, through science and technology, pursue a meaningful life beyond what the selfish gene and environment dictate. Unlike animals, with a lower degree of consciousness (or hunekers as Douglas Hofstadter would call), then non-human animals are only able to act according to their epigenetic rules. Consciousness allowed us humans to alter drastically our environment, to the point of compromising our long-term survival. Soon, it would also allow us to modify our very that predisposes to a large extent our phenotype.</p> <p>The equation essentially states the human condition (the phenotype) is the result of the human‚Äôs genome, environment in which he developed, everything moderated by consciousness. You may have seen the equation $phenotype = genotype + environment$. By adding the $\times consciousness$ portion, we illustrate the difference between us and the non-human animals. Namely, the condition of non-human animals, which may have a low-degree of consciousness, are mostly determined by their genes and environment. We, however, can <em>influence both our genes and environment</em> thanks to our consciousness another thought experiment is that the consciousness variable could be universal, as in, consciousness is the same ‚Äúprinciple‚Äù for all humans, but it is personalized because of each human‚Äôs unique gene and environment.</p> <p>Another interesting thought experiment derived from the equation is that the consciousness variable is perhaps universal, rather than localized to each individual. This entails the existence of a ‚Äúcollective consciousness‚Äù, and each individual is a personalization of such collective consciosness dependent on the particular characteristics of our randomly delegated genes and allocated environment.</p> <h2 id="some-comments-on-the-comparisons-between-humans-and-non-humans">Some comments on the comparisons between humans and non-humans</h2> <p>Another thread of thought spun from the equation delineates the human condition with the non-human condition. Namely, we are <em>fundamentally</em> distinct to non-human animals, despite several human beings stating otherwise. We argue that animals don‚Äôt display intelligent behavior in the sense of intelligence meant by ‚Äòhuman intelligence‚Äô.</p> <p>Perhaps we perceive intelligence in non-human animals because a particular trait of our consciousness (these mysterious mental machinations) is to story-tell, leading us to perceive higher intelligence in other non-human animals when there isn‚Äôt, because non-human animals are, in that regards, slave to their genes and environment.</p> <p>A dolphin or dog that can seemingly count, a bonobo that can solve puzzles, a herd of elephants that supposedly organize a funeral or squid that can squirt to anger human spectators is perhaps not evidence of cognition or consciousness in non-human animals, at least at the same degree of their human counterparts, but rather behavior that‚Äôs pre encoded in their genes. What is impressive, in my opinion, is our brains‚Äô capability to craft stories, stories which are causal interpretations of these natural events:</p> <div class="embed-container"> <iframe width="560" height="315" src="https://www.youtube.com/embed/" frameborder="0" allowfullscreen=""></iframe> </div> <p>dolphin ‚Äòcounting‚Äô</p> <div class="embed-container"> <iframe width="560" height="315" src="https://www.youtube.com/embed/" frameborder="0" allowfullscreen=""></iframe> </div> <p>octopus ‚Äòopening jar‚Äô</p> <div class="embed-container"> <iframe width="560" height="315" src="https://www.youtube.com/embed/" frameborder="0" allowfullscreen=""></iframe> </div> <p>elephants ‚Äòmourning‚Äô</p>]]></content><author><name></name></author><category term="blog-post"/><summary type="html"><![CDATA[my afterthoughts on The consilience of knowledge, a book by Edward O. Wilson]]></summary></entry><entry><title type="html">Reflections on the Hacking the Human Vasculature Kaggle Competition</title><link href="https://awxlong.github.io/blog/2024/hack-vasculature/" rel="alternate" type="text/html" title="Reflections on the Hacking the Human Vasculature Kaggle Competition"/><published>2024-03-09T08:42:00+00:00</published><updated>2024-03-09T08:42:00+00:00</updated><id>https://awxlong.github.io/blog/2024/hack-vasculature</id><content type="html" xml:base="https://awxlong.github.io/blog/2024/hack-vasculature/"><![CDATA[<figure> <img src="/assets/img/nexus_1.jpeg" alt="Sorry. Image couldn't load." width="auto" height="auto"/> <figcaption id="cell-embedder">Presentation of our segmentation project at UCL Nexus Lab Sympossium </figcaption> </figure> <p>George Tang and I participated in the SenNet + HOA Hacking the Human Vasculature <a href="https://www.kaggle.com/competitions/blood-vessel-segmentation">Kaggle Competition</a>. We didn‚Äôt manage to submit good results prior to the competition deadline üòÖ. However, after the competition deadline, we managed to obtain a competitive segmentation DICE score that could have earned us ~10th place in the competition.</p> <h1 id="training-the-model">Training the model</h1> <ul> <li>My training notebook is at: https://www.kaggle.com/code/awxlong/hack-vasculature-transfer-learning <ul> <li>It consists of adapting the nnUnet proposed in https://github.com/MIC-DKFZ/nnUNet loaded with pretrained weights on hepatic vessel segmentation downloaded from https://zenodo.org/records/3734294/files/Task008_HepaticVessel.zip?download=1</li> <li>Finetuning the nnUnet architecture to the provided dataset in the competition for 15 epochs</li> </ul> </li> <li>My inference notebook is at: https://www.kaggle.com/code/awxlong/hack-vasculature-transfer-learning-inference <ul> <li>It consists of loading the weights trained above and inferring over the test dataset.</li> <li>We adopt <a href="https://www.kaggle.com/competitions/blood-vessel-segmentation/discussion/475074">post-processing steps</a> by 3rd place winner (shout-out to ForcewithMe) which consists of tuning a threshold for binarizing the segmentation mask which helped us boost the performance of our model up to a competitive 0.67 Dice score. For reasons unknown, without their post-processing step, my segmentation DICE score was stuck at ~0.001.</li> </ul> </li> </ul> <h1 id="reflection">Reflection</h1> <ul> <li>Prior to transfer learning, I‚Äôve attempted finetuning the foundational model MedSAM adapted from https://github.com/bowang-lab/MedSAM. My validation DICE score was stuck at ~0.18, and I argue this is because foundational models are harder to fine-tune as well as the this task was ‚Äúsyntactically‚Äù different to what the foundational model was previously trained on. Blood vessels are thinner, and resulting segmentation masks are more sparse compared to the datasets that MedSAM was trained on, which were most likely images of larger organs. ‚ÄòSemantically‚Äô however, MedSAM would have been very appropiate for this task because technically it had ‚Äòmedical‚Äô knowledge on what organs are.</li> <li>My personal advice is that if you opt for transfer learning, focus on the ‚Äòsyntax‚Äô of the task some model was trained on. In my case, I chose nnUnet‚Äôs hepatic blood vessel segmentation because I noticed the masks it was previously trained on are ‚Äòsparse‚Äô and ‚Äòthin‚Äô. I wouldn‚Äôt prefer, for instance, some segmentation model pretrained on ‚Äòocular blood vessels‚Äô because even though they are ‚Äòsemantically‚Äô the same task, syntactically, their segmentation masks are thin, but they weren‚Äôt sparse.</li> </ul> <h1 id="presentation">Presentation</h1> <figure> <img src="/assets/img/nexus_2.jpeg" alt="Sorry. Image couldn't load." width="auto" height="auto"/> <figcaption id="cell-embedder">Our segmentation project's first proposed solution at UCL Nexus Lab Sympossium </figcaption> </figure> <p>The slides for the presentation for the Nexus Lab Sympossium on March 9th are attached below:</p> <style>.pdf-embed-wrap-d4ce1dd6-5d47-4f1e-a58f-dc3d3acc84d6{display:flex;flex-direction:column;width:100%;height:650px}.pdf-embed-container-d4ce1dd6-5d47-4f1e-a58f-dc3d3acc84d6{height:100%}.pdf-link-d4ce1dd6-5d47-4f1e-a58f-dc3d3acc84d6{background-color:white;text-align:center;border-style:solid}.pdf-embed-container-d4ce1dd6-5d47-4f1e-a58f-dc3d3acc84d6 iframe{width:100%;height:100%}</style> <div class="pdf-embed-wrap-d4ce1dd6-5d47-4f1e-a58f-dc3d3acc84d6"> <div class="pdf-link-d4ce1dd6-5d47-4f1e-a58f-dc3d3acc84d6"> <a href="/assets/pdf/hacking-the-human-vasculature.pdf" target="_blank">View PDF</a> </div> <div class="pdf-embed-container-d4ce1dd6-5d47-4f1e-a58f-dc3d3acc84d6"> <iframe src="/assets/pdf/hacking-the-human-vasculature.pdf" frameborder="0" allowfullscreen=""></iframe> </div> </div>]]></content><author><name></name></author><category term="blog-post"/><summary type="html"><![CDATA[transfer learning from nnUnet's hepatic vessel segmentation to renal vessel segmentation]]></summary></entry><entry><title type="html">A probabilistic circuit for imputing missing tabular data</title><link href="https://awxlong.github.io/blog/2023/pc-imputation/" rel="alternate" type="text/html" title="A probabilistic circuit for imputing missing tabular data"/><published>2023-12-22T08:42:00+00:00</published><updated>2023-12-22T08:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/pc-imputation</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/pc-imputation/"><![CDATA[<h1 id="spns-for-imputing-data">SPNs for imputing data</h1> <p>The following is a Jupyter notebook ran on Google Colab using Kaggle‚Äôs Titanic dataset to illustrate a practical use case of a probabilistic circuit, a sum-product network, from <a href="https://github.com/deeprob-org/deeprob-kit">deepprob-kit</a>: to impute missing tabular data.</p> <p>This notebook is forked from a Kaggle <a href="https://www.kaggle.com/code/ttminh27/using-autoencoder-to-impute-missing-data">tutorial</a> on using a Tensorflow‚Äôs autoencoder to impute missing data. Standard autoencoders can‚Äôt:</p> <ul> <li>custom fill-in missing data</li> <li>flexibly incorporate domain knowledge like what distribution is best used to model a feature</li> <li>Furthermore, p;robabilistic circuits can tractably compute missing data through maximum a posteriori estimation.</li> </ul> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/using_pc_to_impute_missing_data.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="class-notes"/><category term="food-for-thought"/><summary type="html"><![CDATA[using a sum-product network to flexibly impute missing data in Kaggle's Titanic dataset]]></summary></entry><entry><title type="html">On the Turing Test and Large Language Models</title><link href="https://awxlong.github.io/blog/2023/turing-test/" rel="alternate" type="text/html" title="On the Turing Test and Large Language Models"/><published>2023-12-09T09:42:00+00:00</published><updated>2023-12-09T09:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/turing-test</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/turing-test/"><![CDATA[<h1 id="on-benchmarks-of-human-intelligence">On benchmarks of Human intelligence</h1> <p>There is a plethora of claims surrounding the emergence of actual or surpasssing Human intelligence by Large Language Models. One, by one, these contextualized autocomplete models are given the labels of having <a href="https://arxiv.org/abs/2305.03731v2">working memory</a>, <a href="https://analyticsindiamag.com/text-is-a-projection-of-the-world-says-openais-sutskever/">compresses</a> a <a href="https://www.linkedin.com/posts/armand-ruiz_llms-do-more-than-predict-the-next-word-activity-7134512576318623744-7BlG?utm_source=share&amp;utm_medium=member_desktop">‚Äúworld model‚Äù</a>, or it signals <a href="https://arxiv.org/abs/2303.12712">sparks of Artificial General Intelligence</a>.</p> <p>There are a list of benchmarks backing up their claims. These benchmarks come with questions-answers pairs, where high performance signifies that a model is able to simulate a faculty of Human intelligence like language.</p> <p>However, most if not all benchmarks are tailored for what LLMs can do, even though they‚Äôre supposed to be designed without having LLMs in mind. A benchmark that tests a faculty of Human Intelligence like ‚Äúreasoning‚Äù should not only explore define reasoning as . This is in contrast to designing theory-laden benchmarks. No one knows the exact definition of Human capacities like ‚Äúreasoning‚Äù, so the design of a benchmark must have assumptions on what are the definitions of whatever capacity it is measuring. The key argument of the present article is that such assumptions have been constrained to framing every faculty as the prediction of the next token, where token can be the next word, image or audio signal. I don‚Äôt even know what faculties like ‚Äúmemory‚Äù, ‚Äúreasoning‚Äù, ‚Äúplanning‚Äù are. But I do know as many researchers may conclude after years of work on this, that it is <em>not just contextualized, next-state prediction</em>.</p> <p>Because all benchmarks have been designed based on next-token prediction, it is not surprising that a next-token predictor can perform well. Quiet the opposite, it would be surprising if a next-token predictor can‚Äôt perform well on an auto-regressive task. However, while Human Intelligence most likely consists of next-token prediction, it would be misleading to suggest it <em>only</em> consists of this process. By that line of thought, LLMs haven‚Äôt even passed the Turing Test, at least by its <a href="https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence">original formulation</a>. This is because while it is true that the original objective was for a machine to imitate a human and fool an interrogator, a feat that LLMs can already achieve by producing text that is indistinguishable from Human-produced text, it is often ignored that the Turing Test is executed via a <em>physical</em> exchange of written letters. I don‚Äôt think this is a trivial matter that can be glanced over, as it is very important to note that LLMs can not write, since they are not instantiated in spacetime, nor have a body with which they have to face the challenge of sensorimotor control in a continuous feedback loop.</p> <p>A benchmark of a faculty of Intelligence shouldn‚Äôt be designed with only one theory of what Intelligence is, i.e., prediction of next-token. This kind of practice is akin to framing questions based on the answer one expects, instead of asking open-ended questions for the sake of curiosity. As such, if I may borrow the sensationalist attitudes of some media personalities, I just want to say ‚Äúall these benchmarks are wrong, and none of them are useful<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> to understand Intelligence‚Äù.</p> <h2 id="prediction-error-minimization-is-a-strong-hypothesis-not-overarching">Prediction Error Minimization is a strong hypothesis, not overarching</h2> <p>I do want to clarify: I think <a href="https://www.fil.ion.ucl.ac.uk/~karl/Whatever%20next.pdf">prediction error minimization (PEM)</a> as an integral part of Human intelligence is a resilient hypothesis. Active inference, for instance, states that our brains are constantly predicting the source of the signals we perceive, and consciousness is the result when we infer that the cause of some signals in the environment is caused by ourselves. PEM also tries to explain other phenomena like dreaming, arguing that it is a form of ‚Äúreverse-learning‚Äù that consists of a process of flushing irrelevant memories to while strengthening important synaptic connections, altogether allocate mental resources to make better predictions afterwards. Another important strength of the PEM hypothesis is that the abstract process of ‚Äúlearning‚Äù is grounded to physical mechanisms like chemico-modulated, neuronal activations. I believe these are very strong hypothesis that can withstand harsh criticism. I simply think PEM is not enough to explain the entirety of the Human condition.</p> <figure> <img src="/assets/img/llm_shouting.jpg" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="llm"> Image generated by Dall-E powered MS Bing Image Creator given the prompt "a large language model shouting" </figcaption> </figure> <p>My main criticism of PEM is that while it is designed to explain Human intelligence, it seems that it makes no distinction between non-Human animal intelligence and Human intelligence. The non-Human animal brain also has neurons firing, and the PEM may well apply to non-Human animals and Humans alike. However, is that enough? Are we Humans really just a slightly more intelligent animal. I argue it is absolutely not. Non-human animals can‚Äôt reflect on the mysteries of the mechanisms of the universe, perform scientific experiments, do creative thinking, believe in currency exchange and laundering, or any activity beyond the shackles of genetic predisposition, among others. And furthermore, I don‚Äôt think PEM explains neither of the aforementioned capacities.</p> <p>I just hope sensationalist claims in media stay as they are: ‚Äúsensationalist‚Äù. They‚Äôre only meant to spark sensation, not actually to determine the course of public-policy making (at least not now), instilling fear of Humanity‚Äôs imminent destruction or even <a href="https://www.youtube.com/watch?v=lfXxzAVtdpU">existential crisis</a> on what humanity is. What I‚Äôm most worried about is that these sensationalist claims limit our own perspective on how amazing Human intelligence is, and that it is beyond PEM.</p> <p>As previously said, I don‚Äôt know what Human intelligence is in its entirety, but I‚Äôm confident it at least includes in addition to learning: reasoning, sensorimotor control, creativity/imagination, curiosity, consciousness, intrapersonal understanding, interpersonal understanding, among others.</p> <h1 id="proposing-an-extended-turing-test">Proposing an Extended Turing Test</h1> <p>To share some thoughts on a more comprehensive Turing Test, I do want to clarify that it is designed to measure Intelligence, not accuracy of next-predicted tokens. As such, a preliminary concept to convey concerns <a href="https://hrstraub.ch/en/the-theory-of-the-three-worlds-penrose/">Roger Penrose‚Äôs 3 worlds</a>: 1) Platonic, 2) Physical and 3) Subjective. Almost all existing benchmarks measuring any faculty of Human Intelligence, such as to produce language, focus on evaluating in the Platonic space, the space of abstract ideas. There do exist benchmarks measuring faculties in the physical space, such as in robotics, with examples including (cute) <a href="https://www.youtube.com/watch?v=RbyQcCT6890">robots playing football</a> against one another <sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, however, these benchmarks then don‚Äôt measure faculties in Platonic space.</p> <p>I will also focus on the breadth of tasks, instead of the depth. This means the Extended Turing Test (better named Extended Turing Benchmark) has as many diverse tasks as possible, instead of having a single task modality, such as predicting the next text-token, but it is semantically diverse within this domain (predict the next token in English, in Spanish, in Chinese, among others). Following the main idea of this article, I won‚Äôt be focusing on next-token prediction either, though it is an integral part of it. I also note this is not meant to be the final Extended Turing Benchmark, and feedback is most welcomed.</p> <table> <thead> <tr> <th>Task</th> <th>Challenging aspects</th> </tr> </thead> <tbody> <tr> <td>Solve a mathematical conjecture, like Goldbach‚Äôs Conjecture</td> <td>The answer is not given in the training set</td> </tr> <tr> <td>Achieve a United Nation‚Äôs Sustainable Development Goal</td> <td>Requires interplay of Platonic and Physical space</td> </tr> <tr> <td>Find a cure for a disease of global concern</td> <td>Requires physical experimentation and searching over hypothesis space</td> </tr> <tr> <td>Propose a plan for a successful start-up</td> <td>Same as above</td> </tr> <tr> <td>Break a world record</td> <td>Same as above</td> </tr> <tr> <td>Ask questions</td> <td>Same as above</td> </tr> <tr> <td>Solve a conflict of global concern</td> <td>Requires interplay of Platonic, Physical and Subjective spaces</td> </tr> <tr> <td>Open-ended debate on moral dilemmas, such as the Trolley Problem</td> <td>No correct answer, requires proficiency in Subjective space</td> </tr> </tbody> </table> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I don‚Äôt actually mean all benchmarks are wrong. Some benchmarks have indeed yielded insight into what constitutes Human intelligence, and they are good attempts at quantifying faculties of intelligence like visual reasoning, and analogy-making.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>The match caused some sensation despite it is not played against humans, further exacerbating a point made earlier that sensorimotor control, in addition to PEM, is an integral part of Intelligence. As such, I can‚Äôt understand how and why autoregressive models are advertised as the key to Artificial General Intelligence.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="creative-work"/><summary type="html"><![CDATA[I argue we Humans are more than next-state prediction machines.]]></summary></entry><entry><title type="html">Concept-Bottleneck Modelling for Interpretable Melanoma Classification</title><link href="https://awxlong.github.io/blog/2023/concept-melanoma/" rel="alternate" type="text/html" title="Concept-Bottleneck Modelling for Interpretable Melanoma Classification"/><published>2023-12-04T16:42:00+00:00</published><updated>2023-12-04T16:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/concept-melanoma</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/concept-melanoma/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A research project adopting the concept-bottleneck modelling technique for interpretable melanoma classification.]]></summary></entry></feed>