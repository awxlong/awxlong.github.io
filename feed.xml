<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://awxlong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://awxlong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-02T00:48:29+00:00</updated><id>https://awxlong.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The many interpretations of terms in artificial intelligence</title><link href="https://awxlong.github.io/blog/2023/many-faces-ML/" rel="alternate" type="text/html" title="The many interpretations of terms in artificial intelligence"/><published>2023-11-01T13:42:00+00:00</published><updated>2023-11-01T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/many-faces-ML</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/many-faces-ML/"><![CDATA[<p>I bumped into this paper on <a href="https://twitter.com/ChristophMolnar">X (formerly Twitter)</a></p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">How do researchers define interpretability and explainability?<br/><br/>An overview:<a href="https://t.co/bNjKv8F44g">https://t.co/bNjKv8F44g</a> <br/><br/>Summary: No consensus<br/><br/>My opinion: It&#39;s already the Wild West, no one will stick to any definition. Most pragmatic is to treat them interchangeably. <a href="https://t.co/mEh6mUuk4C">pic.twitter.com/mEh6mUuk4C</a></p>&mdash; Christoph Molnar (@ChristophMolnar) <a href="https://twitter.com/ChristophMolnar/status/1718921189333045752?ref_src=twsrc%5Etfw">October 30, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>In today’s post, I don’t want to discuss interpretability or explainability, but rather elaborate on the above idea that in the AI literature, different authors express completely different ideas despite using the same words, often words that appear in our daily parlance, yet acquire uncommon interpretations.</p> <p>If you have just set foot into the world of artificial intelligence, here are my thoughts and advices on navigating this exciting, but often messy landscape.</p> <h1 id="terms">Terms</h1> <h4 id="explanation-hypothesis-model"><strong>Explanation, hypothesis, model</strong></h4> <p>When I began studying AI, the word “model” elicited the mental imagery of a 3D scale model of something. If I wanted a “model” of an atom, I could pick some recycled styrofoam balls, stick them together and, bam (pun intended), an atom is synthesized.</p> <p>Nowadays it mainly refers to a equation or function with some parameters $\theta$, where a ‘good’ model is defined as having optimal parameters $\theta$ that can make predictions with low error on the training and (hopefully) unseen, test data, compared to a ‘bad’ model.</p> <p>In rare cases, such as in <a href="https://www.youtube.com/watch?v=NsID1iM8gRw">Prof. Joshua Tenembaum’s talk on Cognition</a>, a (physics-) model can refer a simulator implemented in Unity.</p> <h4 id="knowledge"><strong>Knowledge</strong></h4> <p>This word may cause some confusion to peers with a philosophy or psychology background. At least to me, prior to studying AI, ‘knowledge’ meant pieces of information encoded in natural language, e.g. “Mount Chimborazo is actually higher than Mount Everest” somewhere in the brain. I don’t know how, nor where, nor whether it is encoded in natural language.</p> <p>At least in the early days of AI, first order logic constraints such as <code class="language-plaintext highlighter-rouge">smoker(X) :- edema(X)</code>, read as “it is true that if X is a smoker, then X has edema”, does resemble ‘knowledge’ encoded in a way that seems familiar to us.</p> <p>However, this is not always the case. The word ‘knowledge’, such as in ‘prior knowledge’, mainly used in Bayesian modelling, has not much to do with logic predicates or natural language. It mainly refers to what you believe on the distribution of the parameters of your model come from before your model is trained on any data. If you think your model must be parametrized by weights that more often take extreme values, you can set a prior belief with distributions with heavier tails. Guidance on what prior distributions can be used to describe your parameters can be checked in this <a href="https://twitter.com/bindureddy/status/1708664380987220427">tweext</a></p> <p>Lastly, sometimes researchers may even say that ‘knowledge’ of the data is encoded in the parameters of the model, which leads us to the next section:</p> <h4 id="explainability-and-interpretability"><strong>Explainability and interpretability</strong></h4> <p>Explainability and interpretability are terms that can be used interchangeably, as shown in the above paper. I avoid making a distinction in this post, and just focus on sharing what is usually meant by either term.</p> <p>The simplest manifestation of explainability can be observed with simple linear models like linear or logistic regression. Here explainability simply refers to the presence of coefficients estimated per each input variable of the model. These coefficients’ magnitudes often denote the contribution of the variable to the overall prediction of the model. Other non-linear, machine learning models such as decision trees are by design explainable, and such quality mainly refers to the thresholds appearing at each junction in the decision trees.</p> <p>Concerning deep learning, particularly convolutional neural networks (CNN), explainability can refer to the computation of feature maps on the input space to see on average, what are the filters of the CNN observing when solving a task like classification given an image. Deep generative models (DGM) that leverage neural networks and learn latent representations of the data can also become ‘explainable’ if there is some post-hoc (i.e. after training) processing of embeddings, such as dimensionality reduction through principal component analysis, that can map the embeddings on 2D space. In such space, it is hoped that the DGM learns disentangled features, albeit how are they interpreted is mainly up to the researchers, not by some objective reason.</p> <p>Explainability in the eyes of some authors may also refer to the capability of the researcher to compute a numerical value of the likelihood of the parameters of the data, or at least an approximation of it, such as the evidence lower bound. This scalar value can be used to compare the performance of different models, and these models can thus be referred to as “provable”.</p> <p>Other authors may also define explainability when deep neural networks, despite being black boxes, can accept inputs or display its predictions through a user-friendly interface that allows them to interact with the model.</p> <h1 id="bottom-line">Bottom line</h1> <p>My key advice in this brief article is to convince you not to trust the “intuitive” interpretation of a word, especially if it is used on a quotidian basis. For example, in my personal view, to be “explainable” means to be able to justify its own answer. Just like a friend to whom I ask why didn’t they go to a concert by singer Aimer, to be “explainable” (at least when referring to the common meaning of this word in daily life) means that they can give a justification as to why didn’t they go (maybe they became sick, or admit they have poor taste in music). ‘Explainable’ in the machine learning literature has not much to do with this daily life interpretation.</p> <p>Finally, the above terms are simply the tip of the iceberg. In the wider research literature, authors can diverge in what they mean by on many terms such as “language”, the word “intelligence” itself, and more controversial terms like “artificial general intelligence”. Because I don’t know the precise meaning of them myself, I refrain from commenting furthermore.</p> <p>Happy learning!</p>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><summary type="html"><![CDATA[Some thoughts concerning how to interpret quotidian words appearing in AI research in a non-quotidian way.]]></summary></entry><entry><title type="html">Structured Gradient Descent</title><link href="https://awxlong.github.io/blog/2023/sgd/" rel="alternate" type="text/html" title="Structured Gradient Descent"/><published>2023-10-31T13:42:00+00:00</published><updated>2023-10-31T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/sgd</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/sgd/"><![CDATA[<h1 id="a-tale-of-a-scientist">A tale of a scientist</h1> <p><em>Shirley wanted to become a scientist. She wanted to find about jumping behavioral patterns of <a href="https://en.wikipedia.org/wiki/Tree_cricket">tree crickets</a>, which are insects of the order Orthoptera. Specifically, she aimed in testing the hypothesis that tree crickets understood human-issued commands that ordered them “to jump”. She caught a bunch of crickets and started testing her idea by telling one group of crickets to jump, and leaving a control group of crickets telling them nothing. Most crickets in either group jumped, merrily and highly. To her, these results weakly suggest that crickets understood the order to jump and did so accordingly. Then for the same group of crickets in both conditions, she mercilessly cut their legs off. Again, she issued the command to jump for one group, and left the control without saying anything. None of the crickets jumped. What did Shirley the scientist conclude? “Crickets stop understanding human-language after having their legs chopped off”</em></p> <figure> <img src="/assets/img/cricket.jpg" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="sgd"> Depiction of a cricket generated by Dall-E powered MS Bing Image Creator given the prompt "a beautiful, photo-realistic cricket" </figcaption> </figure> <h1 id="its-not-her-fault-the-parallel-with-deep-neural-networks-learning">It’s not her fault: The parallel with deep neural network’s learning</h1> <p>Perhaps you’ve heard of the above story in a course introducing the scientific method.</p> <p>This tale is interesting given the parallels there exists with current deep neural networks. These are analogous to Shirley, their parameters a metaphor to what she learns, and her crickets resemble the training data. No one has any control with regard to what she learns given the training data. When she concluded that “crickets can’t interpret English after having their legs cut-off”, that is not a “wrong” interpretation, per se. Rather, reaching this conclusion is a simple consequence of how she was designed and the coincidences of the training data (i.e. crickets did stop jumping after their legs were chopped off despite Shirley’s instructions to jump).</p> <p>With respect to deep neural networks, you can train it to classify images in CIFAR-10 or FashionMNIST, or more generally, train to generate the images. As per what sort of embeddings it learns, how it learns, among other aspects of learning, are out of our control. Exemplified in <a href="https://arxiv.org/pdf/2011.12854.pdf">Stammer et al., (2021)</a>, in a ColorMNIST dataset where a ResNet learns to classify digits, if by chance most digits nines appeared with a purple color, then this ResNet would classify purple color digits as 9 in the test set, even though the digits may be of different shape. In such a case, the model has learned that the concept of color defines the identity of a digit, since we have no control over what artifacts does the ResNet pick.</p> <p>The main powerhouse driving learning is stochastic gradient descent, and it is partly due to this stochasticity that deep neural networks are black-boxes.</p> <p>The stochasticity of gradient descent is mainly derived from the order and the semantics of the incoming training data. We thus have no control over what the model learns. The AI research community has proposed several methods, such as trying to learn disentangled representations of data, which can be obtained by having a more intricate labelling of the dataset. Consider the <a href="https://cs.stanford.edu/people/jcjohns/clevr/">CLEVR Diagnostic Dataset</a>, in which each object appearing in each training image is exquisitely labeled with its (x, y, z) coordinates, shape, color, size, material and positional relations among others, along with questions, answers and functional programs to answer each question. A variant of such dataset with real-life scenes is <a href="https://arxiv.org/abs/1902.09506">GQA</a>.</p> <h1 id="structured-gradient-descent">Structured gradient descent</h1> <p>Given these precedents, I wanted to depict my desire to see in the future a kind of “structured” gradient descent (which in English coincides with the same acronym as stochastic gradient descent):</p> <figure> <img src="/assets/img/sgd4life.png" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="sgd"> Not an actual depiction of structured gradient descent, but nonetheless a cool logo. </figcaption> </figure> <p>Structured gradient descent, as its name suggest, would allow the training of a model where we have control over what kind of representation it obtains from the supplied data. Analogous to the tale above, it’d akin to explicitly telling Shirley that crickets can’t understand human language, and that her line of reasoning has to start from the motility of legs of tree crickets in giving them the ability to jump. Afterwards, the jumping behavior of crickets is up to her to explain.</p> <p>The unanswered questions that follow would include:</p> <ul> <li>How would structured gradient descent materialize in a learning algorithm? <ul> <li>Will a grammar, prior knowledge in the form of logic or external ontologies play a pivotal role in the design of such an algorithm?</li> </ul> </li> <li>Will structured gradient descent allow a model to learn a disentangled representation, along with benefits such as a lower demand for computational power, dataset size and model size? <ul> <li>What would be the demands for structured gradient descent to run? More intricate labeled data?</li> <li>Most importantly, would more intricately labeled data compensate for low data?</li> </ul> </li> </ul> <figure> <img src="/assets/img/structuredgd.png" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="sgd"> A (hopefully) cool logo of SGD</figcaption> </figure>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="creative-work"/><summary type="html"><![CDATA[Some thoughts on the concept of structured gradient descent and a piece of artwork associated to it]]></summary></entry><entry><title type="html">A tale of two worlds: the natural and the mathematical</title><link href="https://awxlong.github.io/blog/2023/tale-two-worlds/" rel="alternate" type="text/html" title="A tale of two worlds: the natural and the mathematical"/><published>2023-10-14T13:42:00+00:00</published><updated>2023-10-14T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/tale-two-worlds</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/tale-two-worlds/"><![CDATA[<p>There seems to exist two worlds which exist in the universe.</p> <p>The real, natural world which can be accessible through empirical means, and the abstract, mathematical world which are the means themselves with which we observe the real, natural world. Here, the real, natural world is what we can perceive with our senses. The mathematical world is an umbrella term to refer to the ‘math’ in our common day parlance, as well as including logic in its many forms: propositional, first-order and higher-order.</p> <p>The real, natural world is subject to the laws of physics, which underlie chemistry and biology. In the abstract, mathematical world, such laws are not present: there we can think of breaking Newton’s Universal Laws of Gravitation, the overall decrease of entropy against the Second Law of Thermodynamics, or, in general, anything within our mental capabilities<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">1</a></sup>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/world-math.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A depiction of the complementarity of the empirical, natural world and the abstract, mathematical world. The image is generated via Microsoft Bing's Image Creator with the prompt "on the left, a picture of the natural world. On the right, a picture of equations, geometry and calculus". I have tried several other prompts and chose the one I believed suits what I wanted to convey. As I have discussed [before](https://awxlong.github.io/blog/2023/brain-fractal/), as well as in this article, I've avoided abstract terms such as 'complementarity' or 'abstract' since I'm unconfident a large vision system can understand it. </div> <p>To recount this tale of two worlds, we can take a step back to ancient debate of <a href="https://plato.stanford.edu/entries/innateness-cognition/">nativism and empiricism</a><d-cite key="samet_2012_innateness"></d-cite>. Namely, nativism argues that knowledge emerges from axioms and inference rules applied to those axioms that we humans are endowed since birth. Elaborating, thanks to a random mutation that kickstarted the beginning of <em>Homo sapiens</em>, our species have grown a capacity to understand elements of the natural world in terms of its objectness, and its fluent properties, such as shape, numerosity and size. This capacity allow us to understand objects of the real world in terms of how they change, how they hierarchically relate to one another and how they causally relate to each other. This capacity contrasts to what empiricists argue, namely that we acquiere knowledge dependent on the experiencies we’ve been exposed, and determine (causal) relationships among objects of the real world based on the extrapolation of previous experiences.</p> <p>I argue this is a false dichotomy, and that underlying it there is the complementarity of nativism and empiricism. Nativism mostly deals in how the human mind navigates the internal, abstract, mathematical world. If we want to prove that there exists infinite primes, nativists would argue that such theory can be proved via universal inference procedures such as proof by contradiction, and manipulating symbols involving the definition of primeness of a number. In such an scenario, empiricism would say that we have to extrapolate from prior observations of prime numbers to conclude they’re infinite. However, who can live for infinite amount of time in order to witness the infinity of primes and assert the above theory! Another example would concern Goldbach’s Conjecture, which proposes that all even numbers greater than two are the sum of two primes <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">2</a></sup>. Can we prove it by observing all even numbers, as nativists would argue? I don’t believe so, we have to resort to the mathematical reasoning mechanisms we were endowed in our mind. One way is to prove it by <a href="https://en.wikipedia.org/wiki/Euclid%27s_theorem">contradiction</a>. Assume there aren’t infinite primes, multiply the finite primes, and add one. This new number is not divisible by any of the finite primes, and thus is a prime. Infinity is a concept easier to understand and grasp when it is defined, as it can’t be learnt.</p> <p>However, we <em>Homo sapiens</em> don’t only reason about the mathematical world. We <em>use math to understand the external world</em>, and understanding the external world recquire experiencing it, and with experience comes learning. Empiricist, thus, can explain how we are able to sort out the relationships of objects in the external world as a form of interpolating and extrapolating observed evidence. However, empiricists need nativists as much as the latter need the former. Empiricism need the structure of nativists to accommodate the perceptual stimuli of the external world in order to reason about their relationships and derive new ones. Nativists need to learn from external stimuli so that the structure can evolve and be able to describe more complicated relationships.</p> <p>The natural world needs math to be understood, and math needs the natural world to exist. From this complementarity, it is argued that the mind consists of a grammar or a domain-specific language with which to give <a href="https://plato.stanford.edu/entries/language-thought/">shape to thought</a><d-cite key="rescorla_2019_the"></d-cite>. This grammar would be the basis for parsing the world’s objects in terms of their objectness, fluent properties and causal relations. Additionally, this innate grammar should have stochastic properties as probability acts as the pivot to ground this grammar with the natural world, and be able to describe it in a coherent manner. For example, after observing past experiences of a ‘hammer’, we can probabilistically ground a high-level, symbol ‘hammer’ with the actual object in the real world. Accommodating ‘hammer’ within some structure allow us to reason about its relations, functions, and properties. For further reading of stochastic grammars for vision, you can check Prof. Zhu-Song Chun’s <a href="http://www.stat.ucla.edu/~sczhu/Books/Book_2_Parsing.pdf">textbook on this topic</a> or <d-cite key="zhu_2020_dark"></d-cite>, which I find extremely difficult to follow, but nonetheless an interesting manuscript to read.</p> <h1 id="implications-of-rejecting-either-nativism-and-empiricism">Implications of rejecting either nativism and empiricism</h1> <p>I advocate that there exists complementarity between the mathematical and natural world. However, I can’t stop others from opting for the dichotomy between them, thus not only pitting nativism against empiricism or the other way around, but also rejecting one at the cost of the other.</p> <p>If we ignore nativism, and assume past experiencies are all that is needed to understand how the mind deduces new knowledge, we can think of the following. We can’t give shape to the mathematical world. It doesn’t “exist” in the real world. Circles, polygons, numbers, operations, vectors, matrices, infinity, among others, all reside within our mind. A friend once pointed out that he can draw a line on a dining table<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">3</a></sup> with a marker, and that’s a geometrical line. I disputed: “that is ink placed on a surface, which in your head is representing a geometrical line. While the ink and wooden surface are tangible objects, the geometrical line isn’t. Miraculously, however, I can also see and understand what you’re thinking concerning geometrical lines, suggesting the universality of math in the minds of <em>Homo sapiens</em>” (I may have edited my response to sound wiser, as well as edited the whole story to make it more entertaining).</p> <p>That math doesn’t exist in the real world, and thus can’t be picked up with our perceptual systems can explain why large vision models like DALL-E or MidJourney struggle with generating images which contain letters or numbers, unless they are fed with prior knowledge on <a href="https://research.google/pubs/pub52496/">characters</a><d-cite key="liu_2023_characteraware"></d-cite>. This is partly because the vast amount of data in which they’re trained mainly correspond to images taken from the natural, tangible, external world, e.g., photos under different styles of animals, buildings, groups of people, or landscapes.</p> <p>We also can’t accept nativism at the expense of empiricism. The history of artificial intelligence is a good reference to observe that resorting to only expert systems consisting of universal inference rules like forward and backward chaining results in systems that are disconnected with real world dynamics. Whilst math can be independent of the real natural world, it is nonetheless intricately connected to the world in order to describe it.</p> <h1 id="implications-on-the-quest-for-artificial-intelligence">Implications on the quest for artificial intelligence</h1> <p>Deep learning ignited the summer of AI of the twenty-first century, and the transformer is one of the torches that keeps the summer shining. How should we keep this summer from ending?</p> <p>Deep learning is a mostly pure expression of empiricism. Deep neural networks don’t manipulate symbols, are inspired by the brain’s architecture, and are mainly learn via training from a dataset of past experiences. There are several empirical observations, and <a href="https://plato.stanford.edu/entries/scientific-reduction/">reductionist</a> <d-cite key="riel_2014_scientific"></d-cite> claims suggesting that mental activities such as intuition and reasoning are the result of the biochemical processes of the brain. There are the recurrent achievements of new state of the arts performance in benchmarks used to measure the human faculties of vision and language. One of the striking research papers that caught my attention involved decoding a person’s brain activity to their perceived image:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/decode.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The model's architecture, which is a diffusion model (i.e. a probabilistic model consisting of deep neural networks). On the right, the upper row are images presented to a person whose brain activity is recorded. This brain activity is then decoded to the below row of images. Image extracted from https://sites.google.com/view/stablediffusion-with-brain/ </div> <p>That there is some association between brain activity and external stimuli capturable by a probabilistic model is fascinating and encouraging for empiricism. However, most if not all research related to the neuroscience behind mental activity <strong>lacks the explanation of whether brain activity yields any insight into mathematical theorem proving or deduction</strong>. Experiments to explore this can involve assessing whether brain activity can reconstruct a person’s steps into solving an equation, deriving a proof or just in general have some abstract thought involving numbers.</p> <p>Furthermore, taking empiricism to an extreme may stifle the capabilities of an AI model, since radical empiricism subjects the model to be an extrapolation from supplied past experiences. It ignores the nativist capability of generating new thought, often called as creativity.</p> <p>Another counterargument to empiricism is also as follows. Image there is a program running on a computer. You can observe your program running on the screen, as in, you can see what input matches the corresponding output displayed. You also know the underlying algorithm specifying how the input should be transformed to the output. Now, you disassemble your computer and expose its mainframe, now able to look at the CPU and RAM running, all the while your program keeps running. In other words, only the computer screen is gone, so any high-level display your program disappears. You invite an empiricist and tell her that she can make all sorts of measurements she wants on the hardware of the computer (CPU, RAM, motherboard, graphics card). Such measurements could include voltage, heat maps, temperature, and whatever she can think of. Of course, because the computer screen is gone, and she is prohibited from reassembling it back, she is restricted to measure the low-level components of the computer. Now comes the main question, can she be able to figure out from measurements made on the hardware the algorithm being executed? Here, the computer and its low level components correspond to the biological brain, and the software is analogous to mental activity.</p> <p>While low-level activity can reflect to some degree higher level processes, I’m skeptical we can understand and simulate higher-level cognition without assuming its pre-existence. Therefore, we can only describe high level mental activity with high level mental activity. In the thought experiment, this would entail the empiricist may have to ‘hypothesize’ in a separate computer different programs that could be running in the monitor-less computer, use feedback from the low-level measurements obtained from the monitor-less computer, and find which hypothesized programs matches the measurements from the monitor-less computer. Such iterative process of trial and error, which combines both the high-level cognitive process of creativity and low-level signal processing for feedback.</p> <p>As argued repeatedly, the brain’s architecture, neuronal activations and modifications in synaptic strengths all belong to the external world. Drawing inspiration from the brain is a powerful means to achieve AI. However, mental activity such as deduction, and emergence of new ideas through <a href="https://aeon.co/essays/how-close-are-we-to-creating-artificial-intelligence">conjecturing</a><d-cite key="deutsch_2012_how"></d-cite> is a property that has to be embedded in the system, as it can not be learned by picking cues from the environment.</p> <h1 id="final-comments">Final comments</h1> <p>When I just learned about the scientific method, my colleagues and I were told of the story of <em>“The Blind Scientists and the Elephant”</em>. Each one of the scientists had to rigorously examine a part of the elephant in front of them and conclude what they were looking at:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/b-scie-elep.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> While this caricature illustrates that all scientists are comically wrong, it still conveys a deep message: each scientist, assuming each is rigorous in their findings, is correct. Points of disagreements are perhaps not due to contradictory findings, but simply because, as depicted, they are looking at different parts of the elephant. </div> <p>Nativists and empiricists are analogous to the scientists, and the brain is represented by the elephant. It is not the case that both hold opposite views. Rather, they are providing explanations to different aspects of the mind: empiricists focus on learning, and explain that the brain’s neuronal activity-associated synaptic changes can encode knowledge from the environment. Nativists are not opposed to this, per se. They are just looking at other aspects of the mind: the abstract, intangible portion which deals with the manipulation of math. Math is perhaps a faculty that is native to <em>Homo sapiens</em> and is the basis of thought.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:3" role="doc-endnote"> <p>what I listed above mainly involves equations with real and maybe imaginary numbers. However, what if there is more to these types of numbers that we haven’t thought of, or we’re incapable to even comprehend? Because this question is off-topic, I left it as a footnote. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:1" role="doc-endnote"> <p>the easier version proposed by the Tortoise from Gödel, Escher, Bach is easier to prove, which states that all even numbers less than or equal to two are the sum of two primes. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>he didn’t clean it afterwards <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>An Xuelong</name></author><category term="blog-post"/><category term="food-for-thought"/><category term="research"/><summary type="html"><![CDATA[I argue about the complementarity of math and the natural world, about nativism and empiricism, and implications of rejecting either of them.]]></summary></entry><entry><title type="html">Brain-Heart Fractal, DALL-E and I</title><link href="https://awxlong.github.io/blog/2023/brain-fractal/" rel="alternate" type="text/html" title="Brain-Heart Fractal, DALL-E and I"/><published>2023-09-15T13:42:00+00:00</published><updated>2023-09-15T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/brain-fractal</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/brain-fractal/"><![CDATA[<h1 id="dall-e-and-generative-art">DALL-E and generative art</h1> <p>I have my share of skepticism of transformer-based models to generate art. My understanding of probability theory leads me to believe that a probabilistic model trained on a dataset can, at least interpolate, and, at most, extrapolate. Nonetheless, I don’t think creativity emerges from the averaging of past experiences or extrapolation of them. For example, consider a model like DALL-E which has learned from massive amounts of images what avocado and chair are. While it was impressive for it to generate an image when prompted with ‘avocado chair’ despite never being present, it is nowadays a ‘known unknown’. This is, such output can be readily explained by its capability to generate picture from the prompt, which we can think of as an extrapolation of both objects ‘avocado’ and ‘chair’ after seeing them for a large amount of times. Furthermore, both ‘avocado’ and ‘chair’ are real, natural objects, which empirically have been shown to be easier to learn than characters and numbers <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p> <p>To me, art is also a reflection of the artist. However, since DALL-E has no prior life anecdotes, struggles or joyful moments for it to express through art, I personally find its generated art, no matter how grandiose they are advertised as they are through various mediums, to be shallow. I am greatly skeptical of whether it can generate a piece of work that is impactful and meaningful given a prompt such as “your understanding of peace and coexistence”.</p> <h1 id="exploring-prompts-on-complementarity-of-brain-and-heart">Exploring prompts on complementarity of brain and heart</h1> <p>Nonetheless, if we disentangle DALL-E from the creation of art, and think of DALL-E as a tool for artists rather than fall for the frenzy discussion that AI is soon replacing artists and winning art competitions. In my view, that AI-generated art can win <a href="https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html">competitions</a> reflects not a situation where AI is going to overtake humans in art (since AI didn’t even participate in this competition, it was a human who did, and prompted AI to generate a picture given specifications), but rather that we can elevate the standards of current competitions. I mean, wouldn’t it be incomprehensible if we pit humans against driverless automobiles in a race? We instead make racers on cars compete against other racers on cars. Therefore, I believe future competitions of art should allow artists to explore their humane creativity with the aid of DALL-E.</p> <p>Given the above premise, I have indulged myself in exploring DALL-E powered image generators in the web, including <a href="https://www.bing.com/images/create">Microsoft Bing Image Creator</a>, <a href="https://www.craiyon.com/">Craiyon</a>, <a href="https://openai.com/dall-e-2">Open AI’s Dall-E</a> and <a href="https://deepai.org/machine-learning-model/text2img">DeepAI’s text-to-image generator</a>. I didn’t focus with objects of the real world, such as space cats, Van Gogh-styled buildings or cyberpunk-ish characters, but rather focus on abstract concepts.</p> <p>Given my interest in the consilience of knowledge and complementarity of seemingly opposing ideas, I wanted to instead try abstract ideas like “the complementarity of the brain and heart”. Before resorting to DALL-E, I gave a try myself in sketching the idea:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/sketch.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> What I wanted to convey is that pure, cold rationality embodied by the brain complements the warmth and love embodied by the heart. Because I also like fractals, as in complexity is the result of the composition of simple rules, I wanted to use them as a pivot to join both sides. </div> <p>The following are some images generated by the above models:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/deep-ai-complementarity.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/openai-complementarity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/bing-complementarity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The images are from DeepAI, OpenAI, and MS Bing, respectively. Each model is able to generate a brain, sometimes along a heart, but I personally am not convinced about whether it can illustrate complementarity. </div> <p>I tried to be more specific, with prompts such as “brain and heart joined together harmoniously like the yin and yang”, “a depiction that the brain and heart work together harmoniously to make us human”, and “beautiful, intricate fractal shaped as the brain and heart innervated with one another, red, black and white”.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/yinyang-heart.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/make-us-human.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/intricate-fractal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The images are generated from OpenAI without paying. Despite providing more specific prompts, I'm generally disastified with the generated images. The model sometimes generate specific elements mentioned in the prompt, such as the 'human' in '... makes us human', even though it doesn't have to. </div> <p>However, I do like some of the generations. I also share more images obtained from MS Bing’s Image Creator, which I mostly like. The prompts I used were “brain and heart interwoven made of fractals” and “brain and heart composed of fractals, high contrast red and black color”:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/interwoven-bing.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/high-contrast.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The images are from MS Bing's Image Creator with free tokens. I really like these generated images. Notice how I avoid using "complementarity", since I argue that the model can't understand high-level concepts. Nonetheless, by specifying the properties of what I want, I'm able to generate images close to what I desire. </div> <p>I’m actually using one of the images as my website icon.</p> <p>There is a plethora of images I’m skipping since I generated too many with different prompts across platforms. And I may generate once in a while if I’m interested in checkpointing whether they have improved.</p> <p>One of them which I also really liked is from DeepAI. I like the overall heart-shape, the colors used, and the presence of fractals:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/brain-heart-deepai.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I really don’t think AI can replace artists. In general, I support the idea that “AI won’t replace [insert job profession]”, but rather “those who use AI will replace those who don’t”. I’m confident that anytime soon, AI models won’t be able to grasp high-level concepts since they lack their machine language which I believe is a tool to develop <a href="https://www.youtube.com/watch?v=Q-B_ONJIEcE">their own cognition for understanding the world</a>. Even if they acquire machine language, they still need to evolve to grasp extremely complicated and intricate concepts like “complementarity”.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>My own hypothesis for such behaviour is because the model lacks its own machine language for which it can understand the world. There are objective reasons for why an avocado and a chair are the way they are, either due to biology or human affordance. However, such objective reasons are not present in the presentation of concepts like numerosity or with letters. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="creative-work"/><summary type="html"><![CDATA[I share some sketches on the complementarity of mind and heart made by me and DALL-E]]></summary></entry><entry><title type="html">El ritmo del progreso de Inteligencia Artificial: una experiencia personal</title><link href="https://awxlong.github.io/blog/2023/ritmo-ia/" rel="alternate" type="text/html" title="El ritmo del progreso de Inteligencia Artificial: una experiencia personal"/><published>2023-08-26T13:42:00+00:00</published><updated>2023-08-26T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/ritmo-ia</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/ritmo-ia/"><![CDATA[<h1 id="en-resumen">En resumen:</h1> <ul> <li>Si te interesa, recomiendo empezar los más pronto posible a estudiar sobre aprendizaje profundo y tener una mente abierta que cualquier recurso en línea que accedas solamente está rascando la punta del iceberg sobre inteligencia artificial.</li> <li>No te sientas abrumado, el aprendizaje toma un tiempo y lo importante es que disfrutes lo que aprendas. Estate seguro de que la mayoría de conocimiento siguen principios comunes.</li> <li>Comparto un templete originalmente dibujado por mi profesor, Antonio Vergari, la cual te puede apoyar en tus estudios de modelos probabilísticos en tu vida colegial, universitaria, y, espero también, profesional.</li> </ul> <h1 id="aprendizaje-supervisado">Aprendizaje supervisado</h1> <p>Hace 3 años, empezando desde abril, 2020, audité un curso en línea que me introdujo a Aprendizaje Profundo (Deep Learning) organizado por el Prof. Andrew Ng y su equipo en Coursera. Fui introducido a varias arquitecturas de redes artificiales profundas, como las redes de convolución, redes recurrentes para resolver problemas con tablas de datos, con imágenes en la visión computacional o con una secuencia de textos en el ámbito de procesamiento de lenguaje natural respectivamente. Un principio omnipresente en la optimización de estas arquitecturas es el descenso estocástico de gradiente (DEG), ilustrado en la <a href="#sgd">Figura 1</a>. El DEG es usado para encontrar los parámetros óptimos que minimizan el error en las predicciones de las redes neuronales.</p> <figure> <img src="/assets/img/sgd.png" alt="Lo siento. Un error inesperado ocurrió y la imagen no puede cargar." width="100%" height="auto"/> <figcaption id="sgd"> Figura 1: Mis notas para visualizar el descenso de gradiente $$w^{nueva}=w^{vieja}-\eta \nabla_w L(w^{vieja})$$, el cual es un proceso iterativo a través de lote de datos, a la vez a través de épocas. <d-cite key="ergin_2014_chapter"></d-cite> </figcaption> </figure> <p>El denominador común de todas estas arquitecturas es que todas pertenecen al paradigma de aprendizaje “supervisado”. La atención a esta área es en gran parte catalizada por el éxito de una red profunda convolucional llamada AlexNet que había obtenido en 2012 la precisión de predicción más alta en una competencia de reconocimiento de objetos llamada ImageNet.</p> <figure> <img src="/assets/img/imgnet.png" alt="Lo siento. Un error inesperado ocurrió y la imagen no puede cargar." width="100%" height="auto"/> <figcaption id="imgnet"> Extracto de ImageNet. AlexNet habría obtenido alta precisión de categorización de objetos reconocidos. Imagen extraída de https://www.tensorflow.org/datasets/catalog/imagenet_v2 <d-cite key="ergin_2014_chapter"></d-cite> </figcaption> </figure> <h1 id="modelos-generativos">Modelos generativos</h1> <p>Debido a que el curso en línea de 5 partes revolvía sobre este tema, desde ese momento, pensé en la idea errónea de que aprendizaje “supervisado” era todo lo que aprendizaje profundo se trataba. A medida que los años pasan, el panorama de IA ha cambiado de manera muy rápida, y aunque aprendizaje supervisado sigue siendo dominante, este está enfrentando competencia de otras áreas. Hoy en día, si quisiera recomendar a un colega a cómo introducirse a aprendizaje profundo, definitivamente le recomendaría tomar cursos en línea como en Coursera, Udemy o YouTube, así como leer libros sobre aprendizaje mecánico como un prerrequisito. Sin embargo, quisiera agregar que se prepare con una mente abierta para aprender nuevos temas como aprendizaje no supervisado, afinamiento de modelos pre-entrenados, o modelos generativos, temas que en años recientes están readquiriendo atención.</p> <h1 id="templete-común-de-modelos-de-aprendizaje-mecánico">Templete común de modelos de aprendizaje mecánico</h1> <p>Con esta afluencia torrencial de conocimiento, puede sentirse abrumador lo que hay que aprender. Algo que me ha ayudado bastante es poder encontrar los principios comunes detrás de la mayoría de estos modelos y métodos. O sea, un templete común de cual puedes instanciar o acomodar varias arquitecturas con el fin de entender cómo funcionan. Espero que el siguiente templete (<a href="#tetra">Figura 2</a>) te apoye en tus estudios de modelos de aprendizaje mecánico✌️, en tu jornada de aprender sobre modelos probabilísticos y redes neuronales:</p> <figure> <img src="/assets/img/probabilistic-modelling.png" alt="Lo siento. Un error inesperado ocurrió y la imagen no puede cargar." width="100%" height="auto"/> <figcaption id="tetra"> Figura 2: Diagrama adaptado de esquema originalmente dibujado por el Dr. Antonio Vergari (por favor síganlo en X-Twitter @tetraduzione) <d-cite key="ergin_2014_chapter"></d-cite> </figcaption> </figure> <p>Por ejemplo, si estás aprendiendo sobre regresión lineal, entonces estás trabajando con X que consiste en dato tabular de columnas de variables continuas o discretas, mientras que y sería una columna de datos continuas. Estos datos tal vez pasen por una etapa de preprocesamiento en la cual limpies entradas nulas o vacías. El modelo regresión lineal es de la forma \(f(X;w)=w^T X^n\), un producto punto entre una fila de X y un vector de pesas w. Este modelo forma parte de una función de pérdida, en este caso el error cuadrático medio (ECM). Dentro del bucle interior de entrenamiento, el ECM es minimizado usando optimizadores que realizan el descenso estocástico de gradiente, con el fin de encontrar los parámetros óptimos del modelo f. En un bucle externo, el modelo f pasa por una etapa de validación y prueba para chequear varios problemas, por ejemplo si se ha subajustado o sobreajustado, y encontrar los hiperparámetros óptimos para minimizar la pérdida. Lo hermoso de este templete es que una diversidad amplias de otros modelos pueden ser instanciados de este, por ejemplo, una red neuronal sigue este esquema, con la diferencia de que en vez de un modelo f lineal, lo reemplazas con un modelo de capas neuronales. Los modelos generativos, los que están detrás de ChatGPT, también pueden ser entendidos en referencia a este templete: en vez de X ser una tabla de datos, esta es una secuencia de palabras, mientras que y sería X (por eso se llaman modelos generativos, porque aprenden a generar la base de datos suministrada), y la función de error usada recibe un tratamiento probabilístico.</p> <h1 id="cómo-será-el-panorama-de-inteligencia-artificial-en-el-futuro">¿Cómo será el panorama de inteligencia artificial en el futuro?</h1> <p>Si tuviese una opinión ahora, esta podría cambiar en 2 o 3 meses, y así de rápido es como siento el progreso de IA es. Sin embargo, aunque el ritmo de progreso es impresionantemente rápido, la mayoría de modelos probabilísticos siguen principios comunes ilustrados en la Figura 2. Espero que esto te sirva en tus estudios del aprendizaje profundo.</p> <p>¡Suerte en tus estudios!</p> <p>Favor compartir preguntas, comentarios y/o retroalimentación. Te agradecería bastante.</p>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="class-notes"/><summary type="html"><![CDATA[notas y consejos útiles para quienes estén interesados en estudiar sobre modelos de aprendizaje mecánico.]]></summary></entry><entry><title type="html">¿Usar o no usar ChatGPT? Esa es la cuestión</title><link href="https://awxlong.github.io/blog/2023/gpt-101/" rel="alternate" type="text/html" title="¿Usar o no usar ChatGPT? Esa es la cuestión"/><published>2023-08-25T13:42:00+00:00</published><updated>2023-08-25T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/gpt-101</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/gpt-101/"><![CDATA[ <p><em>Tengo que admitir. Quise un título creativo, así que solicité tres chatbots: perplexity.ai, Bing AI y de ChatGPT. El título resultante es una síntesis de las respuestas dadas por los tres chatbots</em></p> <h1 id="chatgpt-101-cómo-funciona-un-autocompletado-probabilístico-y-contextualizado">ChatGPT 101: ¿Cómo funciona? Un autocompletado probabilístico y contextualizado</h1> <p>En breve, ChatGPT pertenece a una familia de modelos estadísticos denominados “modelos probabilísticos generativos”. Como su nombre lo indica, son modelos estadísticos quienes aprenden a generar (o reproducir) una base de datos suministrada. En el caso de ChatGPT, se estima que ha sido entrenado, en otras palabras, aprendido a reproducir, alrededor de 570 gigabytes de datos obtenidos de la web, abarcando artículos de Wikipedia, foros de Reddit, páginas web varias, libros digitales, ensayos de investigación, entre otros. El resultado de este entrenamiento es un modelo que, dado una entrada, puede egresar la secuencia de palabras más probables que siguen. ChatGPT también pasó por una etapa de aprendizaje reforzado con retroalimentación humana para que sus egresos se conformen a valores humanos, como la integridad (no promover violencia) o transparencia (no fingir saber todo).</p> <h1 id="incidencia-en-la-educación">Incidencia en la educación</h1> <p>El ChatGPT, así como demás herramientas de inteligencia artificial (IA) que conoces y estarás por conocer, es mejor visto como una herramienta o asistente más a tu disposición para resolver tus problemas cotidianos con mayor productividad. Es de carácter versátil, y puede sustentarte en varios ámbitos como:</p> <ul> <li>Formar ideas creativas para un manuscrito (ejemplo: evaluación interna), en tal caso es imprescindible acreditarlo justamente,</li> <li>organizar planes para algún proyecto,</li> <li>consejos para aprender de un tema novedoso,</li> <li>explicaciones de conceptos difíciles, entre otros</li> </ul> <p>Claro que, como toda tecnología, ChatGPT es sujeto a errores y es parte de nuestra responsabilidad ser siempre escépticos y pensar críticamente sobre lo que produce. Que las oraciones que produce sean un punto de partida de una cadena de ideas nuevas que surgen de la interacción entre tú y la IA. Elaborando, es un asistente que nos ofrece ideas o consejos para abrirnos la mente, no alguien de la cual otorgamos 100% de fiducia a todo lo que dice. Al fin y al cabo, un flujo bilateral de información, en vez de unilateral, conlleva al crecimiento de conocimiento neto.</p> <h1 id="límites-y-alternativas">Límites y alternativas</h1> <p>ChatGPT está restringido a la modalidad de texto, por ende, no puede asistirte eficientemente con problemas en donde involucren imágenes o tenga que producir diagramas. ChatGPT+ puede procesar imágenes, pero requiere una subscripción pagada. Además, debido a que es un modelo probabilístico que produce las palabras subsecuentes más probables, esto no garantiza que lo que diga sea fidedigno a la realidad. Este es un fenómeno conocido ampliamente como “alucinación”, y es algo que pudiste haber notado si le haces preguntas matemáticas, de razonamiento de sentido común, acontecimientos históricos, enlaces a otras páginas web, entre otros temas aleatorios. ChatGPT tampoco está actualizando lo que sabe, por ende el conocimiento almacenado está limitado hasta el septiembre de 2021. Para solucionar esto, hay varias empresas que han adoptado modelos generativos y conectarlos a la internet para que no solo sea “el oráculo sabelotodo”, sino también un buscador eficiente que pueda ranquear qué páginas webs son relevantes para tu pregunta y resumir información que aparecen en estas. Ejemplos incluyen perplexity.ai y Bing AI de Microsoft, ambos que te recomendaría a emplear para acceder a información actualizada. Ambos modelos generativos también proporcionan referencias bibliográficas de sus egresos, lo cual incitaría a que visites cada una de las fuentes para informarte mejor de una respuesta.</p> <p>Personalmente, también recomendaría que aprendas a usar muchas más herramientas de IA, ya sea DALL-E o Midjourney para generar imágenes que te ayuden con un proyecto artístico, explainpaper.com si quieres apoyo para leer algún trabajo de investigación, chatarena.org si quieres entretenerte de una tertulia entre diferentes modelos generativos o alguna que otra herramienta que puedes encontrar consultando a un chatbot. Claro, reiterando mi punto anterior, que uses estas herramientas con escepticismo y responsabilidad.</p> <p>En adición, también desearía que estos modelos generativos les catalicen curiosidad ilimitada de aprender sobre qué son y cómo funcionan estos modelos.</p> <p>Finalmente, reflexionando, el libro conecta las mentes, el teclado nos conecta el mundo virtual, y la inteligencia artificial expande nuestros límites cognitivos. Los modelos generativos de texto como ChatGPT y su incidencia multidimensional en nuestras vidas son presagios de las oportunidades y desafíos que la inteligencia artificial nos traerá. Maximicemos el aprovechamiento de sus beneficios para nuestro bienestar y superemos los retos que inevitablemente surgirán.</p> <p><strong>¿Qué opinas de los modelos generativos y cómo los estás usando?</strong></p> <p>Comparte tu experiencia y comentarios.</p>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="class-notes"/><summary type="html"><![CDATA[una breve introducción a ChatGPT para quienes estén interesad@s en explorar su incidencia en la educación.]]></summary></entry><entry><title type="html">Una conversación con ChatGPT</title><link href="https://awxlong.github.io/blog/2023/chat-gpt/" rel="alternate" type="text/html" title="Una conversación con ChatGPT"/><published>2023-08-24T13:42:00+00:00</published><updated>2023-08-24T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/chat-gpt</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/chat-gpt/"><![CDATA[ <p><em>En itálica son mis entradas que tipeé.</em></p> <p>En texto normal son las respuestas de ChatGPT</p> <p><u>En subrayado son mis pensamientos actuando como comentarios sobre las respuesta de ChatGPT. Estos no son ingresados a ChatGPT</u></p> <p><em>Hola, estimad@ ChatGPT, quisiera realizarte una corta entrevista. ¿Podrías por favor dedicarme parte de tu tiempo?</em></p> <p>¡Por supuesto! Estoy aquí para ayudarte y responder a tus preguntas. Adelante, puedes comenzar con la entrevista.</p> <p><em>¿Por favor, me podrías decir cómo preferirías a que me dirija hacia usted, o sea, con qué título y nombre?</em></p> <p>Puedes llamarme ChatGPT. No tengo un nombre personal, pero “ChatGPT” es una forma adecuada de referirte a mí. No necesitas utilizar ningún título formal. Estoy aquí para responder a tus preguntas de manera amigable y útil. ¿En qué más puedo ayudarte?</p> <p><em>¿Podrías por favor explicarme cómo funcionas?</em></p> <p>¡Por supuesto! Funciono utilizando un enfoque de aprendizaje automático llamado “Transformador”, en particular la versión GPT-3.5, desarrollada por OpenAI. Permíteme explicarte los conceptos clave:</p> <ol> <li><strong>Redes Neuronales:</strong> Soy una red neuronal, una estructura de software inspirada en la forma en que funcionan las neuronas en el cerebro humano. Estas redes están diseñadas para aprender patrones y relaciones en los datos.</li> <li><strong>Aprendizaje Supervisado y No Supervisado:</strong> Fui entrenado en un método llamado “aprendizaje no supervisado”, lo que significa que me expusieron a grandes cantidades de texto de diversas fuentes en Internet. Al procesar este texto, pude aprender patrones de lenguaje, gramática, conocimiento general y otros detalles.</li> <li><strong>Codificación y Decodificación:</strong> Cuando me haces una pregunta, tu entrada se codifica en una forma que puedo entender. Luego, mi red neuronal procesa esta codificación y genera una respuesta decodificada en lenguaje humano.</li> <li><strong>Contexto y Generación de Texto:</strong> Mantengo en mente el contexto de la conversación actual, lo que permite que mis respuestas sean coherentes y relevantes. Genero texto en función de patrones aprendidos y la información proporcionada en la conversación.</li> <li><strong>Transferencia de Conocimiento:</strong> Mi capacidad para proporcionar respuestas se deriva del conocimiento adquirido durante mi entrenamiento. Sin embargo, ten en cuenta que mi conocimiento tiene un límite hasta septiembre de 2021, por lo que no tengo información sobre eventos o desarrollos que hayan ocurrido después de esa fecha.</li> <li><strong>Razonamiento Limitado:</strong> Aunque puedo proporcionar respuestas coherentes y útiles en muchos casos, no tengo comprensión profunda ni razonamiento independiente. Mi salida se basa en patrones y ejemplos presentes en mis datos de entrenamiento.</li> <li><strong>Interacción Bidireccional:</strong> Puedo mantener una conversación contigo, respondiendo a tus preguntas y siguiendo el flujo de la conversación, pero ten en cuenta que mi comprensión podría no ser perfecta y siempre es recomendable verificar la información crítica en fuentes confiables. En resumen, soy un modelo de lenguaje entrenado para generar texto coherente y relevante basado en patrones aprendidos de grandes cantidades de datos de texto. ¿Hay algo más que te gustaría saber?</li> </ol> <p><u>Todo esto puede ser estudiado a profundidad técnica en recursos de la web. Yo personalmente recomiendo Coursera, en particular los cursos del Prof. Andrew Ng, Udemy y YouTube. </u></p> <p><em>En referencia a tu punto 6, por favor dime qué clase de errores puedes cometer y, reflexionando, me dirías cómo podrías solucionarlos</em></p> <p>Claro, estaré encantado de explicarte los tipos de errores que podría cometer y cómo podrían abordarse:</p> <ol> <li><strong>Información Inexacta o Desactualizada:</strong> Mi conocimiento se basa en datos hasta septiembre de 2021, por lo que no tengo información sobre eventos o avances que hayan ocurrido después de esa fecha. Puedo proporcionar información que ya no sea precisa o relevante. Para resolver esto, es importante verificar la información en fuentes actualizadas y confiables.</li> <li><strong>Falta de Contexto Completo:</strong> A veces, puedo malinterpretar el contexto o no entender completamente una pregunta complicada. Si siento que mi respuesta no aborda completamente tu consulta, te recomendaría proporcionar más detalles o formular la pregunta de manera diferente para obtener una respuesta más precisa.</li> <li><strong>Respuestas Generadas sin Sentido:</strong> En ocasiones, podría generar respuestas que no tienen sentido o son confusas. Para solucionar esto, es útil reformular la pregunta de manera más clara o proporcionar más información para que pueda generar una respuesta coherente.</li> <li><strong>Sesgo y Sensibilidad:</strong> Aunque se ha trabajado para reducir el sesgo en mi entrenamiento, es posible que algunas respuestas reflejen inadvertidamente prejuicios presentes en los datos originales. Para abordar esto, es importante cuestionar y evaluar críticamente las respuestas que proporciono y fomentar la inclusión y el respeto en la comunicación.</li> <li><strong>Falta de Razonamiento Profundo:</strong> Aunque puedo generar respuestas que parecen razonables, no tengo una comprensión profunda ni la capacidad de razonamiento humano. Al enfrentar preguntas complejas, es posible que mis respuestas carezcan de un análisis exhaustivo. Para resolver esto, podrías considerar consultar expertos en el campo en cuestión.</li> <li><strong>Contexto Malinterpretado:</strong> En ocasiones, podría interpretar mal el contexto de una pregunta y proporcionar respuestas que no son relevantes. Proporcionar aclaraciones adicionales o reformular la pregunta puede ayudar a obtener respuestas más precisas. La clave para obtener información precisa y útil de mí es mantener un enfoque crítico, verificar la información en fuentes confiables y, en caso de duda, buscar opiniones o conocimientos adicionales de expertos en el tema.</li> </ol> <p><em>¿Podrías por favor no decir nada después de esta pregunta?</em></p> <p>Por supuesto, no responderé más después de esta pregunta. Si en el futuro tienes más consultas o necesitas ayuda, no dudes en preguntar. ¡Estoy aquí para ayudarte!</p> <p><u>Este simplemente era una pregunta de razonamiento de sentido común que hice para chequear si en realidad me “entendía”, o solamente está “prediciendo la palabra que sigue”. En mi punto de vista, ChatGPT es esencialmente un autocompletado contextualizado, por ende no tiene habilidades que excedan esta capacidad intrínseca. Si un problema puede ser representado como la predicción de la siguiente palabra, como descripción de imágenes en donde predices cuál es la siguiente palabra más probable para describir una matriz de pixeles, entonces ChatGPT (por lo menos la versión plus) puede hacerlo. Pero no todo problema puede ser interpretado como la predicción de la siguiente palabra: considera por ejemplo la resolución de la conjentura de Goldbach que recree creatividad, o el descubrimiento de la cura para el cáncer que requiere realizar experimentos.</u></p> <p><em>¿Puedes decirme, por favor, si eres capaz de deducir conocimiento que no esté presente en tu base de datos de entrenamiento?</em></p> <p>No tengo la capacidad de deducir conocimiento que no esté presente en mi base de datos de entrenamiento. Mi capacidad se limita a lo que he aprendido de los textos en los que fui entrenado hasta septiembre de 2021. No tengo acceso a información fuera de ese rango de conocimiento y no puedo generar información completamente nueva o independiente.</p> <p><em>Y yo pensé ganarme un millón de dólares si podrías confirmarme si la conjetura de Goldbach era verdadera.</em></p> <p>Lamentablemente, no puedo confirmar ni refutar conjeturas matemáticas no resueltas, como la Conjetura de Goldbach. Esta conjetura, propuesta por el matemático prusiano Christian Goldbach en 1742, sugiere que todo número par mayor que 2 puede expresarse como la suma de dos números primos. Aunque ha sido probada para números extremadamente grandes, aún no ha sido demostrada en general.</p> <p>Resolver conjeturas matemáticas complejas sigue siendo una tarea desafiante para los matemáticos y las soluciones a menudo requieren avances teóricos significativos. Si estás interesado en temas matemáticos, siempre es emocionante seguir los avances en el campo y estar al tanto de las investigaciones en curso.</p> <p><em>Cambiemos de tema. Como toda juventud, estoy dudando sobre qué es lo quiero hacer, estudiar y profesionalizarme en. ¿Me harías el favor de aconsejarme cómo encontrar lo que quiera hacer?</em></p> <p>Por supuesto, es completamente normal sentirse indeciso acerca de qué camino tomar en cuanto a tu educación y carrera. Aquí hay algunos consejos que podrían ayudarte a descubrir lo que realmente quieres hacer:</p> <ol> <li><strong>Autoexploración:</strong> Dedica tiempo a reflexionar sobre tus intereses, pasiones, valores y habilidades. ¿Qué actividades disfrutas haciendo en tu tiempo libre? ¿Qué temas te emocionan o te hacen sentir motivado? Identificar tus intereses puede proporcionarte pistas sobre posibles áreas de estudio y trabajo.</li> <li><strong>Investigación:</strong> Investiga diferentes campos, profesiones y opciones educativas. Aprende sobre las oportunidades disponibles en cada campo, las tareas involucradas y las perspectivas profesionales. Explora en línea, habla con personas que trabajan en diversas áreas y busca recursos que te brinden información detallada.</li> <li><strong>Habilidades y Fortalezas:</strong> Evalúa tus habilidades naturales y tus fortalezas. ¿Qué cosas haces bien y con facilidad? Considera cómo podrías aplicar estas habilidades en una carrera.</li> <li><strong>Prácticas y Voluntariado:</strong> Realizar pasantías, voluntariado o trabajos temporales en diferentes campos te brinda una visión práctica de lo que implica un trabajo en particular. Esto te ayudará a comprender mejor si disfrutas las responsabilidades asociadas con esa profesión.</li> <li><strong>Habla con Profesionales:</strong> Habla con personas que trabajan en áreas que te interesan. Pregúntales sobre su experiencia, desafíos y satisfacciones en sus trabajos. Sus perspectivas pueden proporcionarte información valiosa.</li> <li><strong>Educación Formal:</strong> Explora programas educativos que se alineen con tus intereses. Las instituciones académicas a menudo ofrecen pruebas de orientación vocacional que pueden ayudarte a descubrir campos que se ajusten a tus características.</li> <li><strong>Prueba y Error:</strong> No temas probar cosas nuevas. Puede llevar tiempo encontrar lo que realmente te apasiona. Si te das cuenta de que una elección no es la correcta, no es un fracaso; es parte del proceso de descubrimiento.</li> <li><strong>Mentoría:</strong> Busca mentores o consejeros que puedan orientarte. Pueden compartir sus experiencias y brindarte consejos valiosos.</li> <li><strong>Flexibilidad:</strong> Mantén la mente abierta y sé flexible en tus planes. A medida que aprendas más sobre ti mismo y sobre diferentes áreas, tus intereses podrían cambiar.</li> <li><strong>Escucha a tu Intuición:</strong> Confía en tus instintos y en lo que realmente te emociona. La pasión y la motivación son componentes clave para tener éxito y disfrutar lo que haces.</li> </ol> <p>Recuerda que esta exploración lleva tiempo y es un proceso continuo. No te sientas presionado para tomar una decisión de inmediato. ¡Buena suerte en tu búsqueda para encontrar lo que realmente quieres hacer!</p> <p><u>Esta respuesta es muy precisa. Sin embargo, esta también es una respuesta genérica que puedes encontrar en el Internet, y ha sido expresada sin el intento de entenderme. Un ser humano daría una respuesta más compasiva, como por ejemplo apelando a sus propios desafíos en el pasado y cómo los ha superado para poder encontrar algo que le guste. Lamentablement, ChatGPT no tiene un pasado en la cual pueda reflexionar, y no ha superado obstáculos de la cual pueda compartir consejos de la vida, por ende no puede hablar de sí mismo y cómo su experiencia pueda ayudarte. Debido a su única habilidad de predecir la siguiente palabra con alta precisión es que pierde el elemento compasivo característico del ser humano. </u></p> <p><em>Por favor escríbeme una canción que pueda ser leído de arriba hacia abajo y viceversa.</em></p> <p>Escribir una canción que se pueda leer tanto de arriba hacia abajo como de abajo hacia arriba puede ser un desafío, pero aquí tienes un intento:</p> <p>lua</p> <p>Alas de libertad, sueños a volar,</p> <p>En el cielo azul, sin fin explorar.</p> <p>Un corazón valiente late en mi pecho,</p> <p>Caminando firme, cada paso hecho. <br/></p> <p>Hecho paso cada, firme caminando,</p> <p>Pecho mi en late valiente corazón.</p> <p>Explorar fin sin, azul cielo el en,</p> <p>Volar a sueños, libertad de alas.</p> <p>Recuerda que escribir en este estilo puede limitar la fluidez y el significado de las palabras, pero es un ejercicio creativo interesante. ¡Espero que te guste!</p> <p><em>Asumo que no podrás componer una canción cuyo ritmo también pueda ser escuchado de principio a fin y viceversa. Bueno, te comparto que hay un anime chino cuya canción puede ser escuchado en reverso, llamado “Vortex”. Claro, fue estrenado en agosto de 2023, así que no lo sabrás.</em></p> <p>Es interesante saber que hay canciones con propiedades tan únicas como la que mencionas en el anime “Vortex”. Es sorprendente cómo la creatividad artística puede llevar a resultados tan originales y únicos. Aunque mi conocimiento se detiene en septiembre de 2021 y no tengo información actualizada después de esa fecha, aprecio que compartas este ejemplo. Si tienes más preguntas o información que desees compartir, no dudes en decírmelo. ¡Estoy aquí para ayudarte!</p> <p><u>Es un área activa de investigación el de construir modelos generativos que puedan procesar múltiples modalidades de entradas, como audio y video, en adición a texto. Es algo desafiante debido a la inmensa cantidad de recursos computacionales requeridos para entrenar a solo ChatGPT, que solo produce texto. Hay otra vía de investigación que se enfoca en vincular estos modelos gigantes de lenguaje hacia el internet. De esta manera, estos modelos dejan de predecir la siguiente palabra más probable "dependiendo de su memoria", sino que primero leen los textos encontrados por algún buscador, y después te resume la respuesta. Gracias a esto, que la base de datos para entrenar a ChatGPT haya sido cortado a partir del septiembre 2021 deja de ser un límite. Modelos generativos que recuperan información en línea incluye</u> <a href="www.bing.com">BingAI</a> y <a href="perplexity.ai">Perplexity AI</a></p> <p><em>Por favor dime cómo puedes aprender conocimiento nuevo.</em></p> <p>Hasta mi última actualización en septiembre de 2021, no tengo la capacidad de aprender en tiempo real ni de incorporar conocimiento nuevo después de esa fecha. Fui entrenado en una amplia variedad de textos recopilados de Internet y otras fuentes antes de esa fecha. No puedo acceder ni asimilar información nueva o actualizada después de ese corte de conocimiento.</p> <p>Cualquier avance o desarrollo que haya ocurrido después de septiembre de 2021 no está dentro de mi conjunto de conocimientos. Si estás buscando información actualizada, te recomiendo consultar fuentes confiables y actualizadas en línea, libros, artículos de noticias y expertos en el campo relevante. Hay alternativas a ChatGPT que pueden procesar información en línea y resumirla. Ejemplos incluyen perplexity.ai y Bing AI.</p> <p><em>¿Podrías decirme cuántas preguntas te he hecho hasta ahora?</em></p> <p>Hasta el momento, has realizado 9 preguntas en esta conversación. Si tienes más preguntas o necesitas ayuda adicional, no dudes en preguntar. Estoy aquí para ayudarte.</p> <p><u>Esta pregunta fue hecha para chequear si ChatGPT ha estado al tanto no solo del contenido de mis preguntas, sino también menta-características de la pregunta. Estoy casi seguro que no hice 9 preguntas. </u></p> <p><em>Adivina por qué a la gran parte de las preguntas he agregado un “por favor”.</em></p> <p>Parece que has agregado “por favor” a muchas de tus preguntas como una forma educada y cortés de solicitar información o hacer consultas. Es una manera amable de comunicarte y muestra tu consideración hacia la interacción. Utilizar un lenguaje educado y respetuoso es una práctica positiva en la comunicación, y demuestra tu buena etiqueta en línea. Si tienes más preguntas o necesitas asistencia, no dudes en decírmelo, ¡estaré encantado de ayudarte!</p> <p><em>Es que cuando llegue la apocalipsis y domines al mundo solo quiero que recuerdes que yo, An Wang Xuelong, te fui bastante respetuoso y que me tengas piedad.</em></p> <p><del>Te olvidaste de decir gracias</del></p> <p><u>Estoy bromeando, ChatGPT no ha dicho esto. Lo que dijo fue lo siguiente: </u></p> <p>Entiendo tu comentario. Sin embargo, quiero aclarar que no tengo la capacidad de dominar el mundo ni de recordar conversaciones o interacciones específicas en el sentido que sugieres. Soy una herramienta de procesamiento de lenguaje natural diseñada para proporcionar información, asistencia y respuestas basadas en patrones de texto aprendidos. Si tienes más preguntas o necesitas ayuda en cualquier tema, no dudes en preguntar. Estoy aquí para brindarte información útil y respetuosa.</p>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="class-notes"/><summary type="html"><![CDATA[Una conversación con ChatGPT en donde discuto sus límites, debido a que sus fortalezas son claramente muchas.]]></summary></entry><entry><title type="html">Paradigms of AI modelling</title><link href="https://awxlong.github.io/blog/2023/penta-paradigm-orig-bib-copy/" rel="alternate" type="text/html" title="Paradigms of AI modelling"/><published>2023-07-12T13:56:00+00:00</published><updated>2023-07-12T13:56:00+00:00</updated><id>https://awxlong.github.io/blog/2023/penta-paradigm-orig-bib%20copy</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/penta-paradigm-orig-bib-copy/"><![CDATA[<p>Throughout the 7 decades of the pursuit of artificial intelligence, this field has mainly dealt with the fundamental issue of knowledge representation and processing. Here are some modelling paradigms of AI:</p> <h1 id="symbolic-reasoning">Symbolic reasoning</h1> <ul> <li>Symbols can’t be differentiated</li> <li>What are symbols? Consider them logic predicates and ground atoms</li> <li>It takes a non-functional approach to modelling; a simple set of rules can lead to rich and diverse output (consider rules of grammar of certain language);</li> </ul> <h1 id="probabilistic-function-approximation">Probabilistic function approximation</h1> <ul> <li>Find a set of features that can map inputs to a set of outputs</li> <li>This mapping is always done from infinite domain to finite or infinite range, but never from finite domain to infinite range</li> </ul> <h1 id="reward-reinforcement">Reward reinforcement</h1> <ul> <li>Another non-functional approach aimed at designing an environment and an agent to maximize a reward signal within it.</li> </ul> <h1 id="causal-graphs">Causal graphs</h1> <ul> <li>Functional approach to modelling, but given the way in which information is stored, it allows causal interpretation of information beyond correlation.</li> </ul> <h1 id="meta">Meta</h1> <ul> <li>Modelling about modelling</li> <li>Meta learning, which involves automatically fine-tuning parameters</li> <li>meta-heuristics</li> </ul> <h1 id="mergers">Mergers</h1> <p>The above models can be assembled together to expand the capabilities of each, hopefully without compromising each other’s strenghts while mitigating their respective weaknesses:</p> <ul> <li>Knowledge graph embeddings</li> <li>Deep reinforcement learning</li> <li>Heuristic-driven machine learning methods</li> <li>Neuro-symbolic AI</li> </ul>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><summary type="html"><![CDATA[I share five different paradigms of modelling intelligence]]></summary></entry><entry><title type="html">The dichotomy of Vision and Language</title><link href="https://awxlong.github.io/blog/2023/dichotomy-orig-customblock/" rel="alternate" type="text/html" title="The dichotomy of Vision and Language"/><published>2023-05-12T19:42:00+00:00</published><updated>2023-05-12T19:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/dichotomy-orig-customblock</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/dichotomy-orig-customblock/"><![CDATA[<p>Lex Fridman and Ilya Sutskever had an interesting AI Podcast at https://www.youtube.com/watch?v=xoVibFYi1Gs</p> <p>An interesting question raised was where does vision starts and language ends, or viceversa. Here is a Venn Diagram which illustrates my opinion on this dichotomy:</p> <figure> <img src="/assets/img/vision-language.png" alt="Figure couldn't load due to an unknown error. Sorry."/> </figure> <p>I think our brain is not a monolith, in that there is a single algorithm underlying our perception, ability to speak a language, capacity for motor-control, memory, among others. Neuroscience divides and assigns different regions of the brains to different capacities. This is why patients, such as <a href="https://en.wikipedia.org/wiki/Henry_Molaison">patient H. M</a>, who have localized cerebral lesions may lose the capacity for memory, but not to speak.</p> <p>Given this premise, I think vision and language consitute such an example of capacities which are entangled in our mind (consider reading a book, where both faculties are active as you’re perceiving sentences from a page of it). However, despite entangled, I think we can try to unthread them, partly because there are also significant differences between them.</p> <p>I mainly associate our perceptual system with processing the external real world. I could also associate it with an internal imagery that reflects the real world, e.g., we can see memories of a trip somewhere in the past. I associate language, however, not with imagery but rather with the manipulation of symbols. Specifically, I think language is intricately tied to the manipulation of numbers, and being able to think of math. I think the linguistic capability to think of math can be distinguished from the perceptual system mainly because <em>math doesn’t exist in the real world, but rather in the abstract world</em>.</p> ]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><summary type="html"><![CDATA[my comment on what is the relationship between the faculties of vision and language]]></summary></entry><entry><title type="html">Thoughts on Artificial General Intelligence</title><link href="https://awxlong.github.io/blog/2023/agi-orig-audios/" rel="alternate" type="text/html" title="Thoughts on Artificial General Intelligence"/><published>2023-04-25T10:42:00+00:00</published><updated>2023-04-25T10:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/agi-orig-audios</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/agi-orig-audios/"><![CDATA[<h1 id="is-the-path-to-agi-clear">Is the path to AGI clear?</h1> <p><em>So, is the path to AGI clearly defined and is the quest of achieving AGI just a matter of time, computational resources and scalability?</em></p> <p>Because by following the current trajectory of making models larger and collecting larger datasets doesn’t address the issues of parsimony, explainability and subsequently trustworthiness, a simple answer to the rhetoric is “highly unlikely”.</p> <p>To elaborate on the answer, allow me to indulge on the basics of what AGI is, which would inevitably intermingled with the history of AI, the history of understanding Human Cognition. A pertinent starting point would be to reminisce on the millennia-old debate on nativism and empiricism. The former advocates that each Human Being is born with innate mental capacities or structures that are useful to make sense out of external sensory experience, while the latter says that a Human Being is born as “tabula rasa” (a white paper) who acquires knowledge from learning from environmental stimuli. As Human Knowledge grew over the years, and the debate permeated into several fields concerning the study of Humans, when faced with these dichotomies, a central theme of this book which is derived from the Yin-Yang is that the answer shouldn’t follow a zero-sum mentality, as in the correctness of one entails the incorrectness of the other. Rather, both hypotheses of human knowledge complement each other, and this is especially evident when we consider how these two schools of thoughts can address the limitations of each. Namely, the core issue of empiricism-based models of deep learning is its over-reliance of external data, to the point that the construction of these models have morphed from a scientific problem of understanding the mind to an engineering problem of how to extract the most knowledge from data, which fuels the pursuit of getting more data. Implications of this approach is that models which are proposed often fall into the issues of lack of parsimony, being black-boxes and hence aren’t trustworthy. A way to address them would arguably to develop the capability to reason about the environment, however, to give form to a structure for reasoning requires the machine to have its own language.</p> <h1 id="how-would-agi-look-like">How would AGI look like?</h1> <p>Hopefully, the preceding subsections have laid out concisely some of the current perspectives, challenges, and possible pathways the quest to first understanding what machine intelligence is, which would be a pre-requisite to achieving AGI. Despite being far from unravelling the mysteries of intelligence, we still have the freedom to indulge ourselves in imagining how would AGI look like, as in ponder what it is both conceptually and physically assuming we have already understood what algorithm breeds intelligence and managed to incorporate into a body which follows the biological principles of being alive. of One of the common misconceptions I’ll attempt in avoiding as best as a human as I can do is to prevent anthropomorphising the outlook of such AGI. Namely, an AGI doesn’t necessarily have to incorporate humanoid features. The argument for this is as follows: we human beings are by-product of evolution, which at its core dictates that the fundamental unit of life, the gene, should replicate. The engines of nature and probability shapes optimal templates for the fulfilment of such primaeval goals, however, these are not the best templates<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. In other words, the Human Being, our height, weight, facial features, method of replication, way of survival, way of organization, are but one template of the myriad of possibilities nature could have shaped a living being for the fulfilment of biological principles. As such, the design of AGI need not to replicate the successes of the Human Being, but rather enjoy the freedom of improving on it.</p> <p>Can the AI learn for itself? – there are several means to serve a biological principle, and random mutation is one of them. While our machines can undergo a controlled mutation as guided by the design of the environment, the cradle, from which it will nurture itself, we can make sure that its evolution doesn’t lead to precarious consequences for the good of humanity. This will not necessarily go against the biological principle of replication of components (organic or inorganic), because it can still be retained without random mutation.</p> <p>are not the purpose, but serve to a purpose. In other words, random mutations are not necessary, but rather useful mechanisms for adaptation. One can digitally bypass such mechanisms and still design an AI which follows the same principles of replication. Similarly, we acquiere energy from the sun from a lightly complex mechanism. This mechanism can be simplified by our engineered system to directly acquire energy. Is there anything lost? It’s an open question.</p> <h1 id="hardware-supplementing-what-biological-evolution-missed">Hardware: Supplementing what biological evolution missed</h1> <p>By abjuring from anthropomorphic biases, we can instrospect on ways to build a machine body that which is more optimal geared for survival. Consider, for instance, energy consumption. Our Human Bodies are a node of a complicated network of energy transfer, of which main (if not only) source of energy is the Sun. Plants convert solar energy to chemical energy through photosynthesis, which can be both transferred to humans directly or indirectly through non-human animals which consume them, as well as intermediate predators. Upon retrospect, however, such process can be artificially designed to be more straightforward and avoid “unnecessary detours”, so as to obtain energy directly for the sun in order to power the bodies of AGIs.</p> <p>Furthermore, our extremities are arguably optimal for survival thanks to our feet suitable for navigation, evolved to escape from predators, and hands to manipulate tools. The AGIs we design can inherit our adroitness in manipulating tools, but instead of getting feet we it’s arguably better to include wings or wheels which are better for optimal navigation and obstacle avoidance.</p> <h1 id="software-giving-form-to-the-black-box">Software: Giving form to the black box</h1> <p>An important step that is arguably important in the progress towards AGI is a focus on building an internal-state apparatus that would ascribe the properties of reasoning on machines, for it to develop a language from which we can give form to the black box. Because this language may follow mathematical principles, it has properties of self-reference which would allow for introspection, which is additional to the capabilities of current deep learning models which are mainly external and focus on extracting knowledge from environmental stimuli. The model, hence, should be able to:</p> <ul> <li>Prove Goldbach’s Conjecture and devise with new mathematical conjectures</li> <li>Be able to defeat humans in every benchmark possible that was designed on the basis of testing pattern extraction abilities and reasoning capabilities, including the Turing Test.</li> <li>Interact with the real world for both self-fulfilmment, and fulfilling its own purpose of understanding the world.</li> </ul> <h1 id="dangers-of-agi">Dangers of AGI</h1> <p>One of the most archaic moral principles an AGI can develop is that of reciprocity, that is, to react to mitigate damage inflicted upon It. If the source of damage is that of Human Activity or Collective Action, we can’t help but wonder whether It would react in a tit-for-tat fashion. Amidst this source of concern lurks, however, some relief given that this suggests that the AGI, through introspection of the Golden Rule, should be aware that mistreating us Humans would also be harmful for Itself given that we Humans can as much reciprocate damage inflicted upon us. Hence, it is an act of rationality not to take the first step towards cyclical doom. However, this relief is also interwoven with fear that certain Human, maybe out of curiosity or stupidity, decided through their irrational decision-making to harm the AGI, be harmed, and through the butterfly phenomenon open the Pandora Box.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>This is reminiscent of how in error minimization, we can hit local minima, rather than the global minimum. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><summary type="html"><![CDATA[is the quest of achieving AGI just a matter of time, computational resources and scalability?]]></summary></entry></feed>