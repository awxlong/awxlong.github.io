<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://awxlong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://awxlong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-09T12:13:56+00:00</updated><id>https://awxlong.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">On the Turing Test and Large Language Models</title><link href="https://awxlong.github.io/blog/2023/turing-test/" rel="alternate" type="text/html" title="On the Turing Test and Large Language Models"/><published>2023-12-09T09:42:00+00:00</published><updated>2023-12-09T09:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/turing-test</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/turing-test/"><![CDATA[<h1 id="on-benchmarks-of-human-intelligence">On benchmarks of Human intelligence</h1> <p>There is a plethora of claims surrounding the emergence of actual or surpasssing Human intelligence by Large Language Models. One, by one, these contextualized autocomplete models are given the labels of having <a href="https://arxiv.org/abs/2305.03731v2">working memory</a>, <a href="https://analyticsindiamag.com/text-is-a-projection-of-the-world-says-openais-sutskever/">compresses</a> a <a href="https://www.linkedin.com/posts/armand-ruiz_llms-do-more-than-predict-the-next-word-activity-7134512576318623744-7BlG?utm_source=share&amp;utm_medium=member_desktop">“world model”</a>, or it signals <a href="https://arxiv.org/abs/2303.12712">sparks of Artificial General Intelligence</a>.</p> <p>There are a list of benchmarks backing up their claims. These benchmarks come with questions-answers pairs, where high performance signifies that a model is able to simulate a faculty of Human intelligence like language.</p> <p>However, most if not all benchmarks are tailored for what LLMs can do, even though they’re supposed to be designed without having LLMs in mind. A benchmark that tests a faculty of Human Intelligence like “reasoning” should not only explore define reasoning as . This is in contrast to designing theory-laden benchmarks. No one knows the exact definition of Human capacities like “reasoning”, so the design of a benchmark must have assumptions on what are the definitions of whatever capacity it is measuring. The key argument of the present article is that such assumptions have been constrained to framing every faculty as the prediction of the next token, where token can be the next word, image or audio signal. I don’t even know what faculties like “memory”, “reasoning”, “planning” are. But I do know as many researchers may conclude after years of work on this, that it is <em>not just contextualized, next-state prediction</em>.</p> <p>Because all benchmarks have been designed based on next-token prediction, it is not surprising that a next-token predictor can perform well. Quiet the opposite, it would be surprising if a next-token predictor can’t perform well on an auto-regressive task. However, while Human Intelligence most likely consists of next-token prediction, it would be misleading to suggest it <em>only</em> consists of this process. By that line of thought, LLMs haven’t even passed the Turing Test, at least by its <a href="https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence">original formulation</a>. This is because while it is true that the original objective was for a machine to imitate a human and fool an interrogator, a feat that LLMs can already achieve by producing text that is indistinguishable from Human-produced text, it is often ignored that the Turing Test is executed via a <em>physical</em> exchange of written letters. I don’t think this is a trivial matter that can be glanced over, as it is very important to note that LLMs can not write, since they are not instantiated in spacetime, nor have a body with which they have to face the challenge of sensorimotor control in a continuous feedback loop.</p> <p>A benchmark of a faculty of Intelligence shouldn’t be designed with only one theory of what Intelligence is, i.e., prediction of next-token. This kind of practice is akin to framing questions based on the answer one expects, instead of asking open-ended questions for the sake of curiosity. As such, if I may borrow the sensationalist attitudes of some media personalities, I just want to say “all these benchmarks are wrong, and none of them are useful<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> to understand Intelligence”.</p> <h2 id="prediction-error-minimization-is-a-strong-hypothesis-not-overarching">Prediction Error Minimization is a strong hypothesis, not overarching</h2> <p>I do want to clarify: I think <a href="https://www.fil.ion.ucl.ac.uk/~karl/Whatever%20next.pdf">prediction error minimization (PEM)</a> as an integral part of Human intelligence is a resilient hypothesis. Active inference, for instance, states that our brains are constantly predicting the source of the signals we perceive, and consciousness is the result when we infer that the cause of some signals in the environment is caused by ourselves. PEM also tries to explain other phenomena like dreaming, arguing that it is a form of “reverse-learning” that consists of a process of flushing irrelevant memories to while strengthening important synaptic connections, altogether allocate mental resources to make better predictions afterwards. Another important strength of the PEM hypothesis is that the abstract process of “learning” is grounded to physical mechanisms like chemico-modulated, neuronal activations. I believe these are very strong hypothesis that can withstand harsh criticism. I simply think PEM is not enough to explain the entirety of the Human condition.</p> <figure> <img src="/assets/img/llm_shouting.jpg" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="llm"> Image generated by Dall-E powered MS Bing Image Creator given the prompt "a large language model shouting" </figcaption> </figure> <p>My main criticism of PEM is that while it is designed to explain Human intelligence, it seems that it makes no distinction between non-Human animal intelligence and Human intelligence. The non-Human animal brain also has neurons firing, and the PEM may well apply to non-Human animals and Humans alike. However, is that enough? Are we Humans really just a slightly more intelligent animal. I argue it is absolutely not. Non-human animals can’t reflect on the mysteries of the mechanisms of the universe, perform scientific experiments, do creative thinking, believe in currency exchange and laundering, or any activity beyond the shackles of genetic predisposition, among others. And furthermore, I don’t think PEM explains neither of the aforementioned capacities.</p> <p>I just hope sensationalist claims in media stay as they are: “sensationalist”. They’re only meant to spark sensation, not actually to determine the course of public-policy making (at least not now), instilling fear of Humanity’s imminent destruction or even <a href="https://www.youtube.com/watch?v=lfXxzAVtdpU">existential crisis</a> on what humanity is. What I’m most worried about is that these sensationalist claims limit our own perspective on how amazing Human intelligence is, and that it is beyond PEM.</p> <p>As previously said, I don’t know what Human intelligence is in its entirety, but I’m confident it at least includes in addition to learning: reasoning, sensorimotor control, creativity/imagination, curiosity, consciousness, intrapersonal understanding, interpersonal understanding, among others.</p> <h1 id="proposing-an-extended-turing-test">Proposing an Extended Turing Test</h1> <p>To share some thoughts on a more comprehensive Turing Test, I do want to clarify that it is designed to measure Intelligence, not accuracy of next-predicted tokens. As such, a preliminary concept to convey concerns <a href="https://hrstraub.ch/en/the-theory-of-the-three-worlds-penrose/">Roger Penrose’s 3 worlds</a>: 1) Platonic, 2) Physical and 3) Subjective. Almost all existing benchmarks measuring any faculty of Human Intelligence, such as to produce language, focus on evaluating in the Platonic space, the space of abstract ideas. There do exist benchmarks measuring faculties in the physical space, such as in robotics, with examples including (cute) <a href="https://www.youtube.com/watch?v=RbyQcCT6890">robots playing football</a> against one another <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, however, these benchmarks then don’t measure faculties in Platonic space.</p> <p>I will also focus on the breadth of tasks, instead of the depth. This means the Extended Turing Test (better named Extended Turing Benchmark) has as many diverse tasks as possible, instead of having a single task modality, such as predicting the next text-token, but it is semantically diverse within this domain (predict the next token in English, in Spanish, in Chinese, among others). Following the main idea of this article, I won’t be focusing on next-token prediction either, though it is an integral part of it. I also note this is not meant to be the final Extended Turing Benchmark, and feedback is most welcomed.</p> <table> <thead> <tr> <th>Task</th> <th>Challenging aspects</th> </tr> </thead> <tbody> <tr> <td>Solve a mathematical conjecture, like Goldbach’s Conjecture</td> <td>The answer is not given in the training set</td> </tr> <tr> <td>Achieve a United Nation’s Sustainable Development Goal</td> <td>Requires interplay of Platonic and Physical space</td> </tr> <tr> <td>Find a cure for a disease of global concern</td> <td>Requires physical experimentation and searching over hypothesis space</td> </tr> <tr> <td>Propose a plan for a successful start-up</td> <td>Same as above</td> </tr> <tr> <td>Break a world record</td> <td>Same as above</td> </tr> <tr> <td>Ask questions</td> <td>Same as above</td> </tr> <tr> <td>Solve a conflict of global concern</td> <td>Requires interplay of Platonic, Physical and Subjective spaces</td> </tr> <tr> <td>Open-ended debate on moral dilemmas, such as the Trolley Problem</td> <td>No correct answer, requires proficiency in Subjective space</td> </tr> </tbody> </table> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>I don’t actually mean all benchmarks are wrong. Some benchmarks have indeed yielded insight into what constitutes Human intelligence, and they are good attempts at quantifying faculties of intelligence like visual reasoning, and analogy-making. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>The match caused some sensation despite it is not played against humans, further exacerbating a point made earlier that sensorimotor control, in addition to PEM, is an integral part of Intelligence. As such, I can’t understand how and why autoregressive models are advertised as the key to Artificial General Intelligence. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="creative-work"/><summary type="html"><![CDATA[I argue we Humans are more than next-state prediction machines.]]></summary></entry><entry><title type="html">Concept-Bottleneck Modelling for Interpretable Melanoma Classification</title><link href="https://awxlong.github.io/blog/2023/concept-melanoma/" rel="alternate" type="text/html" title="Concept-Bottleneck Modelling for Interpretable Melanoma Classification"/><published>2023-12-04T16:42:00+00:00</published><updated>2023-12-04T16:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/concept-melanoma</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/concept-melanoma/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A research project adopting the concept-bottleneck modelling technique for interpretable melanoma classification.]]></summary></entry><entry><title type="html">Review of the paper Prediction of mechanistic subtypes of Parkinson’s using patient derived stem cell models</title><link href="https://awxlong.github.io/blog/2023/mechanistic-subtypes-parkinson-copy-copy/" rel="alternate" type="text/html" title="Review of the paper Prediction of mechanistic subtypes of Parkinson’s using patient derived stem cell models"/><published>2023-11-02T19:42:00+00:00</published><updated>2023-11-02T19:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/mechanistic-subtypes-parkinson-copy%20copy</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/mechanistic-subtypes-parkinson-copy-copy/"><![CDATA[<h1 id="brief-summary">Brief summary</h1> <p>The paper by <d-cite key="dsa_2023_prediction"></d-cite> leverages deep learning architectures to solve a pentanary classification task given either a cell’s tabular features or images. The five independent classes are one healthy control and four subtypes of Parkinson’s Disease: familial proteinopathy (SNCA), environmental proteinopathy (\(\alpha\)-Syn oligomer), and two subtypes characterized by different mitochondria dysfunction pathways. These pathologies were chemically induced on stem cells. Fifty-six phenotypical features of them were extracted automatically and recorded as tabular data, along with images of the cells extracted via microscopy. Both data modalities were labeled with one of the five classes.</p> <p>The research team trained separately a dense feedforward neural network (DNN) to classify on the tabular data, as well as a convolutional neural network (CNN) to classify on image data. The test classification accuracy achieved by the DNN reached around 83%, while the CNN 95%.</p> <figure> <img src="/assets/img/parkinson.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="bottleneck">Two separate models are trained on different datasets on the same task of Parkinson subtype classification. Figure extracted from the original research article at https://www.nature.com/articles/s42256-023-00702-9</figcaption> </figure> <h1 id="my-comments-and-future-research-directions">My comments and future research directions</h1> <p>Generally, in the deep learning literature, it is acknowledged that the usage of DNNs comes at the expense of poor explainability. Despite achieving high classification accuracy, these models are black-boxes. Nonetheless, there are ways to identify what are the features that the neural networks pay the most attention when deciding on a classification label, mainly by looking at its last layer’s activation and tracing back to the input space which input feature is associated to it. In CNNs, the technique employed by the research team is called the ShAP (SHapley Additive exPlanations) method.</p> <p>The authors managed to identify in both the DNN and CNN that the mitochondria, lysosome and the interaction of both features mainly contributed to the classification decisions of both models.</p> <p>One future research direction I am interested is exploring whether by integrating both data sources can improve performance and yield explainability, because the original work trains separate models, trained on different datasets.</p> <p>One source of inspiration is from <d-cite key="li_2023_v1t"></d-cite>, where they integrate image data along with a mouse’s behavioral features to predict its neural responses collected from neural recordings. Another source of inspiration is drawn from concept-bottleneck models <d-cite key="koh_2020_concept"></d-cite>. There, a CNN in charge of processing images doesn’t learn to output a classification label, but instead to output features that are relevant to the image. These features, in turn, are annotations of the image stored in tabular:</p> <figure> <img src="/assets/img/bottleneck.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="bottleneck">A depiction of the pipeline of a concept-bottleneck model. The first half outputs a set of concepts given an image, which can be learnt from intricate annotations, or metadata, of the image. The second half outputs a classification label. Figure extracted from the original paper </figcaption> </figure> <p>Altogether, with regards to the work by <d-cite key="dsa_2023_prediction"></d-cite>, one interesting extension to their CNN is to have it not predict a Parkinson subtype, but rather learn to predict the cell’s physiological features stored as tabular data given image input. Subsequently, use the features to train a multi-class regressor using standard softmax to output a classification label. The prospect is that this hybrid model can leverage the high accuracy prediction of the CNN, whilst being explainable thanks to the logistic regressor.</p> <p>As a further improvement, we can use a <a href="https://arxiv.org/abs/2210.11394">Slot Transformer</a> instead of the CNN with the hope of learning a disentangled representation given the image with its annotations. However, the architecture will be more computationally expensive. A pretrained Slot Transformer that already learnt to disentangle CLEVR-Scenes may be more powerful than training it from scratch.</p>]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><summary type="html"><![CDATA[comments on a paper that leverages deep learning to classify cells into Parkinson's subtypes]]></summary></entry><entry><title type="html">A mathematical analogy for understanding creativity</title><link href="https://awxlong.github.io/blog/2023/creativity/" rel="alternate" type="text/html" title="A mathematical analogy for understanding creativity"/><published>2023-11-02T13:42:00+00:00</published><updated>2023-11-02T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/creativity</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/creativity/"><![CDATA[<p>Math is really a beautiful framework with which we can analyze several ideas. For instance, beauty can be seen through the lenses of math using symmetry and geometry, and the study of complex systems can be cast through calculus and probability theory.</p> <p>Given this premise, can we use math to understand what creativity is. Here is my attempt:</p> <h1 id="how-can-creativity-be-defined">How can creativity be defined?</h1> <p>Let us run the following thought experiment. Imagine you’re an enthusiastic pursuer of truth. The whole universe is simply an empty space. In this void, in front of you are a some numbers and two operators: + (ordinary addition) and * (ordinary multiplication)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p> <p>Science, or the discovery of knowledge, is analogous to discovering new numbers. You randomly add two numbers, 1 and -1 and discover 0. Then you add 1 to 1 and obtain 2. Repeating this process, you end up discovering all the integers. Furthermore, you can find patterns that lead to theorems among the integers, such as properties of commutativity, transitivity and distributivity.</p> <p>However, from a metaphysical point of view, addition and multiplication over integers is closed in this set, therefore you can’t discover anything beyond the horizon of integers.</p> <h1 id="can-you-discover-anything-else">Can you discover anything else?</h1> <p>Creativity, in this scenario, would be proposing another operator such as division. Having this new operator in your mind’s eye to perceive reality, you can now compute a new class of numbers, the rationals and thus discover the set of all real numbers.</p> <p>In either case, prior and after the discovery of the division operator, the infiniteness of numbers is analogous to the infinity of knowledge.</p> <p>Creativity can also materialize by introducing the square root, and this new operator extends our perceptual horizon to encompass the realm of complex/imaginary numbers.</p> <h1 id="more-analogies">More analogies</h1> <p>A different civilization from another point in space, using a different set of symbols to encode numbers, may make similar discoveries, i.e., numbers obtained following the same principles of computation using + and *. Moreover, they may also discover commutativity, transitivity and distributivity. This analogy represents the universality of knowledge.</p> <p>In the above thought experiment, I’ve ignored letters, phonemes, and words to simplify this overly-idealized world to get my ideas on what is creativity across.</p> <figure> <img src="/assets/img/creativity.jpg" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="creativity"> The thumbnail is generate by Dall-E powered MS Bing Image Generator given the prompt "your interpretation of what creativity is". </figcaption> </figure> <p>PS: Demis Hassabis, Deepmind CEO, also comments on another definition of <a href="https://www.youtube.com/watch?v=Gfr50f6ZBvo">creativity, or AGI</a>. This is going beyond statistical interpolation and extrapolation of training data. An AGI model that is creative should be able to give an output to an instruction such as “design a game that has simple rules, but is extremely hard to master, and can be played for hours”.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Perhaps these numbers and operators existed there a priori, maybe they emerged from a Creator, or perhaps they emerged from a singularity. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><summary type="html"><![CDATA[Using numbers to illustrate an intuitive definition of what creativity is]]></summary></entry><entry><title type="html">The many interpretations of terms in artificial intelligence</title><link href="https://awxlong.github.io/blog/2023/many-faces-ML/" rel="alternate" type="text/html" title="The many interpretations of terms in artificial intelligence"/><published>2023-11-01T13:42:00+00:00</published><updated>2023-11-01T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/many-faces-ML</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/many-faces-ML/"><![CDATA[<p>I bumped into this paper on <a href="https://twitter.com/ChristophMolnar">X (formerly Twitter)</a></p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">How do researchers define interpretability and explainability?<br/><br/>An overview:<a href="https://t.co/bNjKv8F44g">https://t.co/bNjKv8F44g</a> <br/><br/>Summary: No consensus<br/><br/>My opinion: It&#39;s already the Wild West, no one will stick to any definition. Most pragmatic is to treat them interchangeably. <a href="https://t.co/mEh6mUuk4C">pic.twitter.com/mEh6mUuk4C</a></p>&mdash; Christoph Molnar (@ChristophMolnar) <a href="https://twitter.com/ChristophMolnar/status/1718921189333045752?ref_src=twsrc%5Etfw">October 30, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>In today’s post, I don’t want to discuss interpretability or explainability, but rather elaborate on the above idea that in the AI literature, different authors express completely different ideas despite using the same words, often words that appear in our daily parlance, yet acquire uncommon interpretations.</p> <p>If you have just set foot into the world of artificial intelligence, here are my thoughts and advices on navigating this exciting, but often messy landscape.</p> <h1 id="terms">Terms</h1> <h4 id="explanation-hypothesis-model"><strong>Explanation, hypothesis, model</strong></h4> <p>When I began studying AI, the word “model” elicited the mental imagery of a 3D scale model of something. If I wanted a “model” of an atom, I could pick some recycled styrofoam balls, stick them together and, bam (pun intended), an atom is synthesized.</p> <p>Nowadays it mainly refers to a equation or function with some parameters $\theta$, where a ‘good’ model is defined as having optimal parameters $\theta$ that can make predictions with low error on the training and (hopefully) unseen, test data, compared to a ‘bad’ model.</p> <p>In rare cases, such as in <a href="https://www.youtube.com/watch?v=NsID1iM8gRw">Prof. Joshua Tenembaum’s talk on Cognition</a>, a (physics-) model can refer a simulator implemented in Unity.</p> <h4 id="knowledge"><strong>Knowledge</strong></h4> <p>This word may cause some confusion to peers with a philosophy or psychology background. At least to me, prior to studying AI, ‘knowledge’ meant pieces of information encoded in natural language, e.g. “Mount Chimborazo is actually higher than Mount Everest” somewhere in the brain. I don’t know how, nor where, nor whether it is encoded in natural language.</p> <p>At least in the early days of AI, first order logic constraints such as <code class="language-plaintext highlighter-rouge">smoker(X) :- edema(X)</code>, read as “it is true that if X is a smoker, then X has edema”, does resemble ‘knowledge’ encoded in a way that seems familiar to us.</p> <p>However, this is not always the case. The word ‘knowledge’, such as in ‘prior knowledge’, mainly used in Bayesian modelling, has not much to do with logic predicates or natural language. It mainly refers to what you believe on the distribution of the parameters of your model come from before your model is trained on any data. If you think your model must be parametrized by weights that more often take extreme values, you can set a prior belief with distributions with heavier tails. Guidance on what prior distributions can be used to describe your parameters can be checked in this <a href="https://twitter.com/bindureddy/status/1708664380987220427">tweext</a></p> <p>Lastly, sometimes researchers may even say that ‘knowledge’ of the data is encoded in the parameters of the model, which leads us to the next section:</p> <h4 id="explainability-and-interpretability"><strong>Explainability and interpretability</strong></h4> <p>Explainability and interpretability are terms that can be used interchangeably, as shown in the above paper. I avoid making a distinction in this post, and just focus on sharing what is usually meant by either term.</p> <p>The simplest manifestation of explainability can be observed with simple linear models like linear or logistic regression. Here explainability simply refers to the presence of coefficients estimated per each input variable of the model. These coefficients’ magnitudes often denote the contribution of the variable to the overall prediction of the model. Other non-linear, machine learning models such as decision trees are by design explainable, and such quality mainly refers to the thresholds appearing at each junction in the decision trees.</p> <p>Concerning deep learning, particularly convolutional neural networks (CNN), explainability can refer to the computation of feature maps on the input space to see on average, what are the filters of the CNN observing when solving a task like classification given an image. Deep generative models (DGM) that leverage neural networks and learn latent representations of the data can also become ‘explainable’ if there is some post-hoc (i.e. after training) processing of embeddings, such as dimensionality reduction through principal component analysis, that can map the embeddings on 2D space. In such space, it is hoped that the DGM learns disentangled features, albeit how are they interpreted is mainly up to the researchers, not by some objective reason.</p> <p>Explainability in the eyes of some authors may also refer to the capability of the researcher to compute a numerical value of the likelihood of the parameters of the data, or at least an approximation of it, such as the evidence lower bound. This scalar value can be used to compare the performance of different models, and these models can thus be referred to as “provable”.</p> <p>Other authors may also define explainability when deep neural networks, despite being black boxes, can accept inputs or display its predictions through a user-friendly interface that allows them to interact with the model.</p> <h1 id="bottom-line">Bottom line</h1> <p>My key advice in this brief article is to convince you not to trust the “intuitive” interpretation of a word, especially if it is used on a quotidian basis. For example, in my personal view, to be “explainable” means to be able to justify its own answer. Just like a friend to whom I ask why didn’t they go to a concert by singer Aimer, to be “explainable” (at least when referring to the common meaning of this word in daily life) means that they can give a justification as to why didn’t they go (maybe they became sick, or admit they have poor taste in music). ‘Explainable’ in the machine learning literature has not much to do with this daily life interpretation.</p> <p>Finally, the above terms are simply the tip of the iceberg. In the wider research literature, authors can diverge in what they mean by on many terms such as “language”, the word “intelligence” itself, and more controversial terms like “artificial general intelligence”. Because I don’t know the precise meaning of them myself, I refrain from commenting furthermore.</p> <p>Happy learning!</p>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><summary type="html"><![CDATA[Some thoughts concerning how to interpret quotidian words appearing in AI research: knowledge, explanation, etc.]]></summary></entry><entry><title type="html">Structured Gradient Descent</title><link href="https://awxlong.github.io/blog/2023/sgd/" rel="alternate" type="text/html" title="Structured Gradient Descent"/><published>2023-10-31T13:42:00+00:00</published><updated>2023-10-31T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/sgd</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/sgd/"><![CDATA[<h1 id="a-tale-of-a-scientist">A tale of a scientist</h1> <p><em>Shirley wanted to become a scientist. She wanted to find about jumping behavioral patterns of <a href="https://en.wikipedia.org/wiki/Tree_cricket">tree crickets</a>, which are insects of the order Orthoptera. Specifically, she aimed in testing the hypothesis that tree crickets understood human-issued commands that ordered them “to jump”. She caught a bunch of crickets and started testing her idea by telling one group of crickets to jump, and leaving a control group of crickets telling them nothing. Most crickets in either group jumped, merrily and highly. To her, these results weakly suggest that crickets understood the order to jump and did so accordingly. Then for the same group of crickets in both conditions, she mercilessly cut their legs off. Again, she issued the command to jump for one group, and left the control without saying anything. None of the crickets jumped. What did Shirley the scientist conclude? “Crickets stop understanding human-language after having their legs chopped off”</em></p> <figure> <img src="/assets/img/cricket.jpg" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="sgd"> Depiction of a cricket generated by Dall-E powered MS Bing Image Creator given the prompt "a beautiful, photo-realistic cricket" </figcaption> </figure> <h1 id="its-not-her-fault-the-parallel-with-deep-neural-networks-learning">It’s not her fault: The parallel with deep neural network’s learning</h1> <p>Perhaps you’ve heard of the above story in a course introducing the scientific method.</p> <p>This tale is interesting given the parallels there exists with current deep neural networks. These are analogous to Shirley, their parameters a metaphor to what she learns, and her crickets resemble the training data. No one has any control with regard to what she learns given the training data. When she concluded that “crickets can’t interpret English after having their legs cut-off”, that is not a “wrong” interpretation, per se. Rather, reaching this conclusion is a simple consequence of how she was designed and the coincidences of the training data (i.e. crickets did stop jumping after their legs were chopped off despite Shirley’s instructions to jump).</p> <p>With respect to deep neural networks, you can train it to classify images in CIFAR-10 or FashionMNIST, or more generally, train to generate the images. As per what sort of embeddings it learns, how it learns, among other aspects of learning, are out of our control. Exemplified in <a href="https://arxiv.org/pdf/2011.12854.pdf">Stammer et al., (2021)</a>, in a ColorMNIST dataset where a ResNet learns to classify digits, if by chance most digits nines appeared with a purple color, then this ResNet would classify purple color digits as 9 in the test set, even though the digits may be of different shape. In such a case, the model has learned that the concept of color defines the identity of a digit, since we have no control over what artifacts does the ResNet pick.</p> <p>The main powerhouse driving learning is stochastic gradient descent, and it is partly due to this stochasticity that deep neural networks are black-boxes.</p> <p>The stochasticity of gradient descent is mainly derived from the order and the semantics of the incoming training data. We thus have no control over what the model learns. The AI research community has proposed several methods, such as trying to learn disentangled representations of data, which can be obtained by having a more intricate labelling of the dataset. Consider the <a href="https://cs.stanford.edu/people/jcjohns/clevr/">CLEVR Diagnostic Dataset</a>, in which each object appearing in each training image is exquisitely labeled with its (x, y, z) coordinates, shape, color, size, material and positional relations among others, along with questions, answers and functional programs to answer each question. A variant of such dataset with real-life scenes is <a href="https://arxiv.org/abs/1902.09506">GQA</a>.</p> <h1 id="structured-gradient-descent">Structured gradient descent</h1> <p>Given these precedents, I wanted to depict my desire to see in the future a kind of “structured” gradient descent (which in English coincides with the same acronym as stochastic gradient descent):</p> <figure> <img src="/assets/img/sgd4life.png" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="sgd"> Not an actual depiction of structured gradient descent, but nonetheless a cool logo. </figcaption> </figure> <p>Structured gradient descent, as its name suggest, would allow the training of a model where we have control over what kind of representation it obtains from the supplied data. Analogous to the tale above, it’d akin to explicitly telling Shirley that crickets can’t understand human language, and that her line of reasoning has to start from the motility of legs of tree crickets in giving them the ability to jump. Afterwards, the jumping behavior of crickets is up to her to explain.</p> <p>The unanswered questions that follow would include:</p> <ul> <li>How would structured gradient descent materialize in a learning algorithm? <ul> <li>Will a grammar, prior knowledge in the form of logic or external ontologies play a pivotal role in the design of such an algorithm?</li> </ul> </li> <li>Will structured gradient descent allow a model to learn a disentangled representation, along with benefits such as a lower demand for computational power, dataset size and model size? <ul> <li>What would be the demands for structured gradient descent to run? More intricate labeled data?</li> <li>Most importantly, would more intricately labeled data compensate for low data?</li> </ul> </li> </ul> <figure> <img src="/assets/img/structuredgd.png" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="sgd"> A (hopefully) cool logo of SGD</figcaption> </figure>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="creative-work"/><summary type="html"><![CDATA[Some thoughts on the concept of structured gradient descent and a piece of artwork associated to it]]></summary></entry><entry><title type="html">A tale of two worlds: the natural and the mathematical</title><link href="https://awxlong.github.io/blog/2023/tale-two-worlds/" rel="alternate" type="text/html" title="A tale of two worlds: the natural and the mathematical"/><published>2023-10-14T13:42:00+00:00</published><updated>2023-10-14T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/tale-two-worlds</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/tale-two-worlds/"><![CDATA[<p>There seems to exist two worlds which exist in the universe.</p> <p>The real, natural world which can be accessible through empirical means, and the abstract, mathematical world which are the means themselves with which we observe the real, natural world. Here, the real, natural world is what we can perceive with our senses. The mathematical world is an umbrella term to refer to the ‘math’ in our common day parlance, as well as including logic in its many forms: propositional, first-order and higher-order.</p> <p>The real, natural world is subject to the laws of physics, which underlie chemistry and biology. In the abstract, mathematical world, such laws are not present: there we can think of breaking Newton’s Universal Laws of Gravitation, the overall decrease of entropy against the Second Law of Thermodynamics, or, in general, anything within our mental capabilities<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">1</a></sup>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/world-math.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A depiction of the complementarity of the empirical, natural world and the abstract, mathematical world. The image is generated via Microsoft Bing's Image Creator with the prompt "on the left, a picture of the natural world. On the right, a picture of equations, geometry and calculus". I have tried several other prompts and chose the one I believed suits what I wanted to convey. As I have discussed [before](https://awxlong.github.io/blog/2023/brain-fractal/), as well as in this article, I've avoided abstract terms such as 'complementarity' or 'abstract' since I'm unconfident a large vision system can understand it. </div> <p>To recount this tale of two worlds, we can take a step back to ancient debate of <a href="https://plato.stanford.edu/entries/innateness-cognition/">nativism and empiricism</a><d-cite key="samet_2012_innateness"></d-cite>. Namely, nativism argues that knowledge emerges from axioms and inference rules applied to those axioms that we humans are endowed since birth. Elaborating, thanks to a random mutation that kickstarted the beginning of <em>Homo sapiens</em>, our species have grown a capacity to understand elements of the natural world in terms of its objectness, and its fluent properties, such as shape, numerosity and size. This capacity allow us to understand objects of the real world in terms of how they change, how they hierarchically relate to one another and how they causally relate to each other. This capacity contrasts to what empiricists argue, namely that we acquiere knowledge dependent on the experiencies we’ve been exposed, and determine (causal) relationships among objects of the real world based on the extrapolation of previous experiences.</p> <p>I argue this is a false dichotomy, and that underlying it there is the complementarity of nativism and empiricism. Nativism mostly deals in how the human mind navigates the internal, abstract, mathematical world. If we want to prove that there exists infinite primes, nativists would argue that such theory can be proved via universal inference procedures such as proof by contradiction, and manipulating symbols involving the definition of primeness of a number. In such an scenario, empiricism would say that we have to extrapolate from prior observations of prime numbers to conclude they’re infinite. However, who can live for infinite amount of time in order to witness the infinity of primes and assert the above theory! Another example would concern Goldbach’s Conjecture, which proposes that all even numbers greater than two are the sum of two primes <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">2</a></sup>. Can we prove it by observing all even numbers, as nativists would argue? I don’t believe so, we have to resort to the mathematical reasoning mechanisms we were endowed in our mind. One way is to prove it by <a href="https://en.wikipedia.org/wiki/Euclid%27s_theorem">contradiction</a>. Assume there aren’t infinite primes, multiply the finite primes, and add one. This new number is not divisible by any of the finite primes, and thus is a prime. Infinity is a concept easier to understand and grasp when it is defined, as it can’t be learnt.</p> <p>However, we <em>Homo sapiens</em> don’t only reason about the mathematical world. We <em>use math to understand the external world</em>, and understanding the external world recquire experiencing it, and with experience comes learning. Empiricist, thus, can explain how we are able to sort out the relationships of objects in the external world as a form of interpolating and extrapolating observed evidence. However, empiricists need nativists as much as the latter need the former. Empiricism need the structure of nativists to accommodate the perceptual stimuli of the external world in order to reason about their relationships and derive new ones. Nativists need to learn from external stimuli so that the structure can evolve and be able to describe more complicated relationships.</p> <p>The natural world needs math to be understood, and math needs the natural world to exist. From this complementarity, it is argued that the mind consists of a grammar or a domain-specific language with which to give <a href="https://plato.stanford.edu/entries/language-thought/">shape to thought</a><d-cite key="rescorla_2019_the"></d-cite>. This grammar would be the basis for parsing the world’s objects in terms of their objectness, fluent properties and causal relations. Additionally, this innate grammar should have stochastic properties as probability acts as the pivot to ground this grammar with the natural world, and be able to describe it in a coherent manner. For example, after observing past experiences of a ‘hammer’, we can probabilistically ground a high-level, symbol ‘hammer’ with the actual object in the real world. Accommodating ‘hammer’ within some structure allow us to reason about its relations, functions, and properties. For further reading of stochastic grammars for vision, you can check Prof. Zhu-Song Chun’s <a href="http://www.stat.ucla.edu/~sczhu/Books/Book_2_Parsing.pdf">textbook on this topic</a> or <d-cite key="zhu_2020_dark"></d-cite>, which I find extremely difficult to follow, but nonetheless an interesting manuscript to read.</p> <h1 id="implications-of-rejecting-either-nativism-and-empiricism">Implications of rejecting either nativism and empiricism</h1> <p>I advocate that there exists complementarity between the mathematical and natural world. However, I can’t stop others from opting for the dichotomy between them, thus not only pitting nativism against empiricism or the other way around, but also rejecting one at the cost of the other.</p> <p>If we ignore nativism, and assume past experiencies are all that is needed to understand how the mind deduces new knowledge, we can think of the following. We can’t give shape to the mathematical world. It doesn’t “exist” in the real world. Circles, polygons, numbers, operations, vectors, matrices, infinity, among others, all reside within our mind. A friend once pointed out that he can draw a line on a dining table<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">3</a></sup> with a marker, and that’s a geometrical line. I disputed: “that is ink placed on a surface, which in your head is representing a geometrical line. While the ink and wooden surface are tangible objects, the geometrical line isn’t. Miraculously, however, I can also see and understand what you’re thinking concerning geometrical lines, suggesting the universality of math in the minds of <em>Homo sapiens</em>” (I may have edited my response to sound wiser, as well as edited the whole story to make it more entertaining).</p> <p>That math doesn’t exist in the real world, and thus can’t be picked up with our perceptual systems can explain why large vision models like DALL-E or MidJourney struggle with generating images which contain letters or numbers, unless they are fed with prior knowledge on <a href="https://research.google/pubs/pub52496/">characters</a><d-cite key="liu_2023_characteraware"></d-cite>. This is partly because the vast amount of data in which they’re trained mainly correspond to images taken from the natural, tangible, external world, e.g., photos under different styles of animals, buildings, groups of people, or landscapes.</p> <p>We also can’t accept nativism at the expense of empiricism. The history of artificial intelligence is a good reference to observe that resorting to only expert systems consisting of universal inference rules like forward and backward chaining results in systems that are disconnected with real world dynamics. Whilst math can be independent of the real natural world, it is nonetheless intricately connected to the world in order to describe it.</p> <h1 id="implications-on-the-quest-for-artificial-intelligence">Implications on the quest for artificial intelligence</h1> <p>Deep learning ignited the summer of AI of the twenty-first century, and the transformer is one of the torches that keeps the summer shining. How should we keep this summer from ending?</p> <p>Deep learning is a mostly pure expression of empiricism. Deep neural networks don’t manipulate symbols, are inspired by the brain’s architecture, and are mainly learn via training from a dataset of past experiences. There are several empirical observations, and <a href="https://plato.stanford.edu/entries/scientific-reduction/">reductionist</a> <d-cite key="riel_2014_scientific"></d-cite> claims suggesting that mental activities such as intuition and reasoning are the result of the biochemical processes of the brain. There are the recurrent achievements of new state of the arts performance in benchmarks used to measure the human faculties of vision and language. One of the striking research papers that caught my attention involved decoding a person’s brain activity to their perceived image:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/decode.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The model's architecture, which is a diffusion model (i.e. a probabilistic model consisting of deep neural networks). On the right, the upper row are images presented to a person whose brain activity is recorded. This brain activity is then decoded to the below row of images. Image extracted from https://sites.google.com/view/stablediffusion-with-brain/ </div> <p>That there is some association between brain activity and external stimuli capturable by a probabilistic model is fascinating and encouraging for empiricism. However, most if not all research related to the neuroscience behind mental activity <strong>lacks the explanation of whether brain activity yields any insight into mathematical theorem proving or deduction</strong>. Experiments to explore this can involve assessing whether brain activity can reconstruct a person’s steps into solving an equation, deriving a proof or just in general have some abstract thought involving numbers.</p> <p>Furthermore, taking empiricism to an extreme may stifle the capabilities of an AI model, since radical empiricism subjects the model to be an extrapolation from supplied past experiences. It ignores the nativist capability of generating new thought, often called as creativity.</p> <p>Another counterargument to empiricism is also as follows. Image there is a program running on a computer. You can observe your program running on the screen, as in, you can see what input matches the corresponding output displayed. You also know the underlying algorithm specifying how the input should be transformed to the output. Now, you disassemble your computer and expose its mainframe, now able to look at the CPU and RAM running, all the while your program keeps running. In other words, only the computer screen is gone, so any high-level display your program disappears. You invite an empiricist and tell her that she can make all sorts of measurements she wants on the hardware of the computer (CPU, RAM, motherboard, graphics card). Such measurements could include voltage, heat maps, temperature, and whatever she can think of. Of course, because the computer screen is gone, and she is prohibited from reassembling it back, she is restricted to measure the low-level components of the computer. Now comes the main question, can she be able to figure out from measurements made on the hardware the algorithm being executed? Here, the computer and its low level components correspond to the biological brain, and the software is analogous to mental activity.</p> <p>While low-level activity can reflect to some degree higher level processes, I’m skeptical we can understand and simulate higher-level cognition without assuming its pre-existence. Therefore, we can only describe high level mental activity with high level mental activity. In the thought experiment, this would entail the empiricist may have to ‘hypothesize’ in a separate computer different programs that could be running in the monitor-less computer, use feedback from the low-level measurements obtained from the monitor-less computer, and find which hypothesized programs matches the measurements from the monitor-less computer. Such iterative process of trial and error, which combines both the high-level cognitive process of creativity and low-level signal processing for feedback.</p> <p>As argued repeatedly, the brain’s architecture, neuronal activations and modifications in synaptic strengths all belong to the external world. Drawing inspiration from the brain is a powerful means to achieve AI. However, mental activity such as deduction, and emergence of new ideas through <a href="https://aeon.co/essays/how-close-are-we-to-creating-artificial-intelligence">conjecturing</a><d-cite key="deutsch_2012_how"></d-cite> is a property that has to be embedded in the system, as it can not be learned by picking cues from the environment.</p> <h1 id="final-comments">Final comments</h1> <p>When I just learned about the scientific method, my colleagues and I were told of the story of <em>“The Blind Scientists and the Elephant”</em>. Each one of the scientists had to rigorously examine a part of the elephant in front of them and conclude what they were looking at:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/b-scie-elep.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> While this caricature illustrates that all scientists are comically wrong, it still conveys a deep message: each scientist, assuming each is rigorous in their findings, is correct. Points of disagreements are perhaps not due to contradictory findings, but simply because, as depicted, they are looking at different parts of the elephant. </div> <p>Nativists and empiricists are analogous to the scientists, and the brain is represented by the elephant. It is not the case that both hold opposite views. Rather, they are providing explanations to different aspects of the mind: empiricists focus on learning, and explain that the brain’s neuronal activity-associated synaptic changes can encode knowledge from the environment. Nativists are not opposed to this, per se. They are just looking at other aspects of the mind: the abstract, intangible portion which deals with the manipulation of math. Math is perhaps a faculty that is native to <em>Homo sapiens</em> and is the basis of thought.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:3" role="doc-endnote"> <p>what I listed above mainly involves equations with real and maybe imaginary numbers. However, what if there is more to these types of numbers that we haven’t thought of, or we’re incapable to even comprehend? Because this question is off-topic, I left it as a footnote. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:1" role="doc-endnote"> <p>the easier version proposed by the Tortoise from Gödel, Escher, Bach is easier to prove, which states that all even numbers less than or equal to two are the sum of two primes. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>he didn’t clean it afterwards <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>An Xuelong</name></author><category term="blog-post"/><category term="food-for-thought"/><category term="research"/><summary type="html"><![CDATA[I argue about the complementarity of math and the natural world, about nativism and empiricism, and implications of rejecting either of them.]]></summary></entry><entry><title type="html">Brain-Heart Fractal, DALL-E and I</title><link href="https://awxlong.github.io/blog/2023/brain-fractal/" rel="alternate" type="text/html" title="Brain-Heart Fractal, DALL-E and I"/><published>2023-09-15T13:42:00+00:00</published><updated>2023-09-15T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/brain-fractal</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/brain-fractal/"><![CDATA[<h1 id="dall-e-and-generative-art">DALL-E and generative art</h1> <p>I have my share of skepticism of transformer-based models to generate art. My understanding of probability theory leads me to believe that a probabilistic model trained on a dataset can, at least interpolate, and, at most, extrapolate. Nonetheless, I don’t think creativity emerges from the averaging of past experiences or extrapolation of them. For example, consider a model like DALL-E which has learned from massive amounts of images what avocado and chair are. While it was impressive for it to generate an image when prompted with ‘avocado chair’ despite never being present, it is nowadays a ‘known unknown’. This is, such output can be readily explained by its capability to generate picture from the prompt, which we can think of as an extrapolation of both objects ‘avocado’ and ‘chair’ after seeing them for a large amount of times. Furthermore, both ‘avocado’ and ‘chair’ are real, natural objects, which empirically have been shown to be easier to learn than characters and numbers <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p> <p>To me, art is also a reflection of the artist. However, since DALL-E has no prior life anecdotes, struggles or joyful moments for it to express through art, I personally find its generated art, no matter how grandiose they are advertised as they are through various mediums, to be shallow. I am greatly skeptical of whether it can generate a piece of work that is impactful and meaningful given a prompt such as “your understanding of peace and coexistence”.</p> <h1 id="exploring-prompts-on-complementarity-of-brain-and-heart">Exploring prompts on complementarity of brain and heart</h1> <p>Nonetheless, if we disentangle DALL-E from the creation of art, and think of DALL-E as a tool for artists rather than fall for the frenzy discussion that AI is soon replacing artists and winning art competitions. In my view, that AI-generated art can win <a href="https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html">competitions</a> reflects not a situation where AI is going to overtake humans in art (since AI didn’t even participate in this competition, it was a human who did, and prompted AI to generate a picture given specifications), but rather that we can elevate the standards of current competitions. I mean, wouldn’t it be incomprehensible if we pit humans against driverless automobiles in a race? We instead make racers on cars compete against other racers on cars. Therefore, I believe future competitions of art should allow artists to explore their humane creativity with the aid of DALL-E.</p> <p>Given the above premise, I have indulged myself in exploring DALL-E powered image generators in the web, including <a href="https://www.bing.com/images/create">Microsoft Bing Image Creator</a>, <a href="https://www.craiyon.com/">Craiyon</a>, <a href="https://openai.com/dall-e-2">Open AI’s Dall-E</a> and <a href="https://deepai.org/machine-learning-model/text2img">DeepAI’s text-to-image generator</a>. I didn’t focus with objects of the real world, such as space cats, Van Gogh-styled buildings or cyberpunk-ish characters, but rather focus on abstract concepts.</p> <p>Given my interest in the consilience of knowledge and complementarity of seemingly opposing ideas, I wanted to instead try abstract ideas like “the complementarity of the brain and heart”. Before resorting to DALL-E, I gave a try myself in sketching the idea:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/sketch.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> What I wanted to convey is that pure, cold rationality embodied by the brain complements the warmth and love embodied by the heart. Because I also like fractals, as in complexity is the result of the composition of simple rules, I wanted to use them as a pivot to join both sides. </div> <p>The following are some images generated by the above models:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/deep-ai-complementarity.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/openai-complementarity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/bing-complementarity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The images are from DeepAI, OpenAI, and MS Bing, respectively. Each model is able to generate a brain, sometimes along a heart, but I personally am not convinced about whether it can illustrate complementarity. </div> <p>I tried to be more specific, with prompts such as “brain and heart joined together harmoniously like the yin and yang”, “a depiction that the brain and heart work together harmoniously to make us human”, and “beautiful, intricate fractal shaped as the brain and heart innervated with one another, red, black and white”.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/yinyang-heart.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/make-us-human.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/intricate-fractal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The images are generated from OpenAI without paying. Despite providing more specific prompts, I'm generally disastified with the generated images. The model sometimes generate specific elements mentioned in the prompt, such as the 'human' in '... makes us human', even though it doesn't have to. </div> <p>However, I do like some of the generations. I also share more images obtained from MS Bing’s Image Creator, which I mostly like. The prompts I used were “brain and heart interwoven made of fractals” and “brain and heart composed of fractals, high contrast red and black color”:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/interwoven-bing.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/high-contrast.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The images are from MS Bing's Image Creator with free tokens. I really like these generated images. Notice how I avoid using "complementarity", since I argue that the model can't understand high-level concepts. Nonetheless, by specifying the properties of what I want, I'm able to generate images close to what I desire. </div> <p>I’m actually using one of the images as my website icon.</p> <p>There is a plethora of images I’m skipping since I generated too many with different prompts across platforms. And I may generate once in a while if I’m interested in checkpointing whether they have improved.</p> <p>One of them which I also really liked is from DeepAI. I like the overall heart-shape, the colors used, and the presence of fractals:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/brain-heart-deepai.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I really don’t think AI can replace artists. In general, I support the idea that “AI won’t replace [insert job profession]”, but rather “those who use AI will replace those who don’t”. I’m confident that anytime soon, AI models won’t be able to grasp high-level concepts since they lack their machine language which I believe is a tool to develop <a href="https://www.youtube.com/watch?v=Q-B_ONJIEcE">their own cognition for understanding the world</a>. Even if they acquire machine language, they still need to evolve to grasp extremely complicated and intricate concepts like “complementarity”.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>My own hypothesis for such behaviour is because the model lacks its own machine language for which it can understand the world. There are objective reasons for why an avocado and a chair are the way they are, either due to biology or human affordance. However, such objective reasons are not present in the presentation of concepts like numerosity or with letters. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="creative-work"/><summary type="html"><![CDATA[I share some sketches on the complementarity of mind and heart made by me and DALL-E]]></summary></entry><entry><title type="html">Reflections on the Neurosymbolic Summer School 2023</title><link href="https://awxlong.github.io/blog/2023/nesy-summer-school/" rel="alternate" type="text/html" title="Reflections on the Neurosymbolic Summer School 2023"/><published>2023-09-01T04:42:00+00:00</published><updated>2023-09-01T04:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/nesy-summer-school</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/nesy-summer-school/"><![CDATA[<p>I attended the NeuroSymbolic (NeSy) Summer School 2023 held <a href="https://neurosymbolic.github.io/nsss2023/">virtually</a> between August 29-30. What follows are my thoughts and comments with regard to some talks. They can be watched thanks to the available <a href="https://ibm.github.io/neuro-symbolic-ai/events/ns-workshop2023/">recordings</a>, which provide context for the ideas below:</p> <h1 id="reflection-on-talks-by-professors-benjamin-grosof-fabrizio-and-fabio-and-luc-de-raedt">Reflection on talks by Professors Benjamin Grosof, Fabrizio and Fabio, and Luc de Raedt</h1> <p>They mostly talked about the use of logic, propositional or first-order, to shape a problem domain and drive neural networks to make explainable predictions, discussing the strenghts and drawbacks of this approach.</p> <p>I am personally skeptical of the use of logic to encode knowledge or describe relations. This is because there exists infinite knowledge, and thus this entails that in order to solve real-world problems that involve complex systems, e.g. predicting cell progression in a tissue, may involve writing infinite amount of rules encompassing cell physiology or pathogenesis. It is also disputable what are the putative rules to encode in order to solve problems, and rarely two experts on the same field may converge on what rules should be encoded into a model for solving a problem like cell trajectory analysis. On a tangent, I also liked the point raised by Prof. Benjamin Grosof on defeasible knowledge, referring to rules which are only true circumstantially. For example, for a rule like ‘every cell has a nucleus’, this is only true when the cell is not going through anaphase of mitosis or if it’s a bacterium. One can relax the “absoluteness” of rules using temporal logic (‘something is true during certain timeframes’) or by encoding probabilistic knowledge, i.e., rules attached with probability distributions on their truth values. However, these only exacerbate the inherent combinatorial search space related to the search for rules that can answer a query.</p> <p>Let us illustrate with the following hypothetical logic program:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>friendOf(kerry,  natalia).
friendOf(natalia, shirley).
smoker(natalia).
emphysema(natalia).
smoker(X) :- friendOf(X, Y), 
             smoker(Y)
emphysema(X) :- smoker(X)

query(emphysema(kerry), ?)
</code></pre></div></div> <p>Given a knowledge base, there is knowledge that is already available (written explicitly in the knowledge base), which is a subset of the Herbrand Base of the knowledge base, namely all possible knowledge that can be deduced given the rules and available facts. For example, the Herbrand Base above would include that Shirley is a smoker given that she is a friend of natalia, therefore Shirley has emphysema. The same with Kerry: he is a smoker since he is a friend of Natalia, and he also has emphysema. If we annotate facts and rules with probabilities to turn into a probabilistic logic program, then this exponentially expands the size of Herbrand Base. Let us just say that Kerry is a friend of Natalia with probability of 0.6, which cascades into Kerry having emphysema with some probability. Given the annotated probabilities, this, in turn, means that there is 0.4 probability he isn’t, which cascades into another series of possibilities where he might be or not be a smoker, depending on the probability annotations for the rule of smoker. You may also disagree on whether this should be the rules written to model this simplistic world. And you can also wonder how such a setting scales in terms of complexity if we aim to solve problems concerning complex biological systems.</p> <p>Furthermore, another problem with encoding logic into a model is that it assumes the expert or researcher knows a priori what knowledge to encode. What if I don’t know how to write correct logic program, and how would I know whether it is incorrect?</p> <h2 id="if-not-knowledge-then-">If not knowledge, then ?</h2> <p>My criticism towards this kind of logic is that it doesn’t reflect the complexity of the real world. Namely, in the real world, there exists infinite knowledge yet to be discovered. I believe it is not the case that a human brain stores knowledge since there is infinitely available.</p> <p>Rather, I believe that the brain stores operations/programs which operate on the world to obtain knowledge. In other words, it doesn’t store knowledge, it scaffolds from the environment to obtain knowledge. For example, it is not the case that a Herbrand Base of some logic program is stored in my brain, rather, inside it there are programs, such as <a href="https://human.libretexts.org/Bookshelves/Philosophy/A_Miniguide_to_Critical_Thinking_(Lau)/01%3A_Chapters/1.08%3A_Patterns_of_Valid_Arguments">modus pollens and modus tollens</a> that allow to process this logic program that is written on the screen in front of you. What is interesting about this is that, for example, Shirley the person is not stored in my mind, she exists on the real world. What is actually in my mind is an operation like modus pollens, which when applied on her and her smoking habit, allows me to deduce she is a smoker. Perhaps the rule of whether she is a smoker is encoded in prior exposure to social dynamics of friends and smokers. Here, the encoding procedure, the manipulation of social dynamics and smoking habits can all be thought of as programs.</p> <p>Another source of beauty with programs is that one doesn’t need to write rules, but rather a domain-specific language with grammar rules for the machine to use it as its language of thought to interpret incoming data. As an analogy, I like to think of it as defining a programming language, where a machine learns to understand the world by writing and editing its own code, a form of self-reference.</p> <p>Furthermore, I hypothesize that perhaps we need a vector embedding for primitive programs and their compositions. Empirically, it has been observed that continuous representation of discrete symbols/concepts tend to speed up their manipulation by exploiting GPU parallelism, such as searching over them.</p> <p>Interesting work revolving on program synthesis include <a href="https://arxiv.org/pdf/1511.02799.pdf">neural module networks</a>, and an <a href="https://arxiv.org/abs/1704.05526">improvement</a> over this architecture.</p> <p></p> <h1 id="reflections-on-talk-by-phd-student-abulchair-and-prof-guy-van-den-broeck-can-llms-learn-to-reason">Reflections on talk by PhD student Abulchair, and Prof. Guy Van den Broeck: Can LLMs learn to reason?</h1> <p>Agreeing with Prof. Guy, I’m skeptical of the idea that large language models (LLMs) can reason. There is no precise definition of reasoning in the first place, but it at least involves searching over a space over all possible knowledge attainable from the current knowledge you have. Reasoning involves hypothesizing (composing new, but possibly invalid, knowledge from existing knowledge), validating with data, and staying consistent with such knowledge.</p> <p>Through reasoning, you should be able to search and derive knowledge that is out of distribution of the available data.</p> <p>On the other hand, because LLMs are statistical models, it is imprinted in their design that they can only estimate a probability space that approximates to the training data’s distribution through counting and averaging. Assuming that the test distribution can always be different to the training data distribution, then I’m unable to be convinced that LLMs are able to overcome the inherent incapability of addressing the inability to generalize out-of-distribution of statistical models.</p> <p>I think that only if the future is the average of the past and present, and there can be nothing novel emerging from them, then statistical models would be good simulators of the production of knowledge. However, the growth of knowledge does not happen as a continuation or reflection of past and present knowledge.</p> <p>That is why NeSy AI is an interesting field regaining attention as it seeks to integrate neural networks with the capability to reason (whatever that means), and achieve good performance on out of distribution tasks. </p> <h1 id="reflections-on-talk-by-ibm-researcher-michael-hersche-resnet-vs-raven-progressive-matrices">Reflections on talk by IBM Researcher Michael Hersche: ResNet vs. RAVEN Progressive Matrices</h1> <p><a href="https://github.com/WellyZhang/RAVEN">RAVEN</a> is a dataset that aims to simulate an IQ test for deep learning models:</p> <figure> <img src="/assets/img/raven.jpg" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="sgd"> Given a sequence of preceding panels, a model is tasked to choose out of multiple choices the correct panel that follows an implicit pattern present before.</figcaption> </figure> <p>IBM Researcher Michael Hersche managed to train a <a href="https://www.nature.com/articles/s42256-023-00630-8">convolutional neural network</a> with encoded rules appearing in the RAVEN dataset that succesfully solved it. However, as pointed out in a later panel discussion, the assumption is that the researcher knows beforehand what primitive rules underlie the generation of the implicit patterns in the progressive matrices.</p> <p>To mitigate this, I’m keen in observing whether NeSy architectures can tackle the still unsolved <a href="https://arxiv.org/abs/1911.01547">Abstract Reasoning Corpus</a>(ARC) by Prof. François Chollet. This is a more challenging problem, because first there is no correct answer to choose from multiple choices, and thus ensures statistical models don’t exploit artifacts of the problem setting. Instead, the model have to generate the correct panel given prior pairs of observations. Furthermore, and most importantly, there is a new underlying pattern every few training examples, and patterns appearing in the held-out test set never appear in the training set either:</p> <figure> <img src="/assets/img/arc.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="arc"> A guided user interface for playing around with the ARC dataset. A few task demonstrations precede an input grid that the model has to fill depending on what it believes is the underlying pattern shown. In this case, I believe the right side has been folded to the left </figcaption> </figure> <p>In my undergraduate <a href="https://awxlong.github.io/publications/">dissertation</a>, I proposed combining different datasets into a single benchmark suite for heterogeneously testing the capabilities of NeSy methods. One of the tasks involve assessing abstract reasoning, which is jointly inspired by RAVEN and ARC:</p> <figure> <img src="/assets/img/abs_eg.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="arc"> An illustration of a training example. The first four panels follow an underlying clockwise rotation pattern, and the model is tasked to generate a fifth panel that completes the sequence. Inspired by the [CLEVR-Hans dataset](https://github.com/ml-research/NeSyXIL), I introduce cofounders during testing on the color of the objects. </figcaption> </figure> <h1 id="reflections-on-the-talks-by-professors-chris-potts-yoshua-bengio">Reflections on the talks by Professors Chris Potts, Yoshua Bengio</h1> <h2 id="will-deep-learning-ever-be-transparent">Will deep learning ever be transparent?</h2> <p>The main constitution of deep neural networks is its parameters stored as values in matrices across layers. Will we ever be able to distill discrete concepts from such matrices?</p> <p>I doubt so. We can ask instead whether humans are interpretable, i.e., whether our neuronal activations are seldom explainable…</p> <p>There is research suggesting that perhaps neuronal activations do shed some insight regarding real-life perceptual stimuli, e.g., think of the <a href="https://en.wikipedia.org/wiki/Grandmother_cell">grandmother cell</a> which is a quotidian example that says that on average, a certain neuron of the brain fires whenever the corresponding human subject is exposed to a stimulus related to the subject’s grandma. For more interesting research associating neuronal activation to external stimuli, please read the Science magazine article <a href="https://www.science.org/content/article/hear-classic-pink-floyd-song-reconstructed-listeners-brain-waves">reconstructing audio</a>, or using generative AI (diffusion models) to <a href="https://github.com/yu-takagi/StableDiffusionReconstruction/blob/main/README.md">reconstruct images from human brain activity</a>.</p> <p>However, most of the research is restricted to reconstruct external stimuli.</p> <p>I am now curious as to what are the neuronal activations corresponding to someone proving the Pythagoras theorem, or the sequence of neuronal activations associated to proving the Goldbach conjecture? I ask because mathematical reality is <a href="https://awxlong.github.io/blog/2023/tale-two-worlds/">different</a> perceptual reality.</p> <h2 id="deep-learning-language-and-truth">Deep learning, language and truth</h2> <p>An interesting point raised above concerns statistics and the truth of a collected dataset. During the talk, it was mentioned that the learning the joint distribution of words is not the same as learning truthful statements. Consider the following example:</p> \[P(\text{the newborn is female}) \neq P(\text{joint occurrence of tokens 'the newborn is female'})\] <h1 id="my-comments-on-some-audience-members-questions">My comments on some audience members’ questions</h1> <p>There were very interesting questions raised through talks and panels I wished to share my thoughts on:</p> <h2 id="applications-of-knowledge-relational-reasoning">Applications of Knowledge Relational Reasoning</h2> <p>A colleague asked on the applications of knowledge relational reasoning. I think you can deploy knowledge bases, knowledge graphs or relational databases (which are subsets of logic programming) that can be used to manage a process pipeline in a business, such as price tagging, or moderation of security measures.</p> <h2 id="are-neural-networks-propositional">Are neural networks propositional?</h2> <p>I interpreted this question as whether neural networks can perform propositional logic, to which I’m skeptical. It is true that in the early history of neural networks, they were used to approximate simple functions representing logic gates, such as AND, OR and NOT gates, often from a small dataset of just two or four datapoints. You can even estimate this by hand to see backpropagation over a single layer network in action. It was later discovered that eeeper networks, in terms of number of hidden layers, could compute more complicated gates like NAND or NOR gates.</p> <figure> <img src="/assets/img/logicgate.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="sgd"> Illustration of the problem setting of simulating an OR, AND and XOR gate. The notation A, B would correspond to the input to the neural network model, the truth table would be the supplied dataset, and the challenge was for the model to learn parameters that given those inputs could replicate the output resembling a logic gate. Single layer networks could learn AND, and OR gates, but the XOR could only be simulated by networks with more than one hidden layer. Illustration 'borrowed' from https://adrian-pardo.github.io/perceptron/</figcaption> </figure> <p>Nonetheless, whether this entails that neural networks can emulate inference in propositional logic is debatable. For example, see experiments and analysis done at the paper by <a href="https://arxiv.org/pdf/2205.11502.pdf">Honghua et. al, “On the Paradox of Learning to Reason”</a>. TL;DR, neural networks, no matter how deep they are, can’t generalize to out of distribution test data in a simple propositional logic task of reasoning.</p> <h2 id="do-humans-actually-perform-forward-and-backward-reasoning">Do humans actually perform forward and backward reasoning?</h2> <p>I think it depends on the context. For proving mathematical theorems, humans arguably perform a reasoning procedure which is represented by implementations of forward and backward reasoning. In other contexts, such as finding the closest emergency exit during a natural disaster, or driving, there is another form of (fast) thinking which is unlikely captured by forward or backward reasoning. This idea is extracted from Daniel Kahneman and Toversky’s book on <a href="https://www.amazon.co.uk/Thinking-Fast-Slow-Daniel-Kahneman/dp/0141033576">Thinking, Fast and Slow</a>. The brain is indeed amazing on being able to switch modes of thinking, such as between intuition and reasoning.</p> <h2 id="will-brain-research-lead-to-engineering-wonders-that-far-surpass-the-brain">Will brain research lead to engineering wonders that far surpass the brain?</h2> <p>A member of the audience appealed to the bird-airplane thought experiment: will something similar happen to the brain-computer relationship, i.e., we can design an algorithm ran on a computer that is devoid of the flaws of the brain, such as the brain’s limited lifespan, biological constraints, and obtain a tool that is far more powerful than the brain itself. This is analogous to how an airplane, despite having its architecture inspired by the phenotype of the bird, is many scales more efficient at flying and carrying cargo than any existing bird.</p> <p>First, I think the analogies are flawed at its core if we want to discuss on the topic of intelligence. While the airplane succeeds at a particular task of flying over long distances, it is devoid of intelligence and is dependant on the human flying it. In some regard, intelligence perhaps is <em>not just about solving a task</em>. Without a human operator, an airplane that is about to crash, or goes low on fuel, has absolutely no way to adapt to changes, and formulate a plan to get out of its predicament by itself. Birds, on the other hand, can engage in simple problem solving concerning survival, reproduction and caring for the young, all while adapting to circumstantial events in their environment. One striking example that is often cited by <a href="https://arxiv.org/pdf/2004.09044.pdf">Prof. Song Zhu-Chun</a> is of a crow that can take advantage of incoming cars to <a href="https://www.youtube.com/watch?v=NenEdSuL7QU">break nut shells</a> and eat them.</p> <p>However, once we have grasped the dark matter of intelligence in a model, then I think scaling it up will lead to wonders that we can now only envision: supercomputers able to <a href="http://www.wadjeteyegames.com/games/technobabylon/">manage cities</a>, accelerate scientific discovery such as in the biological sciences and take us to Mars.</p> <h2 id="what-is-hardcoded-in-the-brain">What is hardcoded in the brain?</h2> <p>I think humans’ capability to think of math is hardcoded, and this is the main distinction between us and non-human animals. By math, I don’t just mean counting, but rather have the fundamental principles of arithmetic encoded in our mind. Moreover, I believe this underlies universal grammar.</p> <h1 id="conclusion">Conclusion</h1> <p>Hope you enjoyed the summer school, and please leave any comment, feedback or points for discussion! Happy learning!</p>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="research"/><summary type="html"><![CDATA[My thoughts, reviews and comments on the talks given during the virtual Neurosymbolic Summer School organized by IBM research scientist Asim Munawar.]]></summary></entry><entry><title type="html">El ritmo del progreso de Inteligencia Artificial: una experiencia personal</title><link href="https://awxlong.github.io/blog/2023/ritmo-ia/" rel="alternate" type="text/html" title="El ritmo del progreso de Inteligencia Artificial: una experiencia personal"/><published>2023-08-26T13:42:00+00:00</published><updated>2023-08-26T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/ritmo-ia</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/ritmo-ia/"><![CDATA[<h1 id="en-resumen">En resumen:</h1> <ul> <li>Si te interesa, recomiendo empezar los más pronto posible a estudiar sobre aprendizaje profundo y tener una mente abierta que cualquier recurso en línea que accedas solamente está rascando la punta del iceberg sobre inteligencia artificial.</li> <li>No te sientas abrumado, el aprendizaje toma un tiempo y lo importante es que disfrutes lo que aprendas. Estate seguro de que la mayoría de conocimiento siguen principios comunes.</li> <li>Comparto un templete originalmente dibujado por mi profesor, Antonio Vergari, la cual te puede apoyar en tus estudios de modelos probabilísticos en tu vida colegial, universitaria, y, espero también, profesional.</li> </ul> <h1 id="aprendizaje-supervisado">Aprendizaje supervisado</h1> <p>Hace 3 años, empezando desde abril, 2020, audité un curso en línea que me introdujo a Aprendizaje Profundo (Deep Learning) organizado por el Prof. Andrew Ng y su equipo en Coursera. Fui introducido a varias arquitecturas de redes artificiales profundas, como las redes de convolución, redes recurrentes para resolver problemas con tablas de datos, con imágenes en la visión computacional o con una secuencia de textos en el ámbito de procesamiento de lenguaje natural respectivamente. Un principio omnipresente en la optimización de estas arquitecturas es el descenso estocástico de gradiente (DEG), ilustrado en la <a href="#sgd">Figura 1</a>. El DEG es usado para encontrar los parámetros óptimos que minimizan el error en las predicciones de las redes neuronales.</p> <figure> <img src="/assets/img/sgd.png" alt="Lo siento. Un error inesperado ocurrió y la imagen no puede cargar." width="100%" height="auto"/> <figcaption id="sgd"> Figura 1: Mis notas para visualizar el descenso de gradiente $$w^{nueva}=w^{vieja}-\eta \nabla_w L(w^{vieja})$$, el cual es un proceso iterativo a través de lote de datos, a la vez a través de épocas. <d-cite key="ergin_2014_chapter"></d-cite> </figcaption> </figure> <p>El denominador común de todas estas arquitecturas es que todas pertenecen al paradigma de aprendizaje “supervisado”. La atención a esta área es en gran parte catalizada por el éxito de una red profunda convolucional llamada AlexNet que había obtenido en 2012 la precisión de predicción más alta en una competencia de reconocimiento de objetos llamada ImageNet.</p> <figure> <img src="/assets/img/imgnet.png" alt="Lo siento. Un error inesperado ocurrió y la imagen no puede cargar." width="100%" height="auto"/> <figcaption id="imgnet"> Extracto de ImageNet. AlexNet habría obtenido alta precisión de categorización de objetos reconocidos. Imagen extraída de https://www.tensorflow.org/datasets/catalog/imagenet_v2 <d-cite key="ergin_2014_chapter"></d-cite> </figcaption> </figure> <h1 id="modelos-generativos">Modelos generativos</h1> <p>Debido a que el curso en línea de 5 partes revolvía sobre este tema, desde ese momento, pensé en la idea errónea de que aprendizaje “supervisado” era todo lo que aprendizaje profundo se trataba. A medida que los años pasan, el panorama de IA ha cambiado de manera muy rápida, y aunque aprendizaje supervisado sigue siendo dominante, este está enfrentando competencia de otras áreas. Hoy en día, si quisiera recomendar a un colega a cómo introducirse a aprendizaje profundo, definitivamente le recomendaría tomar cursos en línea como en Coursera, Udemy o YouTube, así como leer libros sobre aprendizaje mecánico como un prerrequisito. Sin embargo, quisiera agregar que se prepare con una mente abierta para aprender nuevos temas como aprendizaje no supervisado, afinamiento de modelos pre-entrenados, o modelos generativos, temas que en años recientes están readquiriendo atención.</p> <h1 id="templete-común-de-modelos-de-aprendizaje-mecánico">Templete común de modelos de aprendizaje mecánico</h1> <p>Con esta afluencia torrencial de conocimiento, puede sentirse abrumador lo que hay que aprender. Algo que me ha ayudado bastante es poder encontrar los principios comunes detrás de la mayoría de estos modelos y métodos. O sea, un templete común de cual puedes instanciar o acomodar varias arquitecturas con el fin de entender cómo funcionan. Espero que el siguiente templete (<a href="#tetra">Figura 2</a>) te apoye en tus estudios de modelos de aprendizaje mecánico✌️, en tu jornada de aprender sobre modelos probabilísticos y redes neuronales:</p> <figure> <img src="/assets/img/probabilistic-modelling.png" alt="Lo siento. Un error inesperado ocurrió y la imagen no puede cargar." width="100%" height="auto"/> <figcaption id="tetra"> Figura 2: Diagrama adaptado de esquema originalmente dibujado por el Dr. Antonio Vergari (por favor síganlo en X-Twitter @tetraduzione) <d-cite key="ergin_2014_chapter"></d-cite> </figcaption> </figure> <p>Por ejemplo, si estás aprendiendo sobre regresión lineal, entonces estás trabajando con X que consiste en dato tabular de columnas de variables continuas o discretas, mientras que y sería una columna de datos continuas. Estos datos tal vez pasen por una etapa de preprocesamiento en la cual limpies entradas nulas o vacías. El modelo regresión lineal es de la forma \(f(X;w)=w^T X^n\), un producto punto entre una fila de X y un vector de pesas w. Este modelo forma parte de una función de pérdida, en este caso el error cuadrático medio (ECM). Dentro del bucle interior de entrenamiento, el ECM es minimizado usando optimizadores que realizan el descenso estocástico de gradiente, con el fin de encontrar los parámetros óptimos del modelo f. En un bucle externo, el modelo f pasa por una etapa de validación y prueba para chequear varios problemas, por ejemplo si se ha subajustado o sobreajustado, y encontrar los hiperparámetros óptimos para minimizar la pérdida. Lo hermoso de este templete es que una diversidad amplias de otros modelos pueden ser instanciados de este, por ejemplo, una red neuronal sigue este esquema, con la diferencia de que en vez de un modelo f lineal, lo reemplazas con un modelo de capas neuronales. Los modelos generativos, los que están detrás de ChatGPT, también pueden ser entendidos en referencia a este templete: en vez de X ser una tabla de datos, esta es una secuencia de palabras, mientras que y sería X (por eso se llaman modelos generativos, porque aprenden a generar la base de datos suministrada), y la función de error usada recibe un tratamiento probabilístico.</p> <h1 id="cómo-será-el-panorama-de-inteligencia-artificial-en-el-futuro">¿Cómo será el panorama de inteligencia artificial en el futuro?</h1> <p>Si tuviese una opinión ahora, esta podría cambiar en 2 o 3 meses, y así de rápido es como siento el progreso de IA es. Sin embargo, aunque el ritmo de progreso es impresionantemente rápido, la mayoría de modelos probabilísticos siguen principios comunes ilustrados en la Figura 2. Espero que esto te sirva en tus estudios del aprendizaje profundo.</p> <p>¡Suerte en tus estudios!</p> <p>Favor compartir preguntas, comentarios y/o retroalimentación. Te agradecería bastante.</p>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="class-notes"/><summary type="html"><![CDATA[notas y consejos útiles para quienes estén interesados en estudiar sobre modelos de aprendizaje mecánico.]]></summary></entry></feed>