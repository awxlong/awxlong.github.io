<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://awxlong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://awxlong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-22T18:53:16+00:00</updated><id>https://awxlong.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">phenotype = (genotype + environment) * consciousness. Reflections on The consilience of knowledge by Edward O. Wilson</title><link href="https://awxlong.github.io/blog/2024/consilience/" rel="alternate" type="text/html" title="phenotype = (genotype + environment) * consciousness. Reflections on The consilience of knowledge by Edward O. Wilson"/><published>2024-06-22T08:42:00+00:00</published><updated>2024-06-22T08:42:00+00:00</updated><id>https://awxlong.github.io/blog/2024/consilience</id><content type="html" xml:base="https://awxlong.github.io/blog/2024/consilience/"><![CDATA[<h1 id="why-consilience"><strong>Why Consilience?</strong></h1> <p>The <strong>consilience</strong> of knowledge essentially means finding the principles which underlying two or more distinct branches of knowledge. People who subscribe to the idea that knowledge, in its many shapes and forms, is intimately connected, can be referred to as “consilientists”. We are often in awe by questions such as “what’s the fundamental question?” to ask in particularly field of knowledge such as math, or artificial intelligence. The unknowns behind such fundamental questions, along with their answers, would shed insight into how seemingly disparate fields like AI and, say, cuisine, are deep down interconnected. The intrisic value of asking fundamental questions, and gauging principles of knowledge rather than superficial forms of knowledge, is that while consilientists can’t know everything, they can <em>know the things that help them understand most things</em>.</p> <p>Consilientists are lateral thinkers, meaning that they like to focus on the “big picture”, as in how a piece of knowledge fits with other knowledge, instead of compromising oneself to understanding thoroughly this piece of knowledge. To achieve consilience, it is required to know something about everything, and find a common principle that could explain most things. It is not being a polymath, i.e., knowing everything. It is being open to several ideas, and that ideas that seemingly contradict each other, may actually complement once we think deeply enough about them. For example, the empiricist vs. nativist debate in cognitive science (https://plato.stanford.edu/entries/innateness-cognition/) . To me, while empiricism and nativism are advertised as opposing ideas, in the eyes of a consilient researcher, they’re complementary. This is because the bottom-up epistemological approach of empiricists is intricately tied to the top-down perception of the world advocated by nativists, as depicted below.</p> <figure> <img src="/assets/img/induction_deduction.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="bottleneck">A depiction of inter-relation between induction and deduction. One can't process data without an a-priori model (theory-landeness). The model is data-dependent, and is part of an active loop of information exchange. </figcaption> </figure> <p>Another seemingly dichotomy is that of questions and answers. Problems are, in essence, solutions. This is because we can only ask questions for which we can have the answer, or we ask the questions based on the answer we can obtain. Consilientists questions can be of the form:</p> <ul> <li> <p>Are two seemingly different ideas just two faces of the same coin?</p> </li> <li> <p>Are two ideas that are often pitted against each other complementary to each other, rather than contradictory?</p> </li> </ul> <p>There are different ways to achieve consilience, i.e., to search for an universal principle, or a theory of everything.</p> <p>Prof. Edward O. Wilson mainly advocates for common principles through an empiricist/reductionist, excluding approaches such as rationalism or deduction. In the literature, empiricists and rationalists are commonly pitted against each other, on the assumption that acceptance of empiricism negates rationalism. But as argued in point 2 above, to my opinion, they not only are not against each other, they complement each other. Empiricist can explain how our mind can learn from external data, but can’t explain how we reason about math, simply because math doesn’t exist in the real world. Nativists can answer how we reason about math by arguing it is endowed by our genes after some random mutation in that marked the beginning of the Homo sapiens species, but this doesn’t mean that they can reject the relevance of learning from environmental cues. I’m personally critical of reductionism or empiricism (for a better take please read David Deutsch’s The Beginning of Infinity). The problem with reductionism is its lack of explanatory power for meaningful phenomena like universal cultural values, history of the emergence of nation-states, or love. At its core, reductionism aims at proposing as a theory of everything the interplay of molecules and fundamental forces, which equifinal outcome make them meaningless to explain any phenomena. The problem with empiricism is that one often shapes hypothesis and instruments of measurement based on the evidence one wants to collect.</p> <p>In the book by Edward Wilson, he discussed the fundaments which tie the biological and the social sciences, of which can be succintly summarized as:</p> \[phenotype = genotype + environment\] <ul> <li>phenotype encapsulates the physical, behavioral and emotional trait of an individual organism.</li> <li>gene refers to the selfish gene, or mutations, that are inherited from progenitors. Genetics is also what predisposes us to fight against each other, and our consciousness can either amplify this predisposition for war or tone it down.</li> <li>environment refers to the geographical constraints in which an organism lives and influences how a gene expresses, and thus how an organism develops. Epigenetic rules or epigenesis is heredity and environmental factors</li> </ul> <p>The beauty of such equation rests in the infinite amount of knowledge which hides behind such abstract simplicity. Phenotype is regarded as an abstract term with which we describe the human or humans’ conditions, the subject of study of the social sciences.</p> <p>I have some criticism of this equation, in that it doesn’t account for the uniqueness of human beings. The equation is ‘universal’ to all living beings, such as animals. One phenotypic trait is intelligence, with which from the equation we can argue that it is intimately endowed by the genes and environment. However, I don’t believe that animals have the same competence as us humans, and instead propose the following equation, which just slightly modifies the equation above:</p> <h1 id="phenotype--genotype--environment--consciousness"><strong>\(phenotype = (genotype + environment) * consciousness\)</strong></h1> <p>Marvelous! Consciousness, which itself is something which we don’t understand nor define, can explain the uniqueness of us human beings: we have more “consciousness” than non-human animals. The consciousness in this equation is an umbrella term to encapsulate many other terms: creativity, hypothesis making, mathematical wisdom, emotions, qualia, among others.</p> <p>I personally add consciousness as an additional component to epigenetic rules which influences our phenotype. I placed it in the right hand side of the equation, instead of the left hand side, on the assumption that it is not a physical property. This is debatable nonetheless. Consciousness refers to our mental capacity (call it intelligence if you want) which can shake off the shackles of our genes and environment. Consciousness, or the capacity to stay conscious, is the central drive of humans to domesticate other species, nature itself and, through science and technology, pursue a meaningful life beyond what the selfish gene and environment dictate. Unlike animals, with a lower degree of consciousness (or hunekers as Douglas Hofstadter would call), then non-human animals are only able to act according to their epigenetic rules. Consciousness allowed us humans to alter drastically our environment, to the point of compromising our long-term survival. Soon, it would also allow us to modify our very that predisposes to a large extent our phenotype.</p> <p>The equation essentially states the human condition (the phenotype) is the result of the human’s genome, environment in which he developed, everything moderated by consciousness. You may have seen the equation $phenotype = genotype + environment$. By adding the $\times consciousness$ portion, we illustrate the difference between us and the non-human animals. Namely, the condition of non-human animals, which may have a low-degree of consciousness, are mostly determined by their genes and environment. We, however, can <em>influence both our genes and environment</em> thanks to our consciousness another thought experiment is that the consciousness variable could be universal, as in, consciousness is the same “principle” for all humans, but it is personalized because of each human’s unique gene and environment.</p> <p>Another interesting thought experiment derived from the equation is that the consciousness variable is perhaps universal, rather than localized to each individual. This entails the existence of a “collective consciousness”, and each individual is a personalization of such collective consciosness dependent on the particular characteristics of our randomly delegated genes and allocated environment.</p> <h2 id="some-comments-on-the-comparisons-between-humans-and-non-humans">Some comments on the comparisons between humans and non-humans</h2> <p>Another thread of thought spun from the equation delineates the human condition with the non-human condition. Namely, we are <em>fundamentally</em> distinct to non-human animals, despite several human beings stating otherwise. We argue that animals don’t display intelligent behavior in the sense of intelligence meant by ‘human intelligence’.</p> <p>Perhaps we perceive intelligence in non-human animals because a particular trait of our consciousness (these mysterious mental machinations) is to story-tell, leading us to perceive higher intelligence in other non-human animals when there isn’t, because non-human animals are, in that regards, slave to their genes and environment.</p> <p>A dolphin or dog that can seemingly count, a bonobo that can solve puzzles, a herd of elephants that supposedly organize a funeral or squid that can squirt to anger human spectators is perhaps not evidence of cognition or consciousness in non-human animals, at least at the same degree of their human counterparts, but rather behavior that’s pre encoded in their genes. What is impressive, in my opinion, is our brains’ capability to craft stories, stories which are causal interpretations of these natural events:</p> <div class="embed-container"> <iframe width="560" height="315" src="https://www.youtube.com/embed/" frameborder="0" allowfullscreen=""></iframe> </div> <p>dolphin ‘counting’</p> <div class="embed-container"> <iframe width="560" height="315" src="https://www.youtube.com/embed/" frameborder="0" allowfullscreen=""></iframe> </div> <p>octopus ‘opening jar’</p> <div class="embed-container"> <iframe width="560" height="315" src="https://www.youtube.com/embed/" frameborder="0" allowfullscreen=""></iframe> </div> <p>elephants ‘mourning’</p>]]></content><author><name></name></author><category term="blog-post"/><summary type="html"><![CDATA[my afterthoughts on The consilience of knowledge, a book by Edward O. Wilson]]></summary></entry><entry><title type="html">Reflections on the Hacking the Human Vasculature Kaggle Competition</title><link href="https://awxlong.github.io/blog/2024/hack-vasculature/" rel="alternate" type="text/html" title="Reflections on the Hacking the Human Vasculature Kaggle Competition"/><published>2024-03-09T08:42:00+00:00</published><updated>2024-03-09T08:42:00+00:00</updated><id>https://awxlong.github.io/blog/2024/hack-vasculature</id><content type="html" xml:base="https://awxlong.github.io/blog/2024/hack-vasculature/"><![CDATA[<p>George Tang and I participated in the SenNet + HOA Hacking the Human Vasculature <a href="https://www.kaggle.com/competitions/blood-vessel-segmentation">Kaggle Competition</a>. We didn’t manage to submit good results prior to the competition deadline 😅. However, after the competition deadline, we managed to obtain a competitive segmentation DICE score that could have earned us ~10th place in the competition.</p> <h1 id="training-the-model">Training the model</h1> <ul> <li>My training notebook is at: https://www.kaggle.com/code/awxlong/hack-vasculature-transfer-learning <ul> <li>It consists of adapting the nnUnet proposed in https://github.com/MIC-DKFZ/nnUNet loaded with pretrained weights on hepatic vessel segmentation downloaded from https://zenodo.org/records/3734294/files/Task008_HepaticVessel.zip?download=1</li> <li>Finetuning the nnUnet architecture to the provided dataset in the competition for 15 epochs</li> </ul> </li> <li>My inference notebook is at: https://www.kaggle.com/code/awxlong/hack-vasculature-transfer-learning-inference <ul> <li>It consists of loading the weights trained above and inferring over the test dataset.</li> <li>We adopt <a href="https://www.kaggle.com/competitions/blood-vessel-segmentation/discussion/475074">post-processing steps</a> by 3rd place winner (shout-out to ForcewithMe) which consists of tuning a threshold for binarizing the segmentation mask which helped us boost the performance of our model up to a competitive 0.67 Dice score. For reasons unknown, without their post-processing step, my segmentation DICE score was stuck at ~0.001.</li> </ul> </li> </ul> <h1 id="reflection">Reflection</h1> <ul> <li>Prior to transfer learning, I’ve attempted finetuning the foundational model MedSAM adapted from https://github.com/bowang-lab/MedSAM. My validation DICE score was stuck at ~0.18, and I argue this is because foundational models are harder to fine-tune as well as the this task was “syntactically” different to what the foundational model was previously trained on. Blood vessels are thinner, and resulting segmentation masks are more sparse compared to the datasets that MedSAM was trained on, which were most likely images of larger organs. ‘Semantically’ however, MedSAM would have been very appropiate for this task because technically it had ‘medical’ knowledge on what organs are.</li> <li>My personal advice is that if you opt for transfer learning, focus on the ‘syntax’ of the task some model was trained on. In my case, I chose nnUnet’s hepatic blood vessel segmentation because I noticed the masks it was previously trained on are ‘sparse’ and ‘thin’. I wouldn’t prefer, for instance, some segmentation model pretrained on ‘ocular blood vessels’ because even though they are ‘semantically’ the same task, syntactically, their segmentation masks are thin, but they weren’t sparse.</li> </ul> <h1 id="presentation">Presentation</h1> <p>I did a presentation for the Nexus Lab Sympossium on March 9th, with slides here attached below:</p> <style>.pdf-embed-wrap-574fc054-1358-4b9b-b112-b7cd693cba24{display:flex;flex-direction:column;width:100%;height:650px}.pdf-embed-container-574fc054-1358-4b9b-b112-b7cd693cba24{height:100%}.pdf-link-574fc054-1358-4b9b-b112-b7cd693cba24{background-color:white;text-align:center;border-style:solid}.pdf-embed-container-574fc054-1358-4b9b-b112-b7cd693cba24 iframe{width:100%;height:100%}</style> <div class="pdf-embed-wrap-574fc054-1358-4b9b-b112-b7cd693cba24"> <div class="pdf-link-574fc054-1358-4b9b-b112-b7cd693cba24"> <a href="/assets/pdf/hacking-the-human-vasculature.pdf" target="_blank">View PDF</a> </div> <div class="pdf-embed-container-574fc054-1358-4b9b-b112-b7cd693cba24"> <iframe src="/assets/pdf/hacking-the-human-vasculature.pdf" frameborder="0" allowfullscreen=""></iframe> </div> </div>]]></content><author><name></name></author><category term="blog-post"/><summary type="html"><![CDATA[transfer learning from nnUnet's hepatic vessel segmentation to renal vessel segmentation]]></summary></entry><entry><title type="html">A probabilistic circuit for imputing missing tabular data</title><link href="https://awxlong.github.io/blog/2023/pc-imputation/" rel="alternate" type="text/html" title="A probabilistic circuit for imputing missing tabular data"/><published>2023-12-22T08:42:00+00:00</published><updated>2023-12-22T08:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/pc-imputation</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/pc-imputation/"><![CDATA[<h1 id="spns-for-imputing-data">SPNs for imputing data</h1> <p>The following is a Jupyter notebook ran on Google Colab using Kaggle’s Titanic dataset to illustrate a practical use case of a probabilistic circuit, a sum-product network, from <a href="https://github.com/deeprob-org/deeprob-kit">deepprob-kit</a>: to impute missing tabular data.</p> <p>This notebook is forked from a Kaggle <a href="https://www.kaggle.com/code/ttminh27/using-autoencoder-to-impute-missing-data">tutorial</a> on using a Tensorflow’s autoencoder to impute missing data. Standard autoencoders can’t:</p> <ul> <li>custom fill-in missing data</li> <li>flexibly incorporate domain knowledge like what distribution is best used to model a feature</li> <li>Furthermore, p;robabilistic circuits can tractably compute missing data through maximum a posteriori estimation.</li> </ul> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/using_pc_to_impute_missing_data.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="class-notes"/><category term="food-for-thought"/><summary type="html"><![CDATA[using a sum-product network to flexibly impute missing data in Kaggle's Titanic dataset]]></summary></entry><entry><title type="html">On the Turing Test and Large Language Models</title><link href="https://awxlong.github.io/blog/2023/turing-test/" rel="alternate" type="text/html" title="On the Turing Test and Large Language Models"/><published>2023-12-09T09:42:00+00:00</published><updated>2023-12-09T09:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/turing-test</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/turing-test/"><![CDATA[<h1 id="on-benchmarks-of-human-intelligence">On benchmarks of Human intelligence</h1> <p>There is a plethora of claims surrounding the emergence of actual or surpasssing Human intelligence by Large Language Models. One, by one, these contextualized autocomplete models are given the labels of having <a href="https://arxiv.org/abs/2305.03731v2">working memory</a>, <a href="https://analyticsindiamag.com/text-is-a-projection-of-the-world-says-openais-sutskever/">compresses</a> a <a href="https://www.linkedin.com/posts/armand-ruiz_llms-do-more-than-predict-the-next-word-activity-7134512576318623744-7BlG?utm_source=share&amp;utm_medium=member_desktop">“world model”</a>, or it signals <a href="https://arxiv.org/abs/2303.12712">sparks of Artificial General Intelligence</a>.</p> <p>There are a list of benchmarks backing up their claims. These benchmarks come with questions-answers pairs, where high performance signifies that a model is able to simulate a faculty of Human intelligence like language.</p> <p>However, most if not all benchmarks are tailored for what LLMs can do, even though they’re supposed to be designed without having LLMs in mind. A benchmark that tests a faculty of Human Intelligence like “reasoning” should not only explore define reasoning as . This is in contrast to designing theory-laden benchmarks. No one knows the exact definition of Human capacities like “reasoning”, so the design of a benchmark must have assumptions on what are the definitions of whatever capacity it is measuring. The key argument of the present article is that such assumptions have been constrained to framing every faculty as the prediction of the next token, where token can be the next word, image or audio signal. I don’t even know what faculties like “memory”, “reasoning”, “planning” are. But I do know as many researchers may conclude after years of work on this, that it is <em>not just contextualized, next-state prediction</em>.</p> <p>Because all benchmarks have been designed based on next-token prediction, it is not surprising that a next-token predictor can perform well. Quiet the opposite, it would be surprising if a next-token predictor can’t perform well on an auto-regressive task. However, while Human Intelligence most likely consists of next-token prediction, it would be misleading to suggest it <em>only</em> consists of this process. By that line of thought, LLMs haven’t even passed the Turing Test, at least by its <a href="https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence">original formulation</a>. This is because while it is true that the original objective was for a machine to imitate a human and fool an interrogator, a feat that LLMs can already achieve by producing text that is indistinguishable from Human-produced text, it is often ignored that the Turing Test is executed via a <em>physical</em> exchange of written letters. I don’t think this is a trivial matter that can be glanced over, as it is very important to note that LLMs can not write, since they are not instantiated in spacetime, nor have a body with which they have to face the challenge of sensorimotor control in a continuous feedback loop.</p> <p>A benchmark of a faculty of Intelligence shouldn’t be designed with only one theory of what Intelligence is, i.e., prediction of next-token. This kind of practice is akin to framing questions based on the answer one expects, instead of asking open-ended questions for the sake of curiosity. As such, if I may borrow the sensationalist attitudes of some media personalities, I just want to say “all these benchmarks are wrong, and none of them are useful<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> to understand Intelligence”.</p> <h2 id="prediction-error-minimization-is-a-strong-hypothesis-not-overarching">Prediction Error Minimization is a strong hypothesis, not overarching</h2> <p>I do want to clarify: I think <a href="https://www.fil.ion.ucl.ac.uk/~karl/Whatever%20next.pdf">prediction error minimization (PEM)</a> as an integral part of Human intelligence is a resilient hypothesis. Active inference, for instance, states that our brains are constantly predicting the source of the signals we perceive, and consciousness is the result when we infer that the cause of some signals in the environment is caused by ourselves. PEM also tries to explain other phenomena like dreaming, arguing that it is a form of “reverse-learning” that consists of a process of flushing irrelevant memories to while strengthening important synaptic connections, altogether allocate mental resources to make better predictions afterwards. Another important strength of the PEM hypothesis is that the abstract process of “learning” is grounded to physical mechanisms like chemico-modulated, neuronal activations. I believe these are very strong hypothesis that can withstand harsh criticism. I simply think PEM is not enough to explain the entirety of the Human condition.</p> <figure> <img src="/assets/img/llm_shouting.jpg" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="llm"> Image generated by Dall-E powered MS Bing Image Creator given the prompt "a large language model shouting" </figcaption> </figure> <p>My main criticism of PEM is that while it is designed to explain Human intelligence, it seems that it makes no distinction between non-Human animal intelligence and Human intelligence. The non-Human animal brain also has neurons firing, and the PEM may well apply to non-Human animals and Humans alike. However, is that enough? Are we Humans really just a slightly more intelligent animal. I argue it is absolutely not. Non-human animals can’t reflect on the mysteries of the mechanisms of the universe, perform scientific experiments, do creative thinking, believe in currency exchange and laundering, or any activity beyond the shackles of genetic predisposition, among others. And furthermore, I don’t think PEM explains neither of the aforementioned capacities.</p> <p>I just hope sensationalist claims in media stay as they are: “sensationalist”. They’re only meant to spark sensation, not actually to determine the course of public-policy making (at least not now), instilling fear of Humanity’s imminent destruction or even <a href="https://www.youtube.com/watch?v=lfXxzAVtdpU">existential crisis</a> on what humanity is. What I’m most worried about is that these sensationalist claims limit our own perspective on how amazing Human intelligence is, and that it is beyond PEM.</p> <p>As previously said, I don’t know what Human intelligence is in its entirety, but I’m confident it at least includes in addition to learning: reasoning, sensorimotor control, creativity/imagination, curiosity, consciousness, intrapersonal understanding, interpersonal understanding, among others.</p> <h1 id="proposing-an-extended-turing-test">Proposing an Extended Turing Test</h1> <p>To share some thoughts on a more comprehensive Turing Test, I do want to clarify that it is designed to measure Intelligence, not accuracy of next-predicted tokens. As such, a preliminary concept to convey concerns <a href="https://hrstraub.ch/en/the-theory-of-the-three-worlds-penrose/">Roger Penrose’s 3 worlds</a>: 1) Platonic, 2) Physical and 3) Subjective. Almost all existing benchmarks measuring any faculty of Human Intelligence, such as to produce language, focus on evaluating in the Platonic space, the space of abstract ideas. There do exist benchmarks measuring faculties in the physical space, such as in robotics, with examples including (cute) <a href="https://www.youtube.com/watch?v=RbyQcCT6890">robots playing football</a> against one another <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, however, these benchmarks then don’t measure faculties in Platonic space.</p> <p>I will also focus on the breadth of tasks, instead of the depth. This means the Extended Turing Test (better named Extended Turing Benchmark) has as many diverse tasks as possible, instead of having a single task modality, such as predicting the next text-token, but it is semantically diverse within this domain (predict the next token in English, in Spanish, in Chinese, among others). Following the main idea of this article, I won’t be focusing on next-token prediction either, though it is an integral part of it. I also note this is not meant to be the final Extended Turing Benchmark, and feedback is most welcomed.</p> <table> <thead> <tr> <th>Task</th> <th>Challenging aspects</th> </tr> </thead> <tbody> <tr> <td>Solve a mathematical conjecture, like Goldbach’s Conjecture</td> <td>The answer is not given in the training set</td> </tr> <tr> <td>Achieve a United Nation’s Sustainable Development Goal</td> <td>Requires interplay of Platonic and Physical space</td> </tr> <tr> <td>Find a cure for a disease of global concern</td> <td>Requires physical experimentation and searching over hypothesis space</td> </tr> <tr> <td>Propose a plan for a successful start-up</td> <td>Same as above</td> </tr> <tr> <td>Break a world record</td> <td>Same as above</td> </tr> <tr> <td>Ask questions</td> <td>Same as above</td> </tr> <tr> <td>Solve a conflict of global concern</td> <td>Requires interplay of Platonic, Physical and Subjective spaces</td> </tr> <tr> <td>Open-ended debate on moral dilemmas, such as the Trolley Problem</td> <td>No correct answer, requires proficiency in Subjective space</td> </tr> </tbody> </table> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>I don’t actually mean all benchmarks are wrong. Some benchmarks have indeed yielded insight into what constitutes Human intelligence, and they are good attempts at quantifying faculties of intelligence like visual reasoning, and analogy-making. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>The match caused some sensation despite it is not played against humans, further exacerbating a point made earlier that sensorimotor control, in addition to PEM, is an integral part of Intelligence. As such, I can’t understand how and why autoregressive models are advertised as the key to Artificial General Intelligence. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><category term="creative-work"/><summary type="html"><![CDATA[I argue we Humans are more than next-state prediction machines.]]></summary></entry><entry><title type="html">Concept-Bottleneck Modelling for Interpretable Melanoma Classification</title><link href="https://awxlong.github.io/blog/2023/concept-melanoma/" rel="alternate" type="text/html" title="Concept-Bottleneck Modelling for Interpretable Melanoma Classification"/><published>2023-12-04T16:42:00+00:00</published><updated>2023-12-04T16:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/concept-melanoma</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/concept-melanoma/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A research project adopting the concept-bottleneck modelling technique for interpretable melanoma classification.]]></summary></entry><entry><title type="html">Review of the paper Learning biophysical determinants of cell fate with deep neural networks</title><link href="https://awxlong.github.io/blog/2023/biophysical/" rel="alternate" type="text/html" title="Review of the paper Learning biophysical determinants of cell fate with deep neural networks"/><published>2023-11-02T19:42:00+00:00</published><updated>2023-11-02T19:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/biophysical</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/biophysical/"><![CDATA[<h1 id="brief-summary">Brief summary</h1> <p>The paper by <d-cite key="dsa_2023_prediction"></d-cite> leverages deep learning architectures to solve a pentanary classification task given either a cell’s tabular features or images. The five independent classes are one healthy control and four subtypes of Parkinson’s Disease: familial proteinopathy (SNCA), environmental proteinopathy (\(\alpha\)-Syn oligomer), and two subtypes characterized by different mitochondria dysfunction pathways. These pathologies were chemically induced on stem cells. Fifty-six phenotypical features of them were extracted automatically and recorded as tabular data, along with images of the cells extracted via microscopy. Both data modalities were labeled with one of the five classes.</p> <p>The research team trained separately a dense feedforward neural network (DNN) to classify on the tabular data, as well as a convolutional neural network (CNN) to classify on image data. The test classification accuracy achieved by the DNN reached around 83%, while the CNN 95%.</p> <figure> <img src="/assets/img/parkinson.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="bottleneck">Two separate models are trained on different datasets on the same task of Parkinson subtype classification. Figure extracted from the original research article at https://www.nature.com/articles/s42256-023-00702-9</figcaption> </figure> <h1 id="my-comments-and-future-research-directions">My comments and future research directions</h1> <p>Generally, in the deep learning literature, it is acknowledged that the usage of DNNs comes at the expense of poor explainability. Despite achieving high classification accuracy, these models are black-boxes. Nonetheless, there are ways to identify what are the features that the neural networks pay the most attention when deciding on a classification label, mainly by looking at its last layer’s activation and tracing back to the input space which input feature is associated to it. In CNNs, the technique employed by the research team is called the ShAP (SHapley Additive exPlanations) method.</p> <p>The authors managed to identify in both the DNN and CNN that the mitochondria, lysosome and the interaction of both features mainly contributed to the classification decisions of both models.</p> <p>One future research direction concerns whether integrating both data sources can improve performance and yield explainability, because the original work trains separate models, trained on different datasets.</p> <p>One source of inspiration is from <d-cite key="li_2023_v1t"></d-cite>, where they integrate image data along with a mouse’s behavioral features to predict its neural responses collected from neural recordings. Another source of inspiration is drawn from concept-bottleneck models <d-cite key="koh_2020_concept"></d-cite>. There, a CNN in charge of processing images doesn’t learn to output a classification label, but instead to output features that are relevant to the image. These features, in turn, are annotations of the image stored in tabular:</p> <figure> <img src="/assets/img/bottleneck.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="bottleneck">A depiction of the pipeline of a concept-bottleneck model. The first half outputs a set of concepts given an image, which can be learnt from intricate annotations, or metadata, of the image. The second half outputs a classification label. Figure extracted from the original paper </figcaption> </figure> <p>Altogether, with regards to the work by <d-cite key="dsa_2023_prediction"></d-cite>, one interesting extension to their CNN is to have it not predict a Parkinson subtype, but rather learn to predict the cell’s physiological features stored as tabular data given image input. Subsequently, use the features to train a multi-class regressor using standard softmax to output a classification label. The prospect is that this hybrid model can leverage the high accuracy prediction of the CNN, whilst being explainable thanks to the logistic regressor.</p> <p>As a further improvement, we can use a <a href="https://arxiv.org/abs/2210.11394">Slot Transformer</a> instead of the CNN with the hope of learning a disentangled representation given the image with its annotations. However, the architecture will be more computationally expensive. A pretrained Slot Transformer that already learnt to disentangle CLEVR-Scenes may be more powerful than training it from scratch.</p>]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><summary type="html"><![CDATA[comments on a paper that leverages deep learning to classify epithelium cell fate by observing its live image trajectory.]]></summary></entry><entry><title type="html">Review of the paper A Deep Learning Approach to Antibiotic Discovery</title><link href="https://awxlong.github.io/blog/2023/halicin/" rel="alternate" type="text/html" title="Review of the paper A Deep Learning Approach to Antibiotic Discovery"/><published>2023-11-02T19:42:00+00:00</published><updated>2023-11-02T19:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/halicin</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/halicin/"><![CDATA[<style>.pdf-embed-wrap-2c51cc6d-80da-4b6b-acf1-a6cc660648d0{display:flex;flex-direction:column;width:100%;height:650px}.pdf-embed-container-2c51cc6d-80da-4b6b-acf1-a6cc660648d0{height:100%}.pdf-link-2c51cc6d-80da-4b6b-acf1-a6cc660648d0{background-color:white;text-align:center;border-style:solid}.pdf-embed-container-2c51cc6d-80da-4b6b-acf1-a6cc660648d0 iframe{width:100%;height:100%}</style> <div class="pdf-embed-wrap-2c51cc6d-80da-4b6b-acf1-a6cc660648d0"> <div class="pdf-link-2c51cc6d-80da-4b6b-acf1-a6cc660648d0"> <a href="/assets/pdf/review_antibiotic.pdf" target="_blank">View PDF</a> </div> <div class="pdf-embed-container-2c51cc6d-80da-4b6b-acf1-a6cc660648d0"> <iframe src="/assets/pdf/review_antibiotic.pdf" frameborder="0" allowfullscreen=""></iframe> </div> </div>]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><summary type="html"><![CDATA[comments on the paper "A Deep Learning Approach to Antibiotic Discovery", with a brief step-by-step algorithm how message passsing works in a graph neural network]]></summary></entry><entry><title type="html">Review of the paper Prediction of mechanistic subtypes of Parkinson’s using patient derived stem cell models</title><link href="https://awxlong.github.io/blog/2023/mechanistic-subtypes-parkinson-copy-copy/" rel="alternate" type="text/html" title="Review of the paper Prediction of mechanistic subtypes of Parkinson’s using patient derived stem cell models"/><published>2023-11-02T19:42:00+00:00</published><updated>2023-11-02T19:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/mechanistic-subtypes-parkinson-copy%20copy</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/mechanistic-subtypes-parkinson-copy-copy/"><![CDATA[<h1 id="brief-summary">Brief summary</h1> <p>The paper by <d-cite key="dsa_2023_prediction"></d-cite> leverages deep learning architectures to solve a pentanary classification task given either a cell’s tabular features or images. The five independent classes are one healthy control and four subtypes of Parkinson’s Disease: familial proteinopathy (SNCA), environmental proteinopathy (\(\alpha\)-Syn oligomer), and two subtypes characterized by different mitochondria dysfunction pathways. These pathologies were chemically induced on stem cells. Fifty-six phenotypical features of them were extracted automatically and recorded as tabular data, along with images of the cells extracted via microscopy. Both data modalities were labeled with one of the five classes.</p> <p>The research team trained separately a dense feedforward neural network (DNN) to classify on the tabular data, as well as a convolutional neural network (CNN) to classify on image data. The test classification accuracy achieved by the DNN reached around 83%, while the CNN 95%.</p> <figure> <img src="/assets/img/parkinson.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="bottleneck">Two separate models are trained on different datasets on the same task of Parkinson subtype classification. Figure extracted from the original research article at https://www.nature.com/articles/s42256-023-00702-9</figcaption> </figure> <h1 id="my-comments-and-future-research-directions">My comments and future research directions</h1> <p>Generally, in the deep learning literature, it is acknowledged that the usage of DNNs comes at the expense of poor explainability. Despite achieving high classification accuracy, these models are black-boxes. Nonetheless, there are ways to identify what are the features that the neural networks pay the most attention when deciding on a classification label, mainly by looking at its last layer’s activation and tracing back to the input space which input feature is associated to it. In CNNs, the technique employed by the research team is called the ShAP (SHapley Additive exPlanations) method.</p> <p>The authors managed to identify in both the DNN and CNN that the mitochondria, lysosome and the interaction of both features mainly contributed to the classification decisions of both models.</p> <p>One future research direction I am interested is exploring whether by integrating both data sources can improve performance and yield explainability, because the original work trains separate models, trained on different datasets.</p> <p>One source of inspiration is from <d-cite key="li_2023_v1t"></d-cite>, where they integrate image data along with a mouse’s behavioral features to predict its neural responses collected from neural recordings. Another source of inspiration is drawn from concept-bottleneck models <d-cite key="koh_2020_concept"></d-cite>. There, a CNN in charge of processing images doesn’t learn to output a classification label, but instead to output features that are relevant to the image. These features, in turn, are annotations of the image stored in tabular:</p> <figure> <img src="/assets/img/bottleneck.png" alt="Sorry. Image couldn't load." width="100%" height="auto"/> <figcaption id="bottleneck">A depiction of the pipeline of a concept-bottleneck model. The first half outputs a set of concepts given an image, which can be learnt from intricate annotations, or metadata, of the image. The second half outputs a classification label. Figure extracted from the original paper </figcaption> </figure> <p>Altogether, with regards to the work by <d-cite key="dsa_2023_prediction"></d-cite>, one interesting extension to their CNN is to have it not predict a Parkinson subtype, but rather learn to predict the cell’s physiological features stored as tabular data given image input. Subsequently, use the features to train a multi-class regressor using standard softmax to output a classification label. The prospect is that this hybrid model can leverage the high accuracy prediction of the CNN, whilst being explainable thanks to the logistic regressor.</p> <p>As a further improvement, we can use a <a href="https://arxiv.org/abs/2210.11394">Slot Transformer</a> instead of the CNN with the hope of learning a disentangled representation given the image with its annotations. However, the architecture will be more computationally expensive. A pretrained Slot Transformer that already learnt to disentangle CLEVR-Scenes may be more powerful than training it from scratch.</p>]]></content><author><name>Xuelong An</name></author><category term="blog-post"/><category term="research"/><summary type="html"><![CDATA[comments on a paper that leverages deep learning to classify cells into Parkinson's subtypes]]></summary></entry><entry><title type="html">A mathematical analogy for understanding creativity</title><link href="https://awxlong.github.io/blog/2023/creativity/" rel="alternate" type="text/html" title="A mathematical analogy for understanding creativity"/><published>2023-11-02T13:42:00+00:00</published><updated>2023-11-02T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/creativity</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/creativity/"><![CDATA[<p>Math is really a beautiful framework with which we can analyze several ideas. For instance, beauty can be seen through the lenses of math using symmetry and geometry, and the study of complex systems can be cast through calculus and probability theory.</p> <p>Given this premise, can we use math to understand what creativity is. Here is my attempt:</p> <h1 id="how-can-creativity-be-defined">How can creativity be defined?</h1> <p>Let us run the following thought experiment. Imagine you’re an enthusiastic pursuer of truth. The whole universe is simply an empty space. In this void, in front of you are a some numbers and two operators: + (ordinary addition) and * (ordinary multiplication)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p> <p>Science, or the discovery of knowledge, is analogous to discovering new numbers. You randomly add two numbers, 1 and -1 and discover 0. Then you add 1 to 1 and obtain 2. Repeating this process, you end up discovering all the integers. Furthermore, you can find patterns that lead to theorems among the integers, such as properties of commutativity, transitivity and distributivity.</p> <p>However, from a metaphysical point of view, addition and multiplication over integers is closed in this set, therefore you can’t discover anything beyond the horizon of integers.</p> <h1 id="can-you-discover-anything-else">Can you discover anything else?</h1> <p>Creativity, in this scenario, would be proposing another operator such as division. Having this new operator in your mind’s eye to perceive reality, you can now compute a new class of numbers, the rationals and thus discover the set of all real numbers.</p> <p>In either case, prior and after the discovery of the division operator, the infiniteness of numbers is analogous to the infinity of knowledge.</p> <p>Creativity can also materialize by introducing the square root, and this new operator extends our perceptual horizon to encompass the realm of complex/imaginary numbers.</p> <h1 id="more-analogies">More analogies</h1> <p>A different civilization from another point in space, using a different set of symbols to encode numbers, may make similar discoveries, i.e., numbers obtained following the same principles of computation using + and *. Moreover, they may also discover commutativity, transitivity and distributivity. This analogy represents the universality of knowledge.</p> <p>In the above thought experiment, I’ve ignored letters, phonemes, and words to simplify this overly-idealized world to get my ideas on what is creativity across.</p> <figure> <img src="/assets/img/creativity.jpg" alt="Sorry, an unanticipated error occured and the image can't load." width="100%" height="auto"/> <figcaption id="creativity"> The thumbnail is generate by Dall-E powered MS Bing Image Generator given the prompt "your interpretation of what creativity is". </figcaption> </figure> <p>PS: Demis Hassabis, Deepmind CEO, also comments on another definition of <a href="https://www.youtube.com/watch?v=Gfr50f6ZBvo">creativity, or AGI</a>. This is going beyond statistical interpolation and extrapolation of training data. An AGI model that is creative should be able to give an output to an instruction such as “design a game that has simple rules, but is extremely hard to master, and can be played for hours”.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Perhaps these numbers and operators existed there a priori, maybe they emerged from a Creator, or perhaps they emerged from a singularity. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><summary type="html"><![CDATA[Using numbers to illustrate an intuitive definition of what creativity is]]></summary></entry><entry><title type="html">The many interpretations of terms in artificial intelligence</title><link href="https://awxlong.github.io/blog/2023/many-faces-ML/" rel="alternate" type="text/html" title="The many interpretations of terms in artificial intelligence"/><published>2023-11-01T13:42:00+00:00</published><updated>2023-11-01T13:42:00+00:00</updated><id>https://awxlong.github.io/blog/2023/many-faces-ML</id><content type="html" xml:base="https://awxlong.github.io/blog/2023/many-faces-ML/"><![CDATA[<p>I bumped into this paper on <a href="https://twitter.com/ChristophMolnar">X (formerly Twitter)</a></p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">How do researchers define interpretability and explainability?<br/><br/>An overview:<a href="https://t.co/bNjKv8F44g">https://t.co/bNjKv8F44g</a> <br/><br/>Summary: No consensus<br/><br/>My opinion: It&#39;s already the Wild West, no one will stick to any definition. Most pragmatic is to treat them interchangeably. <a href="https://t.co/mEh6mUuk4C">pic.twitter.com/mEh6mUuk4C</a></p>&mdash; Christoph Molnar (@ChristophMolnar) <a href="https://twitter.com/ChristophMolnar/status/1718921189333045752?ref_src=twsrc%5Etfw">October 30, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>In today’s post, I don’t want to discuss interpretability or explainability, but rather elaborate on the above idea that in the AI literature, different authors express completely different ideas despite using the same words, often words that appear in our daily parlance, yet acquire uncommon interpretations.</p> <p>If you have just set foot into the world of artificial intelligence, here are my thoughts and advices on navigating this exciting, but often messy landscape.</p> <h1 id="terms">Terms</h1> <h4 id="explanation-hypothesis-model"><strong>Explanation, hypothesis, model</strong></h4> <p>When I began studying AI, the word “model” elicited the mental imagery of a 3D scale model of something. If I wanted a “model” of an atom, I could pick some recycled styrofoam balls, stick them together and, bam (pun intended), an atom is synthesized.</p> <p>Nowadays it mainly refers to a equation or function with some parameters $\theta$, where a ‘good’ model is defined as having optimal parameters $\theta$ that can make predictions with low error on the training and (hopefully) unseen, test data, compared to a ‘bad’ model.</p> <p>In rare cases, such as in <a href="https://www.youtube.com/watch?v=NsID1iM8gRw">Prof. Joshua Tenembaum’s talk on Cognition</a>, a (physics-) model can refer a simulator implemented in Unity.</p> <h4 id="knowledge"><strong>Knowledge</strong></h4> <p>This word may cause some confusion to peers with a philosophy or psychology background. At least to me, prior to studying AI, ‘knowledge’ meant pieces of information encoded in natural language, e.g. “Mount Chimborazo is actually higher than Mount Everest” somewhere in the brain. I don’t know how, nor where, nor whether it is encoded in natural language.</p> <p>At least in the early days of AI, first order logic constraints such as <code class="language-plaintext highlighter-rouge">smoker(X) :- edema(X)</code>, read as “it is true that if X is a smoker, then X has edema”, does resemble ‘knowledge’ encoded in a way that seems familiar to us.</p> <p>However, this is not always the case. The word ‘knowledge’, such as in ‘prior knowledge’, mainly used in Bayesian modelling, has not much to do with logic predicates or natural language. It mainly refers to what you believe on the distribution of the parameters of your model come from before your model is trained on any data. If you think your model must be parametrized by weights that more often take extreme values, you can set a prior belief with distributions with heavier tails. Guidance on what prior distributions can be used to describe your parameters can be checked in this <a href="https://twitter.com/bindureddy/status/1708664380987220427">tweext</a></p> <p>Lastly, sometimes researchers may even say that ‘knowledge’ of the data is encoded in the parameters of the model, which leads us to the next section:</p> <h4 id="explainability-and-interpretability"><strong>Explainability and interpretability</strong></h4> <p>Explainability and interpretability are terms that can be used interchangeably, as shown in the above paper. I avoid making a distinction in this post, and just focus on sharing what is usually meant by either term.</p> <p>The simplest manifestation of explainability can be observed with simple linear models like linear or logistic regression. Here explainability simply refers to the presence of coefficients estimated per each input variable of the model. These coefficients’ magnitudes often denote the contribution of the variable to the overall prediction of the model. Other non-linear, machine learning models such as decision trees are by design explainable, and such quality mainly refers to the thresholds appearing at each junction in the decision trees.</p> <p>Concerning deep learning, particularly convolutional neural networks (CNN), explainability can refer to the computation of feature maps on the input space to see on average, what are the filters of the CNN observing when solving a task like classification given an image. Deep generative models (DGM) that leverage neural networks and learn latent representations of the data can also become ‘explainable’ if there is some post-hoc (i.e. after training) processing of embeddings, such as dimensionality reduction through principal component analysis, that can map the embeddings on 2D space. In such space, it is hoped that the DGM learns disentangled features, albeit how are they interpreted is mainly up to the researchers, not by some objective reason.</p> <p>Explainability in the eyes of some authors may also refer to the capability of the researcher to compute a numerical value of the likelihood of the parameters of the data, or at least an approximation of it, such as the evidence lower bound. This scalar value can be used to compare the performance of different models, and these models can thus be referred to as “provable”.</p> <p>Other authors may also define explainability when deep neural networks, despite being black boxes, can accept inputs or display its predictions through a user-friendly interface that allows them to interact with the model.</p> <h1 id="bottom-line">Bottom line</h1> <p>My key advice in this brief article is to convince you not to trust the “intuitive” interpretation of a word, especially if it is used on a quotidian basis. For example, in my personal view, to be “explainable” means to be able to justify its own answer. Just like a friend to whom I ask why didn’t they go to a concert by singer Aimer, to be “explainable” (at least when referring to the common meaning of this word in daily life) means that they can give a justification as to why didn’t they go (maybe they became sick, or admit they have poor taste in music). ‘Explainable’ in the machine learning literature has not much to do with this daily life interpretation.</p> <p>Finally, the above terms are simply the tip of the iceberg. In the wider research literature, authors can diverge in what they mean by on many terms such as “language”, the word “intelligence” itself, and more controversial terms like “artificial general intelligence”. Because I don’t know the precise meaning of them myself, I refrain from commenting furthermore.</p> <p>Happy learning!</p>]]></content><author><name></name></author><category term="blog-post"/><category term="food-for-thought"/><summary type="html"><![CDATA[Some thoughts concerning how to interpret quotidian words appearing in AI research: knowledge, explanation, etc.]]></summary></entry></feed>